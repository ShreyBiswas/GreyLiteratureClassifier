{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from FastFitClassifier import FastFitClassifier\n",
    "import pandas as pd\n",
    "from tqdm.auto import tqdm\n",
    "\n",
    "tqdm.pandas()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "model_path = \"./models/relevance/avsolatorio/GIST-small-Embedding-v0\"\n",
    "\n",
    "\n",
    "\n",
    "if not DEV:\n",
    "    model_path = './models/relevance/avsolatorio/GIST-Embedding-v0'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 5000 entries, 0 to 4999\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           5000 non-null   object\n",
      " 1   text          5000 non-null   object\n",
      " 2   relevance     5000 non-null   object\n",
      " 3   multiclasses  5000 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 156.4+ KB\n"
     ]
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/level-0.5/data.json\"):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    return data\n",
    "\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "test_dataset = import_labelled_data(path=\"../../../data/level-0.5/irrelevant.json\", )\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "if DEV:\n",
    "    test_dataset = test_dataset.sample(5000)\n",
    "\n",
    "test_dataset = test_dataset.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "\n",
    "test_dataset.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>multiclasses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>http://repositorio.unap.edu.pe/bitstream/handl...</td>\n",
       "      <td>UNIVERSIDAD NACIONAL DEL ALTIPLANO - PUNO FACU...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.wycokck.org/files/assets/public/v/...</td>\n",
       "      <td>ARPA Facilities Review February 8, 2022 1 $91....</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.chelsealogistics.ph/wp-content/upl...</td>\n",
       "      <td>The following document has been received: Rece...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.letran.edu.ph/Documents/AY2023-202...</td>\n",
       "      <td>www.letran.edu.ph/Home/Resources Tuesday - Jul...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://deepdivestocks.com/wp-content/uploads/...</td>\n",
       "      <td>$SPY on 6/17/2022 VoEx Graph 5.0 2.5 x E 0.0 o...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  http://repositorio.unap.edu.pe/bitstream/handl...   \n",
       "1  https://www.wycokck.org/files/assets/public/v/...   \n",
       "2  https://www.chelsealogistics.ph/wp-content/upl...   \n",
       "3  https://www.letran.edu.ph/Documents/AY2023-202...   \n",
       "4  https://deepdivestocks.com/wp-content/uploads/...   \n",
       "\n",
       "                                                text   relevance multiclasses  \n",
       "0  UNIVERSIDAD NACIONAL DEL ALTIPLANO - PUNO FACU...  irrelevant           []  \n",
       "1  ARPA Facilities Review February 8, 2022 1 $91....  irrelevant           []  \n",
       "2  The following document has been received: Rece...  irrelevant           []  \n",
       "3  www.letran.edu.ph/Home/Resources Tuesday - Jul...  irrelevant           []  \n",
       "4  $SPY on 6/17/2022 VoEx Graph 5.0 2.5 x E 0.0 o...  irrelevant           []  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_dataset.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from ./models/relevance/avsolatorio/GIST-small-Embedding-v0 ...\n",
      "Model loaded.\n",
      "Tokenizer loaded....\n",
      "Building classifier pipeline...\r"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "The model 'FastFit' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Classifier pipeline built.\n"
     ]
    }
   ],
   "source": [
    "classifier = FastFitClassifier(\n",
    "    model_path=model_path,\n",
    "    text_overlap_proportion=0.2,\n",
    "    device='cuda'\n",
    ")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Profiling Section"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Looking in indexes: https://pypi.org/simple, https://pypi.ngc.nvidia.com\n",
      "Requirement already satisfied: line_profiler in /root/.local/share/virtualenvs/GreyLiteratureClassifier-eJH_GeT1/lib/python3.10/site-packages (4.1.3)\n",
      "\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m A new release of pip is available: \u001b[0m\u001b[31;49m24.1.2\u001b[0m\u001b[39;49m -> \u001b[0m\u001b[32;49m24.2\u001b[0m\n",
      "\u001b[1m[\u001b[0m\u001b[34;49mnotice\u001b[0m\u001b[1;39;49m]\u001b[0m\u001b[39;49m To update, run: \u001b[0m\u001b[32;49mpip install --upgrade pip\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!pip install line_profiler\n",
    "%load_ext line_profiler"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "749730e173fe499b8f9faec15afc4a6b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data chunked.\n",
      "Predicting from chunks...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6c5afdcc49548539762c038d444fa48",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files\t:   0%|          | 0/100 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "08b9508760c447229c65d293a12df48d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks\t:   0%|          | 0/28580 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calculated predictions.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Timer unit: 0.001 s\n",
      "\n",
      "Total time: 51.2633 s\n",
      "File: /GreyLitDocker/GreyLiteratureClassifier/src/FastFit/level-1/FastFitClassifier.py\n",
      "Function: predict_chunks at line 50\n",
      "\n",
      "Line #      Hits         Time  Per Hit   % Time  Line Contents\n",
      "==============================================================\n",
      "    50                                               def predict_chunks(self, chunked_text, chunk_id_counts,chunk_ids):\n",
      "    51         1          0.0      0.0      0.0          predictions = []\n",
      "    52         1          0.0      0.0      0.0          scores = []\n",
      "    53                                           \n",
      "    54         1          0.0      0.0      0.0          i = 0\n",
      "    55                                           \n",
      "    56         1          0.1      0.1      0.0          print(\"Predicting from chunks...\", end=\"\\r\")\n",
      "    57                                           \n",
      "    58         2         32.1     16.0      0.1          with tqdm(total=len(chunk_id_counts),desc='Files\\t') as files_pbar:\n",
      "    59         2         41.7     20.8      0.1              with tqdm(total=len(chunked_text),desc='Chunks\\t',miniters=50) as chunks_pbar:\n",
      "    60                                           \n",
      "    61     28581      47472.9      1.7     92.6                  for output in self.classifier(chunked_text,batch_size=64,num_workers=16,truncation=True):\n",
      "    62     28580         45.1      0.0      0.1                      predictions.append(output['label'])\n",
      "    63     28580         27.4      0.0      0.1                      scores.append(output['score'])\n",
      "    64     28580        930.7      0.0      1.8                      chunks_pbar.update(1)\n",
      "    65                                           \n",
      "    66     28580       1394.6      0.0      2.7                      chunk_id_counts[chunk_ids.iloc[i]] -= 1\n",
      "    67     28580       1141.5      0.0      2.2                      if chunk_id_counts[chunk_ids.iloc[i]] == 0:\n",
      "    68       100        145.6      1.5      0.3                          files_pbar.update(1)\n",
      "    69     28580         31.6      0.0      0.1                      i += 1\n",
      "    70                                           \n",
      "    71         1          0.1      0.1      0.0          print(\"Calculated predictions.\")\n",
      "    72         1          0.0      0.0      0.0          return predictions, scores"
     ]
    }
   ],
   "source": [
    "from transformers.pipelines.pt_utils import KeyDataset\n",
    "from datasets import Dataset\n",
    "print(\"Chunking data...\", end=\"\\r\")\n",
    "from chunking import chunk_dataset_and_explode\n",
    "\n",
    "data = test_dataset.sample(100)\n",
    "\n",
    "chunked_data = chunk_dataset_and_explode(\n",
    "    data, max_len=512, overlap=int(512 * 0.2)\n",
    ")\n",
    "\n",
    "# get dict of chunk_ids to counts of that id so we can track when we've finished all chunks for a file\n",
    "chunk_id_counts = chunked_data[\"chunk_id\"].value_counts().to_dict()\n",
    "\n",
    "print(\"Data chunked.\")\n",
    "\n",
    "chunked_dataset = Dataset.from_pandas(chunked_data)\n",
    "\n",
    "print(\"Predicting...\", end=\"\\r\")\n",
    "\n",
    "%lprun -u 1e-3 -f FastFitClassifier.predict_chunks classifier.predict_chunks(KeyDataset(chunked_dataset,'text'),chunk_id_counts, chunked_data['chunk_id'])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##### End Profiling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunking data...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3be7accd8c4149178a8d3a89e76b18a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data chunked.\n",
      "Predicting from chunks...\r"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "cb643cd883a3474a8fdd46e8e1892fbf",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Files\t:   0%|          | 0/5000 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f0cd7d19baf44bae8e8746b092e5dc3f",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Chunks\t:   0%|          | 0/1516037 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "\n",
    "#! on fastfit/modeling.py, line 838, inserting **kwargs\n",
    "#! it's easier than getting the tokenizer to stop outputting\n",
    "#! extra info like token_type_ids\n",
    "\n",
    "predicted_data = classifier.evaluate(\n",
    "    test_dataset,\n",
    "    metrics=[\n",
    "        \"accuracy\",\n",
    "        \"precision\",\n",
    "        \"classification-report\",\n",
    "        \"specificity\",\n",
    "        \"confusion-matrix-mpl\",\n",
    "    ],\n",
    "    aggregate=\"majority\",\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = predicted_data[predicted_data[\"predictions\"] == \"relevant\"]\n",
    "potential = potential.sort_values(by=\"score-lv1\", ascending=False)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "potential = potential[potential['text'].apply(lambda x: x!='No summary for this intervention.')]\n",
    "potential = potential[potential['score-lv1'].apply(lambda x: x>=0.95)].reset_index(drop=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Max score: 1.0\n",
      "Min score: 0.9506133066283332\n"
     ]
    }
   ],
   "source": [
    "# get max min range of score\n",
    "max_score = potential['score-lv1'].max()\n",
    "min_score = potential['score-lv1'].min()\n",
    "print(f\"Max score: {max_score}\")\n",
    "print(f\"Min score: {min_score}\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "if DEV:\n",
    "    from os import makedirs, path\n",
    "    if not path.exists(\"../../../data/level-1.5/potential/dev\"):\n",
    "        makedirs(\"../../../data/level-1.5/potential/dev\")\n",
    "    potential.to_json(\"../../../data/level-1.5/potential/dev/data.json\", orient=\"records\", indent=4)\n",
    "else:\n",
    "    potential.to_json(\"../../../data/level-1.5/potential/data.json\", orient=\"records\", indent=4)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-pwi3iMQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import tqdm\n",
    "import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = False\n",
    "model_name = \"avsolatorio/GIST-small-Embedding-v0\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>class</th>\n",
       "      <th>relevance</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.wetlands.org/wp-content/uploads/20...</td>\n",
       "      <td>\\n \\n \\nFlamingo\\nFlamingo\\nFlamingo\\nFlaming...</td>\n",
       "      <td>Birds</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.wetlands.org/publications/flamingo...</td>\n",
       "      <td>\\n\\n \\n \\n \\n \\n \\n \\nABOUT THE GROUP \\n \\nThe...</td>\n",
       "      <td>Birds</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.wetlands.org/publications/the-stat...</td>\n",
       "      <td>\\n\\n\\n\\n\\n\\n(FIRST PAGE) \\n \\n \\n \\n \\nTHE STA...</td>\n",
       "      <td>Birds</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.sciencedirect.com/science/article/...</td>\n",
       "      <td>\\nPlease contact us via our\\nsupport center fo...</td>\n",
       "      <td>Mammals</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.wetlands.org/publications/strategi...</td>\n",
       "      <td>Strategies for wise use of Wetlands:\\nBest Pra...</td>\n",
       "      <td>Wetlands</td>\n",
       "      <td>relevant</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.wetlands.org/wp-content/uploads/20...   \n",
       "1  https://www.wetlands.org/publications/flamingo...   \n",
       "2  https://www.wetlands.org/publications/the-stat...   \n",
       "3  https://www.sciencedirect.com/science/article/...   \n",
       "4  https://www.wetlands.org/publications/strategi...   \n",
       "\n",
       "                                                text     class relevance  \n",
       "0   \\n \\n \\nFlamingo\\nFlamingo\\nFlamingo\\nFlaming...     Birds  relevant  \n",
       "1  \\n\\n \\n \\n \\n \\n \\n \\nABOUT THE GROUP \\n \\nThe...     Birds  relevant  \n",
       "2  \\n\\n\\n\\n\\n\\n(FIRST PAGE) \\n \\n \\n \\n \\nTHE STA...     Birds  relevant  \n",
       "3  \\nPlease contact us via our\\nsupport center fo...   Mammals  relevant  \n",
       "4  Strategies for wise use of Wetlands:\\nBest Pra...  Wetlands  relevant  "
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/labelled/data.json\", group_relevant=True):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    data[\"relevance\"] = data[\"class\"].apply(\n",
    "        lambda x: \"relevant\" if x != \"irrelevant\" else x\n",
    "    )\n",
    "    return data\n",
    "\n",
    "\n",
    "data = import_labelled_data(path=\"../../data/labelled/data.json\", group_relevant=False)\n",
    "\n",
    "# drop null classes\n",
    "data = data.dropna(subset=[\"class\"])\n",
    "\n",
    "\n",
    "if DEV:\n",
    "    data = data.sample(5000)\n",
    "\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "train_data, test_data = train_test_split(data, test_size=0.2, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.2, random_state=42)\n",
    "\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "from chunking import chunk_dataset_and_explode\n",
    "\n",
    "\n",
    "# roughly 4 characters per token\n",
    "max_len = 2048\n",
    "\n",
    "train_data = chunk_dataset_and_explode(train_data, max_len=max_len, overlap=int(max_len * 0.2))\n",
    "test_data = chunk_dataset_and_explode(test_data, max_len=max_len, overlap=int(max_len * 0.2))\n",
    "val_data = chunk_dataset_and_explode(val_data, max_len=max_len, overlap=int(max_len * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunk_id', 'url', 'text', 'class', 'relevance'],\n",
       "    num_rows: 93801\n",
       "})"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data, split=\"train\")\n",
    "test_dataset = Dataset.from_pandas(test_data, split=\"test\")\n",
    "val_dataset = Dataset.from_pandas(val_data, split=\"val\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from fastfit import sample_dataset, FastFitTrainer\n",
    "\n",
    "\n",
    "train_dataset = sample_dataset(train_dataset, label_column='relevance',num_samples_per_label=150,seed=42)\n",
    "val_dataset = val_dataset.shuffle(seed=42).select(range(200))\n",
    "test_dataset = test_dataset.shuffle(seed=42).select(range(1500))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! had to modify FastFitTrainer to at /fastfit/train.py, line 879, to add trust_remote_code=True to the loading of 'accuracy' metrics\n",
    "#! don't know why it's not default, since accuracy is the default in fastfit\n",
    "\n",
    "#* note that since SetFit uses evaluation_strategy as the argument name rather than eval_strategy\n",
    "#* I had to change it in the FastFitTrainer call below\n",
    "#* if using the latest transformers version (transformers>=4.41.0), use eval_strategy\n",
    "\n",
    "#! another change in FastFitTrainer, also at line 879; commented out the fixed version above\n",
    "#! since load_metric is deprecated in favour of evaluate.load()\n",
    "#! using evaluate means we can use evaluate.combine(), which lets us calculate multiple metrics at once\n",
    "#! also, add the ability to just send in our own compute_metrics function\n",
    "#! essentially, copy the below code to replace line 879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into line 879.\n",
    "\n",
    "```python\n",
    "        # metric = load_metric(self.data_args.metric_name, experiment_id=uuid.uuid4())\n",
    "        from evaluate import combine, load\n",
    "        if type(self.data_args.metric_name) == str: # single metric name\n",
    "            metrics = [load(self.data_args.metric_name, experiment_id=uuid.uuid4())]\n",
    "        elif type(self.data_args.metric_name) == list: # compute multiple metrics\n",
    "            metrics = [load(metric,experiment_id=uuid.uuid4()) for metric in self.data_args.metric_name]\n",
    "\n",
    "        # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "        # predictions and label_ids field) and has to return a dictionary string to float.\n",
    "        def compute_metrics(p: EvalPrediction):\n",
    "            predictions = (\n",
    "                p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "            )\n",
    "            predictions = (\n",
    "                np.squeeze(predictions)\n",
    "                if self.is_regression\n",
    "                else np.argmax(predictions, axis=1)\n",
    "            )\n",
    "            references = p.label_ids\n",
    "\n",
    "            results = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                if metric.name != 'accuracy':\n",
    "                    results.update(metric.compute(predictions=predictions, references=references,average='macro'))\n",
    "                else:\n",
    "                    results.update(metric.compute(predictions=predictions, references=references))\n",
    "\n",
    "            return results\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/23/2024 15:27:31 - WARNING - fastfit.train - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/root/.local/share/virtualenvs/GreyLiteratureClassifier-eJH_GeT1/lib/python3.10/site-packages/transformers/training_args.py:1494: FutureWarning: `evaluation_strategy` is deprecated and will be removed in version 4.46 of 🤗 Transformers. Use `eval_strategy` instead\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "16d163f5736849ea8eea6edbc066e6c2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "1957de03cf8e4a2d95a8f98e70102580",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Flattening the indices:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "55835a3086184ae7b37f399cb6274073",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/300 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c1a83346967a4beab92389bc56ef6879",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/200 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "a69e949f37d34bcd80445f8a0fe2afd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/1500 [00:00<?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4490d8ad0505431998ae0591fb37ccd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/300 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46468d412cee41a2a041300c241226dd",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/200 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2d24c1ae88634f159cbfa4915cda0d4d",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# same args as the huggingface TrainingArguments\n",
    "\n",
    "\n",
    "trainer = FastFitTrainer(\n",
    "    model_name_or_path=model_name,\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    output_dir=f'models/{model_name}',\n",
    "    overwrite_output_dir=True,\n",
    "    label_column_name=\"relevance\",\n",
    "    text_column_name=\"text\",\n",
    "    num_train_epochs=20,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_text_length=2048,\n",
    "    seed=42,\n",
    "    num_repeats=1,\n",
    "    evaluation_strategy=\"epoch\",\n",
    "    fp16=True,\n",
    "    logging_steps=1,\n",
    "    metric_name=['precision','accuracy','f1']\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:1198] 2024-07-23 15:28:00,927 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='100' max='100' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [100/100 01:03, Epoch 20/20]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Accuracy</th>\n",
       "      <th>F1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.299000</td>\n",
       "      <td>4.486179</td>\n",
       "      <td>0.971910</td>\n",
       "      <td>0.975000</td>\n",
       "      <td>0.974536</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>3.960200</td>\n",
       "      <td>4.249847</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.989770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>3.889800</td>\n",
       "      <td>4.215579</td>\n",
       "      <td>0.988372</td>\n",
       "      <td>0.990000</td>\n",
       "      <td>0.989770</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>3.838900</td>\n",
       "      <td>4.182316</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>3.820600</td>\n",
       "      <td>4.225079</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>3.814100</td>\n",
       "      <td>4.203447</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>3.808900</td>\n",
       "      <td>4.185375</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>3.799500</td>\n",
       "      <td>4.189353</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>3.797700</td>\n",
       "      <td>4.207273</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>3.790500</td>\n",
       "      <td>4.220812</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>11</td>\n",
       "      <td>3.787500</td>\n",
       "      <td>4.224096</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>12</td>\n",
       "      <td>3.809900</td>\n",
       "      <td>4.224817</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>13</td>\n",
       "      <td>3.781900</td>\n",
       "      <td>4.224833</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>14</td>\n",
       "      <td>3.784200</td>\n",
       "      <td>4.225881</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>15</td>\n",
       "      <td>3.780000</td>\n",
       "      <td>4.226523</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>16</td>\n",
       "      <td>3.787400</td>\n",
       "      <td>4.227547</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>17</td>\n",
       "      <td>3.778700</td>\n",
       "      <td>4.228691</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>18</td>\n",
       "      <td>3.777100</td>\n",
       "      <td>4.229496</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>19</td>\n",
       "      <td>3.776700</td>\n",
       "      <td>4.229513</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>20</td>\n",
       "      <td>3.776200</td>\n",
       "      <td>4.229564</td>\n",
       "      <td>0.982759</td>\n",
       "      <td>0.985000</td>\n",
       "      <td>0.984678</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       20.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     4.1634\n",
      "  train_runtime            = 0:01:05.49\n",
      "  train_samples            =        300\n",
      "  train_samples_per_second =      91.61\n",
      "  train_steps_per_second   =      1.527\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! another fastfit library modification\n",
    "#! in /fastfit/train.py, line 971, change ignore_keys_for_eval from type set to a list\n",
    "#! since it gets concatenated to a list later on\n",
    "#! note that since we've added lines above, this is now line 981\n",
    "#! the line beginning ignore_keys_for_eval={\"doc_input_ids\",\"doc_attention_mask\",\"labels\"}\n",
    "\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='28' max='4' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [4/4 00:08]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =      0.985\n",
      "  eval_f1                 =     0.9847\n",
      "  eval_loss               =     4.2296\n",
      "  eval_precision          =     0.9828\n",
      "  eval_runtime            = 0:00:01.04\n",
      "  eval_samples            =        200\n",
      "  eval_samples_per_second =    191.977\n",
      "  eval_steps_per_second   =       3.84\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.985\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {results[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "model.save_pretrained(f'models/{model_name}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "tokenizer = trainer.tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['SPECIAL_TOKENS_ATTRIBUTES',\n",
       " '__annotations__',\n",
       " '__call__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__module__',\n",
       " '__ne__',\n",
       " '__new__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__setattr__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__subclasshook__',\n",
       " '__weakref__',\n",
       " '_add_tokens',\n",
       " '_additional_special_tokens',\n",
       " '_auto_class',\n",
       " '_batch_encode_plus',\n",
       " '_bos_token',\n",
       " '_call_one',\n",
       " '_cls_token',\n",
       " '_compile_jinja_template',\n",
       " '_convert_encoding',\n",
       " '_convert_id_to_token',\n",
       " '_convert_token_to_id_with_added_voc',\n",
       " '_create_repo',\n",
       " '_decode',\n",
       " '_decode_use_source_tokenizer',\n",
       " '_encode_plus',\n",
       " '_eos_token',\n",
       " '_eventual_warn_about_too_long_sequence',\n",
       " '_eventually_correct_t5_max_length',\n",
       " '_from_pretrained',\n",
       " '_get_files_timestamps',\n",
       " '_get_padding_truncation_strategies',\n",
       " '_in_target_context_manager',\n",
       " '_mask_token',\n",
       " '_pad',\n",
       " '_pad_token',\n",
       " '_pad_token_type_id',\n",
       " '_processor_class',\n",
       " '_save_pretrained',\n",
       " '_sep_token',\n",
       " '_set_processor_class',\n",
       " '_switch_to_input_mode',\n",
       " '_switch_to_target_mode',\n",
       " '_tokenizer',\n",
       " '_unk_token',\n",
       " '_upload_modified_files',\n",
       " 'add_special_tokens',\n",
       " 'add_tokens',\n",
       " 'added_tokens_decoder',\n",
       " 'added_tokens_encoder',\n",
       " 'additional_special_tokens',\n",
       " 'additional_special_tokens_ids',\n",
       " 'all_special_ids',\n",
       " 'all_special_tokens',\n",
       " 'all_special_tokens_extended',\n",
       " 'apply_chat_template',\n",
       " 'as_target_tokenizer',\n",
       " 'backend_tokenizer',\n",
       " 'batch_decode',\n",
       " 'batch_encode_plus',\n",
       " 'bos_token',\n",
       " 'bos_token_id',\n",
       " 'build_inputs_with_special_tokens',\n",
       " 'can_save_slow_tokenizer',\n",
       " 'chat_template',\n",
       " 'clean_up_tokenization',\n",
       " 'clean_up_tokenization_spaces',\n",
       " 'cls_token',\n",
       " 'cls_token_id',\n",
       " 'convert_added_tokens',\n",
       " 'convert_ids_to_tokens',\n",
       " 'convert_tokens_to_ids',\n",
       " 'convert_tokens_to_string',\n",
       " 'create_token_type_ids_from_sequences',\n",
       " 'decode',\n",
       " 'decoder',\n",
       " 'default_chat_template',\n",
       " 'deprecation_warnings',\n",
       " 'do_lower_case',\n",
       " 'encode',\n",
       " 'encode_plus',\n",
       " 'eos_token',\n",
       " 'eos_token_id',\n",
       " 'from_pretrained',\n",
       " 'get_added_vocab',\n",
       " 'get_special_tokens_mask',\n",
       " 'get_vocab',\n",
       " 'init_inputs',\n",
       " 'init_kwargs',\n",
       " 'is_fast',\n",
       " 'mask_token',\n",
       " 'mask_token_id',\n",
       " 'max_len_sentences_pair',\n",
       " 'max_len_single_sentence',\n",
       " 'model_input_names',\n",
       " 'model_max_length',\n",
       " 'name_or_path',\n",
       " 'num_special_tokens_to_add',\n",
       " 'pad',\n",
       " 'pad_token',\n",
       " 'pad_token_id',\n",
       " 'pad_token_type_id',\n",
       " 'padding_side',\n",
       " 'prepare_for_model',\n",
       " 'prepare_seq2seq_batch',\n",
       " 'pretrained_vocab_files_map',\n",
       " 'push_to_hub',\n",
       " 'register_for_auto_class',\n",
       " 'sanitize_special_tokens',\n",
       " 'save_pretrained',\n",
       " 'save_vocabulary',\n",
       " 'sep_token',\n",
       " 'sep_token_id',\n",
       " 'set_truncation_and_padding',\n",
       " 'slow_tokenizer_class',\n",
       " 'special_tokens_map',\n",
       " 'special_tokens_map_extended',\n",
       " 'split_special_tokens',\n",
       " 'tokenize',\n",
       " 'train_new_from_iterator',\n",
       " 'truncate_sequences',\n",
       " 'truncation_side',\n",
       " 'unk_token',\n",
       " 'unk_token_id',\n",
       " 'verbose',\n",
       " 'vocab',\n",
       " 'vocab_files_names',\n",
       " 'vocab_size']"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(tokenizer)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =       20.0\n",
      "  eval_accuracy           =     0.9833\n",
      "  eval_f1                 =     0.9819\n",
      "  eval_loss               =     4.3387\n",
      "  eval_precision          =     0.9802\n",
      "  eval_runtime            = 0:00:07.06\n",
      "  eval_samples_per_second =    212.386\n",
      "  eval_steps_per_second   =      3.398\n",
      "  test_samples            =       1500\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-pwi3iMQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "usage: train.py [-h] [--model MODEL] [--data-path DATA_PATH]\n",
      "                [--output-path OUTPUT_PATH] [--test-frac TEST_FRAC]\n",
      "                [--seed SEED] [--vectorizer VECTORIZER] [--timer]\n",
      "                [--embedding-model EMBEDDING_MODEL] [--chunk-size CHUNK_SIZE]\n",
      "                [--samples-per-label SAMPLES_PER_LABEL]\n",
      "                [--batch-size BATCH_SIZE]\n",
      "\n",
      "options:\n",
      "  -h, --help            show this help message and exit\n",
      "  --model MODEL         Type of model to train\n",
      "  --data-path DATA_PATH\n",
      "                        Path to data\n",
      "  --output-path OUTPUT_PATH\n",
      "                        Path to save model\n",
      "  --test-frac TEST_FRAC\n",
      "                        Fraction of data to use for testing\n",
      "  --seed SEED           Random seed\n",
      "  --vectorizer VECTORIZER\n",
      "                        Vectorizer to use\n",
      "  --timer               Time the training process\n",
      "  --embedding-model EMBEDDING_MODEL\n",
      "                        Embedding model to use. Only used if --model is\n",
      "                        EMBEDDINGS or FASTFIT\n",
      "  --chunk-size CHUNK_SIZE\n",
      "                        Size of chunks to process data in. Only used if\n",
      "                        --model is EMBEDDINGS or FASTFIT\n",
      "  --samples-per-label SAMPLES_PER_LABEL\n",
      "                        Number of samples to use per label. Leave excluded for\n",
      "                        uncapped. Only used if --model is EMBEDDINGS or\n",
      "                        FASTFIT\n",
      "  --batch-size BATCH_SIZE\n",
      "                        Batch size for training. Only used if --model is\n",
      "                        EMBEDDINGS or FASTFIT\n"
     ]
    }
   ],
   "source": [
    "def recommended_batch_size(memory_use, max_tokens): # on the Alienware\n",
    "    from math import log2\n",
    "    # derived from GIST-Embedding-v0 runs\n",
    "    gist_difficulty = 0.41 * 512\n",
    "    difficulty = memory_use * max_tokens\n",
    "\n",
    "    estimation = gist_difficulty / difficulty * 64\n",
    "    return 2**int(log2(estimation))\n",
    "\n",
    "recommended_batch_size(1.5, 512) # 64"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "Using default paths:\n",
      "Irrelevant: ../../data/unprocessed/irrelevant/\n",
      "Spreadsheet: ../../data/unprocessed/spreadsheet.json\n",
      "Synopses: ../../data/unprocessed/synopses/\n",
      "Studies: ../../data/unprocessed/studies.json\n",
      "\n",
      "\n",
      "\n",
      "Cleaning irrelevant data.\n",
      "Loading ~10000 files...\n",
      " 86%|███████████████████████████████▋     | 8556/10000 [00:13<00:02, 612.00it/s]\n",
      "\n",
      "8556 irrelevant articles loaded. \n",
      "Removing corrupt files...\n",
      "Removed 0 broken JSON files.\n",
      "Saving irrelevant data...\n",
      "Irrelevant data saved.\n"
     ]
    }
   ],
   "source": [
    "!python preprocess.py \\\n",
    "    --use-default-paths \\\n",
    "    --only-irrelevant \\\n",
    "    --limit-irrelevant 50000"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading data...\n",
      "\n",
      "/usr/local/lib/python3.10/dist-packages/cudf/utils/_ptxcompiler.py:61: UserWarning: Error getting driver and runtime versions:\n",
      "\n",
      "stdout:\n",
      "\n",
      "\n",
      "\n",
      "stderr:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 254, in ensure_initialized\n",
      "    self.cuInit(0)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 327, in safe_cuda_api_call\n",
      "    self._check_ctypes_error(fname, retcode)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 395, in _check_ctypes_error\n",
      "    raise CudaAPIError(retcode, msg)\n",
      "numba.cuda.cudadrv.driver.CudaAPIError: [100] Call to cuInit results in CUDA_ERROR_NO_DEVICE\n",
      "\n",
      "During handling of the above exception, another exception occurred:\n",
      "\n",
      "Traceback (most recent call last):\n",
      "  File \"<string>\", line 4, in <module>\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 292, in __getattr__\n",
      "    self.ensure_initialized()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/numba/cuda/cudadrv/driver.py\", line 258, in ensure_initialized\n",
      "    raise CudaSupportError(f\"Error at driver init: {description}\")\n",
      "numba.cuda.cudadrv.error.CudaSupportError: Error at driver init: Call to cuInit results in CUDA_ERROR_NO_DEVICE (100)\n",
      "\n",
      "\n",
      "Not patching Numba\n",
      "  warnings.warn(msg, UserWarning)\n",
      "/usr/local/lib/python3.10/dist-packages/cudf/utils/gpu_utils.py:149: UserWarning: No NVIDIA GPU detected\n",
      "  warnings.warn(\"No NVIDIA GPU detected\")\n",
      "/usr/local/lib/python3.10/dist-packages/cudf/io/json.py:108: UserWarning: Using CPU via Pandas to read JSON dataset, this may be GPU accelerated in the future\n",
      "  warnings.warn(\n",
      "Traceback (most recent call last):\n",
      "  File \"/workspace/GreyLitDocker/src/scripts/train.py\", line 160, in <module>\n",
      "    main(**vars(args))\n",
      "  File \"/workspace/GreyLitDocker/src/scripts/train.py\", line 134, in main\n",
      "    train_cuML(**kwargs)\n",
      "  File \"/workspace/GreyLitDocker/src/scripts/train.py\", line 38, in train_cuML\n",
      "    data = pd.read_json(input_path, encoding='latin-1')\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/io/json.py\", line 140, in read_json\n",
      "    df = cudf.from_pandas(pd_value)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvtx/nvtx.py\", line 116, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/dataframe.py\", line 8065, in from_pandas\n",
      "    return DataFrame.from_pandas(obj, nan_as_null=nan_as_null)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/nvtx/nvtx.py\", line 116, in inner\n",
      "    result = func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/dataframe.py\", line 5376, in from_pandas\n",
      "    data = {\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/dataframe.py\", line 5377, in <dictcomp>\n",
      "    col_name: column.as_column(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/column/column.py\", line 1951, in as_column\n",
      "    return as_column(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/column/column.py\", line 1832, in as_column\n",
      "    col = ColumnBase.from_arrow(arbitrary)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/cudf/core/column/column.py\", line 386, in from_arrow\n",
      "    result = libcudf.interop.from_arrow(data)[0]\n",
      "  File \"/usr/lib/python3.10/contextlib.py\", line 79, in inner\n",
      "    return func(*args, **kwds)\n",
      "  File \"interop.pyx\", line 162, in cudf._lib.interop.from_arrow\n",
      "  File \"/usr/lib/python3.10/functools.py\", line 889, in wrapper\n",
      "    return dispatch(args[0].__class__)(*args, **kw)\n",
      "  File \"interop.pyx\", line 88, in cudf._lib.pylibcudf.interop._from_arrow_table\n",
      "RuntimeError: Fatal CUDA error encountered at: /__w/cudf/cudf/cpp/src/bitmask/null_mask.cu:93: 100 cudaErrorNoDevice no CUDA-capable device is detected\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --model CuML \\\n",
    "    --data-path=../../data/level-0.5/data.json \\\n",
    "    --output-path=./models/level-1/cuML_classifier.pkl \\\n",
    "    --seed 1347 \\\n",
    "    --timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "Loading model from ./models/level-1/cuML_classifier.pkl...\n",
      "Model loaded.\n",
      "\n",
      "Loading data...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8556 entries, 0 to 8555\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           8556 non-null   object\n",
      " 1   text          8556 non-null   object\n",
      " 2   relevance     8556 non-null   object\n",
      " 3   multiclasses  8556 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 267.5+ KB\n",
      "\n",
      "Data loaded.\n",
      "\n",
      "Generating classification predictions...\n",
      "\n",
      "Classifying 8556 files.\n",
      "\u001b[1m\n",
      "Estimated time: \u001b[0m 0.0  minutes  53  seconds\n",
      "\n",
      "\n",
      "\n",
      "Testing time on 8556 articles:  64.36612033843994  seconds\n",
      "\n",
      "Files processed per second: 132.92707335803632\n",
      "Evaluating classifier...\n",
      "\n",
      "prediction\n",
      "irrelevant    8526\n",
      "relevant        30\n",
      "Name: count, dtype: int64\n",
      "Accuracy:  0.9964936886395512\n",
      "Precision:  0.5\n",
      "Recall:  0.4982468443197756\n",
      "F1:  0.49912188268352653\n",
      "Confusion matrix: \n",
      " [[8526   30]\n",
      " [   0    0]]\n",
      "\u001b[1m\n",
      "\n",
      "Potential new Conservation Evidence:\u001b[0m\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 30 entries, 3696 to 3275\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   url           30 non-null     object \n",
      " 1   text          30 non-null     object \n",
      " 2   relevance     30 non-null     object \n",
      " 3   multiclasses  30 non-null     object \n",
      " 4   score-lv1     30 non-null     float64\n",
      " 5   prediction    30 non-null     object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 1.6+ KB\n",
      "\n",
      "\n",
      "       score-lv1                                                url\n",
      "3696   0.815387  https://wsobirds.org/images/pdfs/BeyondBirdFee...\n",
      "6949   0.806360  https://digitalarchive.worldfishcenter.org/bit...\n",
      "8059   0.775780  https://digitalarchive.worldfishcenter.org/bit...\n",
      "7110   0.735814  https://cms.zsl.org/sites/default/files/2022-1...\n",
      "7756   0.730042  https://cms.zsl.org/sites/default/files/2022-1...\n",
      "1348   0.709979  https://wwjournal.org/wp-content/uploads/sites...\n",
      "6925   0.694740  https://zooreach.org/downloads/ZOO_CAMP_PHVA_r...\n",
      "7043   0.655013  https://apps.worldagroforestry.org/downloads/P...\n",
      "2644   0.637559  http://www.wpcouncil.org/wp-content/uploads/20...\n",
      "7498   0.611437  https://wos.org/documents/Publications/WA%20Bi...\n",
      "6721   0.577350  https://zenodo.org/records/1234577/files/artic...\n",
      "70     0.559298  https://zooreach.org/downloads/ZOO_CAMP_PHVA_r...\n",
      "4002   0.556397  https://zooreach.org/downloads/Education_pkt_c...\n",
      "3367   0.543745  https://www.forestfoundation.ph/wp-content/upl...\n",
      "4498   0.506252  https://www.wrmp.org/wp-content/uploads/2023/0...\n",
      "3626   0.492419  https://www.wppcinc.org/uploads/3/8/0/4/380435...\n",
      "4148   0.490777  https://cms.zsl.org/sites/default/files/2022-1...\n",
      "751    0.479021  https://apps.worldagroforestry.org/downloads/P...\n",
      "7900   0.478120  https://www.xerces.org/sites/default/files/201...\n",
      "844    0.472038  https://cms.zsl.org/sites/default/files/2022-1...\n",
      "Saving 200 potential results...\n",
      "Saved to ../../results/level-1.5/urls.csv.\n",
      "\n",
      "Saved to ../../data/level-1.5/potential.json\n",
      "\n",
      "\u001b[1m\n",
      "\n",
      "Prediction complete.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python predict.py \\\n",
    "    --model=CuML \\\n",
    "    --model-path=./models/level-1/cuML_classifier.pkl \\\n",
    "    --data-path=../../data/level-0.5/irrelevant.json \\\n",
    "    --output-path=../../results/level-1.5 \\\n",
    "    --level 1 \\\n",
    "    --save-top 1000 \\\n",
    "    --timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m************ RUNNING LEVEL 2 PREDICTIONS WITH EMBEDDING MODEL ************\u001b[0m\n",
      "\n",
      "\u001b[1mModel: \u001b[0m['avsolatorio', 'GIST-large-Embedding-v0']\n",
      "\n",
      "\n",
      "Initialising classifier...\n",
      "Loading model from ./models/level-2/avsolatorio/GIST-large-Embedding-v0 ...\n",
      "Model loaded.\n",
      "Tokenizer loaded with max_length: 512\n",
      "Building classifier pipeline...\n",
      "NOTE: Ignore the below warning about 'FastFi' not being supported for text-classification; it's not necessary.\n",
      "The model 'FastFit' is not supported for text-classification. Supported models are ['AlbertForSequenceClassification', 'BartForSequenceClassification', 'BertForSequenceClassification', 'BigBirdForSequenceClassification', 'BigBirdPegasusForSequenceClassification', 'BioGptForSequenceClassification', 'BloomForSequenceClassification', 'CamembertForSequenceClassification', 'CanineForSequenceClassification', 'LlamaForSequenceClassification', 'ConvBertForSequenceClassification', 'CTRLForSequenceClassification', 'Data2VecTextForSequenceClassification', 'DebertaForSequenceClassification', 'DebertaV2ForSequenceClassification', 'DistilBertForSequenceClassification', 'ElectraForSequenceClassification', 'ErnieForSequenceClassification', 'ErnieMForSequenceClassification', 'EsmForSequenceClassification', 'FalconForSequenceClassification', 'FlaubertForSequenceClassification', 'FNetForSequenceClassification', 'FunnelForSequenceClassification', 'GemmaForSequenceClassification', 'Gemma2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPT2ForSequenceClassification', 'GPTBigCodeForSequenceClassification', 'GPTNeoForSequenceClassification', 'GPTNeoXForSequenceClassification', 'GPTJForSequenceClassification', 'IBertForSequenceClassification', 'JambaForSequenceClassification', 'JetMoeForSequenceClassification', 'LayoutLMForSequenceClassification', 'LayoutLMv2ForSequenceClassification', 'LayoutLMv3ForSequenceClassification', 'LEDForSequenceClassification', 'LiltForSequenceClassification', 'LlamaForSequenceClassification', 'LongformerForSequenceClassification', 'LukeForSequenceClassification', 'MarkupLMForSequenceClassification', 'MBartForSequenceClassification', 'MegaForSequenceClassification', 'MegatronBertForSequenceClassification', 'MistralForSequenceClassification', 'MixtralForSequenceClassification', 'MobileBertForSequenceClassification', 'MPNetForSequenceClassification', 'MptForSequenceClassification', 'MraForSequenceClassification', 'MT5ForSequenceClassification', 'MvpForSequenceClassification', 'NemotronForSequenceClassification', 'NezhaForSequenceClassification', 'NystromformerForSequenceClassification', 'OpenLlamaForSequenceClassification', 'OpenAIGPTForSequenceClassification', 'OPTForSequenceClassification', 'PerceiverForSequenceClassification', 'PersimmonForSequenceClassification', 'PhiForSequenceClassification', 'Phi3ForSequenceClassification', 'PLBartForSequenceClassification', 'QDQBertForSequenceClassification', 'Qwen2ForSequenceClassification', 'Qwen2MoeForSequenceClassification', 'ReformerForSequenceClassification', 'RemBertForSequenceClassification', 'RobertaForSequenceClassification', 'RobertaPreLayerNormForSequenceClassification', 'RoCBertForSequenceClassification', 'RoFormerForSequenceClassification', 'SqueezeBertForSequenceClassification', 'StableLmForSequenceClassification', 'Starcoder2ForSequenceClassification', 'T5ForSequenceClassification', 'TapasForSequenceClassification', 'TransfoXLForSequenceClassification', 'UMT5ForSequenceClassification', 'XLMForSequenceClassification', 'XLMRobertaForSequenceClassification', 'XLMRobertaXLForSequenceClassification', 'XLNetForSequenceClassification', 'XmodForSequenceClassification', 'YosoForSequenceClassification'].\n",
      "\n",
      "Classifier pipeline built.\n",
      "Classifier initialised.\n",
      "\n",
      "Loading data...\n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 1000 entries, 0 to 999\n",
      "Data columns (total 6 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   url           1000 non-null   object \n",
      " 1   text          1000 non-null   object \n",
      " 2   relevance     1000 non-null   object \n",
      " 3   multiclasses  1000 non-null   object \n",
      " 4   score-lv1     1000 non-null   float64\n",
      " 5   prediction    1000 non-null   object \n",
      "dtypes: float64(1), object(5)\n",
      "memory usage: 47.0+ KB\n",
      "\n",
      "Data loaded.\n",
      "\n",
      "Generating classification predictions...\n",
      "\n",
      "Classifying 1000 files.\n",
      "100%|█████████████████████████████████████| 1000/1000 [00:00<00:00, 5806.45it/s]\n",
      "Data chunked.\n",
      "Converting to Dataset format...\n",
      "Converted.\n",
      "\n",
      "Predicting from chunks...\n",
      "Files:   0%|                                           | 0/1000 [00:00<?, ?it/s]\n",
      "Chunks:   0%|                                        | 0/171712 [00:00<?, ?it/s]\u001b[A\n",
      "Files:   0%|                                   | 1/1000 [00:03<50:32,  3.04s/it]\u001b[A\n",
      "Chunks:   0%|                             | 65/171712 [00:03<1:55:19, 24.81it/s]\u001b[A\n",
      "Chunks:   0%|                              | 129/171712 [00:03<57:05, 50.09it/s]\u001b[A\n",
      "Chunks:   0%|                              | 193/171712 [00:04<36:51, 77.54it/s]\u001b[A\n",
      "Files:   0%|                                   | 2/1000 [00:04<36:37,  2.20s/it]\u001b[A\n",
      "Files:   0%|                                   | 3/1000 [00:04<25:11,  1.52s/it]\u001b[A\n",
      "Files:   0%|▏                                  | 4/1000 [00:05<19:33,  1.18s/it]\u001b[A\n",
      "Files:   0%|▏                                  | 5/1000 [00:05<16:32,  1.00it/s]\u001b[A\n",
      "Chunks:   0%|                             | 513/171712 [00:05<18:28, 154.51it/s]\u001b[A\n",
      "Files:   1%|▏                                  | 6/1000 [00:06<15:35,  1.06it/s]\u001b[A\n",
      "Chunks:   0%|                             | 641/171712 [00:06<16:21, 174.22it/s]\u001b[A\n",
      "Chunks:   0%|                             | 705/171712 [00:06<15:54, 179.20it/s]\u001b[A\n",
      "Files:   1%|▏                                  | 7/1000 [00:07<15:31,  1.07it/s]\u001b[A\n",
      "Chunks:   0%|▏                            | 833/171712 [00:07<14:52, 191.45it/s]\u001b[A\n",
      "Chunks:   1%|▏                            | 897/171712 [00:07<15:05, 188.70it/s]\u001b[A\n",
      "Chunks:   1%|▏                            | 961/171712 [00:08<15:06, 188.38it/s]\u001b[A\n",
      "Files:   1%|▎                                  | 8/1000 [00:08<16:37,  1.01s/it]\u001b[A\n",
      "Files:   1%|▎                                  | 9/1000 [00:08<14:38,  1.13it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1153/171712 [00:09<14:15, 199.28it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1217/171712 [00:09<15:36, 182.14it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1281/171712 [00:10<16:38, 170.64it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1345/171712 [00:10<16:43, 169.81it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1409/171712 [00:10<18:00, 157.63it/s]\u001b[A\n",
      "Chunks:   1%|▏                           | 1473/171712 [00:11<17:38, 160.81it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1537/171712 [00:11<16:48, 168.74it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1601/171712 [00:12<17:28, 162.28it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1665/171712 [00:12<17:14, 164.40it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1729/171712 [00:12<16:41, 169.76it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1793/171712 [00:13<17:22, 163.05it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1857/171712 [00:13<17:33, 161.21it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 1921/171712 [00:13<17:14, 164.10it/s]\u001b[A\n",
      "Files:   1%|▎                                 | 10/1000 [00:14<26:16,  1.59s/it]\u001b[A\n",
      "Chunks:   1%|▎                           | 2049/171712 [00:14<17:16, 163.62it/s]\u001b[A\n",
      "Chunks:   1%|▎                           | 2113/171712 [00:15<17:50, 158.38it/s]\u001b[A\n",
      "Files:   1%|▍                                 | 12/1000 [00:15<22:01,  1.34s/it]\u001b[A\n",
      "Chunks:   1%|▎                           | 2241/171712 [00:15<15:53, 177.69it/s]\u001b[A\n",
      "Chunks:   1%|▍                           | 2305/171712 [00:16<15:46, 178.94it/s]\u001b[A\n",
      "Files:   1%|▍                                 | 13/1000 [00:16<21:26,  1.30s/it]\u001b[A\n",
      "Files:   1%|▍                                 | 14/1000 [00:17<20:40,  1.26s/it]\u001b[A\n",
      "Chunks:   1%|▍                           | 2497/171712 [00:17<22:21, 126.14it/s]\u001b[A\n",
      "Chunks:   1%|▍                           | 2561/171712 [00:18<19:42, 143.09it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2625/171712 [00:18<19:35, 143.87it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2689/171712 [00:19<20:24, 138.07it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2753/171712 [00:19<21:01, 133.89it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2817/171712 [00:20<19:18, 145.76it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2881/171712 [00:20<19:15, 146.07it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 2945/171712 [00:20<19:49, 141.89it/s]\u001b[A\n",
      "Chunks:   2%|▍                           | 3009/171712 [00:21<17:27, 161.01it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3073/171712 [00:21<16:15, 172.81it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3137/171712 [00:22<17:45, 158.28it/s]\u001b[A\n",
      "Files:   2%|▌                                 | 15/1000 [00:22<27:44,  1.69s/it]\u001b[A\n",
      "Files:   2%|▌                                 | 17/1000 [00:22<22:31,  1.38s/it]\u001b[A\n",
      "Chunks:   2%|▌                           | 3329/171712 [00:23<15:28, 181.26it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3393/171712 [00:23<18:28, 151.81it/s]\u001b[A\n",
      "Files:   2%|▋                                 | 19/1000 [00:24<20:19,  1.24s/it]\u001b[A\n",
      "Chunks:   2%|▌                           | 3521/171712 [00:24<18:00, 155.72it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3585/171712 [00:24<16:32, 169.42it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3649/171712 [00:25<16:14, 172.53it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3713/171712 [00:25<16:03, 174.31it/s]\u001b[A\n",
      "Chunks:   2%|▌                           | 3777/171712 [00:25<16:12, 172.66it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 3841/171712 [00:26<15:52, 176.27it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 3905/171712 [00:26<15:40, 178.49it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 3969/171712 [00:26<15:29, 180.51it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 4033/171712 [00:27<15:17, 182.66it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 4097/171712 [00:27<15:19, 182.28it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 4161/171712 [00:27<15:25, 180.96it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 4225/171712 [00:28<15:47, 176.77it/s]\u001b[A\n",
      "Chunks:   2%|▋                           | 4289/171712 [00:28<16:29, 169.13it/s]\u001b[A\n",
      "Chunks:   3%|▋                           | 4353/171712 [00:29<16:16, 171.32it/s]\u001b[A\n",
      "Chunks:   3%|▋                           | 4417/171712 [00:29<17:03, 163.48it/s]\u001b[A\n",
      "Chunks:   3%|▋                           | 4481/171712 [00:29<16:33, 168.30it/s]\u001b[A\n",
      "Chunks:   3%|▋                           | 4545/171712 [00:30<16:00, 174.05it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4609/171712 [00:30<15:48, 176.17it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4673/171712 [00:30<15:31, 179.24it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4737/171712 [00:31<16:26, 169.31it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4801/171712 [00:31<16:25, 169.36it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4865/171712 [00:32<15:59, 173.80it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4929/171712 [00:32<16:14, 171.14it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 4993/171712 [00:32<16:43, 166.15it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 5057/171712 [00:33<17:06, 162.36it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 5121/171712 [00:33<16:58, 163.53it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 5185/171712 [00:34<16:44, 165.82it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 5249/171712 [00:34<17:09, 161.77it/s]\u001b[A\n",
      "Chunks:   3%|▊                           | 5313/171712 [00:34<17:26, 158.97it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5377/171712 [00:35<17:36, 157.44it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5441/171712 [00:35<17:43, 156.33it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5505/171712 [00:36<17:18, 160.05it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5569/171712 [00:36<17:03, 162.26it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5633/171712 [00:36<16:55, 163.54it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5697/171712 [00:37<18:28, 149.72it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5761/171712 [00:37<19:32, 141.49it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5825/171712 [00:38<18:20, 150.69it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5889/171712 [00:38<17:15, 160.18it/s]\u001b[A\n",
      "Chunks:   3%|▉                           | 5953/171712 [00:38<15:38, 176.65it/s]\u001b[A\n",
      "Chunks:   4%|▉                           | 6017/171712 [00:39<15:29, 178.35it/s]\u001b[A\n",
      "Chunks:   4%|▉                           | 6081/171712 [00:39<15:49, 174.45it/s]\u001b[A\n",
      "Files:   2%|▋                                 | 20/1000 [00:39<42:45,  2.62s/it]\u001b[A\n",
      "Files:   2%|▋                                 | 21/1000 [00:40<40:09,  2.46s/it]\u001b[A\n",
      "Chunks:   4%|█                            | 6273/171712 [00:41<28:37, 96.34it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6337/171712 [00:42<26:36, 103.56it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6401/171712 [00:42<22:11, 124.12it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6465/171712 [00:42<19:20, 142.38it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6529/171712 [00:43<19:15, 142.93it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6593/171712 [00:43<20:06, 136.83it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6657/171712 [00:44<20:44, 132.59it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6721/171712 [00:44<21:09, 129.92it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6785/171712 [00:45<21:27, 128.09it/s]\u001b[A\n",
      "Chunks:   4%|█                           | 6849/171712 [00:45<21:36, 127.20it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 6913/171712 [00:46<21:46, 126.16it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 6977/171712 [00:46<21:48, 125.86it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7041/171712 [00:47<20:38, 132.92it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7105/171712 [00:47<21:02, 130.37it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7169/171712 [00:48<21:19, 128.63it/s]\u001b[A\n",
      "Files:   2%|▋                                 | 22/1000 [00:49<49:28,  3.03s/it]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7297/171712 [00:49<25:02, 109.42it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7361/171712 [00:50<21:24, 127.91it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7425/171712 [00:50<18:51, 145.15it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7489/171712 [00:50<17:09, 159.50it/s]\u001b[A\n",
      "Chunks:   4%|█▏                          | 7553/171712 [00:51<16:42, 163.74it/s]\u001b[A\n",
      "Files:   2%|▊                                 | 24/1000 [00:51<43:48,  2.69s/it]\u001b[A\n",
      "Chunks:   4%|█▎                          | 7681/171712 [00:51<16:15, 168.19it/s]\u001b[A^C\n"
     ]
    }
   ],
   "source": [
    "!python predict.py \\\n",
    "    --model=FastFit \\\n",
    "    --model-path=./models/level-2/avsolatorio/GIST-large-Embedding-v0 \\\n",
    "    --data-path=../../data/level-1.5/potential.json \\\n",
    "    --output-path=../../results/level-2.5/ \\\n",
    "    --save-top 50 \\\n",
    "    --batch-size 64 \\\n",
    "    --level 2 \\\n",
    "    --timer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m************ TRAINING EMBEDDING MODEL ************\u001b[0m\n",
      "\n",
      "\u001b[1mavsolatorio/GIST-Embedding-v0\u001b[0m\n",
      "\n",
      "\n",
      "Loading data...\n",
      "Data loaded. \n",
      "Splitting data...\n",
      "Data split.\n",
      "\n",
      "Chunking data...\n",
      "100%|███████████████████████████████████| 12310/12310 [00:04<00:00, 2572.59it/s]\n",
      "100%|█████████████████████████████████████| 3848/3848 [00:01<00:00, 2737.74it/s]\n",
      "100%|█████████████████████████████████████| 3078/3078 [00:01<00:00, 2808.82it/s]\n",
      "Data chunked.\n",
      "\n",
      "Downsampling data...\n",
      "Data downsampled.\n",
      "\n",
      "Converting data to FastFit format...\n",
      "\n",
      "Train data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses'],\n",
      "    num_rows: 2122614\n",
      "})\n",
      "\n",
      "Test data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses', '__index_level_0__'],\n",
      "    num_rows: 200\n",
      "})\n",
      "\n",
      "Initialising FastFit trainer...\n",
      "[WARNING  | fastfit.train      ]: Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "tokenizer_config.json: 100%|███████████████| 1.24k/1.24k [00:00<00:00, 4.63MB/s]\n",
      "vocab.txt: 100%|█████████████████████████████| 232k/232k [00:00<00:00, 1.39MB/s]\n",
      "tokenizer.json: 100%|████████████████████████| 711k/711k [00:00<00:00, 2.34MB/s]\n",
      "special_tokens_map.json: 100%|█████████████████| 695/695 [00:00<00:00, 2.92MB/s]\n",
      "config.json: 100%|█████████████████████████████| 747/747 [00:00<00:00, 3.10MB/s]\n",
      "model.safetensors: 100%|█████████████████████| 438M/438M [00:48<00:00, 9.08MB/s]\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset: 100%|█| 2122614/2122614 [31:06<00:00, 1136.99 exam\n",
      "Running tokenizer on dataset: 100%|██| 100/100 [00:00<00:00, 1000.69 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 200/200 [00:00<00:00, 1068.59 examples/s]\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Initialisation complete.\n",
      "\n",
      "\n",
      "Training classifier...\n",
      "  0%|                                                | 0/165830 [00:00<?, ?it/s][WARNING|modeling_utils.py:1239] 2024-08-30 10:46:55,796 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "  0%|                                     | 1/165830 [00:01<91:01:39,  1.98s/it]Traceback (most recent call last):\n",
      "  File \"/GreyLiteratureClassifier/src/scripts/train.py\", line 172, in <module>\n",
      "    main(**vars(args))\n",
      "  File \"/GreyLiteratureClassifier/src/scripts/train.py\", line 149, in main\n",
      "    train_embeddings(**kwargs)\n",
      "  File \"/GreyLiteratureClassifier/src/scripts/train.py\", line 131, in train_embeddings\n",
      "    trainer.train_classifier()\n",
      "  File \"/GreyLiteratureClassifier/src/scripts/FastFit/Trainer.py\", line 65, in train_classifier\n",
      "    self.model = self.trainer.train()\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastfit/train.py\", line 992, in train\n",
      "    train_result = self.trainer.train(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 1948, in train\n",
      "    return inner_training_loop(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 2289, in _inner_training_loop\n",
      "    tr_loss_step = self.training_step(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3328, in training_step\n",
      "    loss = self.compute_loss(model, inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/transformers/trainer.py\", line 3373, in compute_loss\n",
      "    outputs = model(**inputs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1552, in _wrapped_call_impl\n",
      "    return self._call_impl(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/nn/modules/module.py\", line 1561, in _call_impl\n",
      "    return forward_call(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 819, in forward\n",
      "    return model_forward(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/accelerate/utils/operations.py\", line 807, in __call__\n",
      "    return convert_to_fp32(self.model_forward(*args, **kwargs))\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/torch/amp/autocast_mode.py\", line 42, in decorate_autocast\n",
      "    return func(*args, **kwargs)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastfit/modeling.py\", line 586, in forward\n",
      "    sim_loss = self.sim_loss(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastfit/modeling.py\", line 630, in sim_loss\n",
      "    sim_mat = self.query_doc_similarity_matrix(\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastfit/modeling.py\", line 756, in query_doc_similarity_matrix\n",
      "    QQ = self.tokens_similarity(Q, Q, Q_mask, Q_mask)\n",
      "  File \"/usr/local/lib/python3.10/dist-packages/fastfit/modeling.py\", line 783, in tokens_similarity\n",
      "    tokens_sim = tokens_sim * tokens_mask\n",
      "torch.OutOfMemoryError: CUDA out of memory. Tried to allocate 2.00 GiB. GPU 0 has a total capacity of 23.47 GiB of which 1.51 GiB is free. Including non-PyTorch memory, this process has 0 bytes memory in use. Of the allocated memory 19.55 GiB is allocated by PyTorch, and 1.37 GiB is reserved by PyTorch but unallocated. If reserved but unallocated memory is large try setting PYTORCH_CUDA_ALLOC_CONF=expandable_segments:True to avoid fragmentation.  See documentation for Memory Management  (https://pytorch.org/docs/stable/notes/cuda.html#environment-variables)\n",
      "  0%|          | 1/165830 [00:05<234:25:07,  5.09s/it]                          \n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --model FastFit \\\n",
    "    --data-path=../../data/level-0.5/data.json \\\n",
    "    --embedding-model=avsolatorio/GIST-Embedding-v0 \\\n",
    "    --output-path=./models/level-2/ \\\n",
    "    --seed 1347 \\\n",
    "    --chunk-size 512 \\\n",
    "    --batch-size 32 \\\n",
    "    --timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m************ TRAINING EMBEDDING MODEL ************\u001b[0m\n",
      "\n",
      "\u001b[1mavsolatorio/GIST-small-Embedding-v0\u001b[0m\n",
      "\n",
      "\n",
      "Loading data...\n",
      "Data loaded. \n",
      "Splitting data...\n",
      "Data split.\n",
      "\n",
      "Chunking data...\n",
      "100%|███████████████████████████████████| 12310/12310 [00:04<00:00, 2584.53it/s]\n",
      "100%|█████████████████████████████████████| 3848/3848 [00:01<00:00, 2766.63it/s]\n",
      "100%|█████████████████████████████████████| 3078/3078 [00:01<00:00, 2813.99it/s]\n",
      "Data chunked.\n",
      "\n",
      "Downsampling data...\n",
      "Data downsampled.\n",
      "\n",
      "Converting data to FastFit format...\n",
      "\n",
      "Train data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Test data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses', '__index_level_0__'],\n",
      "    num_rows: 200\n",
      "})\n",
      "\n",
      "Initialising FastFit trainer...\n",
      "[WARNING  | fastfit.train      ]: Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset: 100%|█| 2000/2000 [00:01<00:00, 1259.24 examples/s\n",
      "Running tokenizer on dataset: 100%|██| 100/100 [00:00<00:00, 1027.81 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 200/200 [00:00<00:00, 1160.76 examples/s]\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Initialisation complete.\n",
      "\n",
      "\n",
      "Training classifier...\n",
      "  0%|                                                   | 0/160 [00:00<?, ?it/s][WARNING|modeling_utils.py:1239] 2024-08-30 10:48:11,021 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 4.4928, 'grad_norm': 0.6100837588310242, 'learning_rate': 4e-05, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 32/160 [00:12<00:37,  3.42it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.6123046875, 'eval_precision': 0.7167277167277167, 'eval_recall': 0.8736842105263158, 'eval_f1': 0.7695852534562212, 'eval_accuracy': 0.94, 'eval_runtime': 0.4569, 'eval_samples_per_second': 218.846, 'eval_steps_per_second': 4.377, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 32/160 [00:12<00:37,  3.42it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.28it/s]\u001b[A\n",
      "{'loss': 4.2614, 'grad_norm': 0.5135780572891235, 'learning_rate': 3e-05, 'epoch': 2.0}\n",
      " 40%|████████████████▊                         | 64/160 [00:25<00:28,  3.41it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.596138000488281, 'eval_precision': 0.7167277167277167, 'eval_recall': 0.8736842105263158, 'eval_f1': 0.7695852534562212, 'eval_accuracy': 0.94, 'eval_runtime': 0.4678, 'eval_samples_per_second': 213.788, 'eval_steps_per_second': 4.276, 'epoch': 2.0}\n",
      " 40%|████████████████▊                         | 64/160 [00:26<00:28,  3.41it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 17.49it/s]\u001b[A\n",
      "{'loss': 4.2033, 'grad_norm': 0.4088871479034424, 'learning_rate': 2e-05, 'epoch': 3.0}\n",
      " 60%|█████████████████████████▏                | 96/160 [00:38<00:18,  3.42it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.594491481781006, 'eval_precision': 0.8280141843971631, 'eval_recall': 0.8894736842105264, 'eval_f1': 0.8556998556998556, 'eval_accuracy': 0.97, 'eval_runtime': 0.4533, 'eval_samples_per_second': 220.609, 'eval_steps_per_second': 4.412, 'epoch': 3.0}\n",
      " 60%|█████████████████████████▏                | 96/160 [00:39<00:18,  3.42it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.50it/s]\u001b[A\n",
      "{'loss': 4.1641, 'grad_norm': 4.839949131011963, 'learning_rate': 1e-05, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████▊        | 128/160 [00:51<00:09,  3.44it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.585561275482178, 'eval_precision': 0.8280141843971631, 'eval_recall': 0.8894736842105264, 'eval_f1': 0.8556998556998556, 'eval_accuracy': 0.97, 'eval_runtime': 0.4553, 'eval_samples_per_second': 219.643, 'eval_steps_per_second': 4.393, 'epoch': 4.0}\n",
      " 80%|████████████████████████████████▊        | 128/160 [00:52<00:09,  3.44it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.36it/s]\u001b[A\n",
      "{'loss': 4.1335, 'grad_norm': 0.9868386387825012, 'learning_rate': 0.0, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 160/160 [01:05<00:00,  3.43it/s]\n",
      "  0%|                                                     | 0/2 [00:00<?, ?it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.629382610321045, 'eval_precision': 0.7445652173913043, 'eval_recall': 0.8789473684210527, 'eval_f1': 0.7943233237350884, 'eval_accuracy': 0.95, 'eval_runtime': 0.4536, 'eval_samples_per_second': 220.44, 'eval_steps_per_second': 4.409, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 160/160 [01:06<00:00,  3.43it/s]\n",
      "100%|█████████████████████████████████████████████| 2/2 [00:00<00:00, 18.56it/s]\u001b[A\n",
      "{'train_runtime': 67.6922, 'train_samples_per_second': 147.727, 'train_steps_per_second': 2.364, 'train_loss': 4.251014518737793, 'epoch': 5.0}\n",
      "100%|█████████████████████████████████████████| 160/160 [01:07<00:00,  2.36it/s]\n",
      "***** train metrics *****\n",
      "  epoch                    =        5.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =      4.251\n",
      "  train_runtime            = 0:01:07.69\n",
      "  train_samples            =       2000\n",
      "  train_samples_per_second =    147.727\n",
      "  train_steps_per_second   =      2.364\n",
      "\n",
      "Classifier trained.\n",
      "\n",
      "<class 'datasets.arrow_dataset.Dataset'>\n",
      "<class 'list'>\n",
      "Training time on 1148 articles:  68.56031775474548  seconds\n",
      "\n",
      "Files processed per second: 16.744379804461158\n",
      "\n",
      "Testing classifier...\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:00<00:00,  6.85it/s]\n",
      "***** test metrics *****\n",
      "  epoch                   =        5.0\n",
      "  eval_accuracy           =       0.96\n",
      "  eval_f1                 =     0.8464\n",
      "  eval_loss               =     4.7203\n",
      "  eval_precision          =     0.8071\n",
      "  eval_recall             =     0.9007\n",
      "  eval_runtime            = 0:00:00.92\n",
      "  eval_samples_per_second =    215.602\n",
      "  eval_steps_per_second   =      4.312\n",
      "  test_samples            =        200\n",
      "\n",
      "Classifier tested.\n",
      "\n",
      "Testing time on 169 articles:  0.9379348754882812  seconds\n",
      "\n",
      "Files processed per second: 180.1830856454932\n",
      "\n",
      "Saving classifier...\n",
      "\u001b[1mClassifier saved to ./models/level-2/avsolatorio/GIST-small-Embedding-v0.\n",
      "\u001b[0m\n"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --model FastFit \\\n",
    "    --data-path=../../data/level-0.5/data.json \\\n",
    "    --embedding-model=avsolatorio/GIST-small-Embedding-v0 \\\n",
    "    --output-path=./models/level-2/ \\\n",
    "    --seed 1347 \\\n",
    "    --chunk-size 512 \\\n",
    "    --batch-size 64 \\\n",
    "    --samples-per-label 1000 \\\n",
    "    --timer\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "\u001b[1m************ TRAINING EMBEDDING MODEL ************\u001b[0m\n",
      "\n",
      "\u001b[1mavsolatorio/GIST-large-Embedding-v0\u001b[0m\n",
      "\n",
      "\n",
      "Loading data...\n",
      "Data loaded. \n",
      "Splitting data...\n",
      "Data split.\n",
      "\n",
      "Chunking data...\n",
      "100%|███████████████████████████████████| 12310/12310 [00:04<00:00, 2665.81it/s]\n",
      "100%|█████████████████████████████████████| 3848/3848 [00:01<00:00, 2583.50it/s]\n",
      "100%|█████████████████████████████████████| 3078/3078 [00:00<00:00, 3219.69it/s]\n",
      "Data chunked.\n",
      "\n",
      "Downsampling data...\n",
      "Data downsampled.\n",
      "\n",
      "Converting data to FastFit format...\n",
      "\n",
      "Train data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses'],\n",
      "    num_rows: 2000\n",
      "})\n",
      "\n",
      "Test data:\n",
      "Dataset({\n",
      "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses', '__index_level_0__'],\n",
      "    num_rows: 200\n",
      "})\n",
      "\n",
      "Initialising FastFit trainer...\n",
      "[WARNING  | fastfit.train      ]: Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset to infer max length for both query and document: 10\n",
      "Running tokenizer on dataset: 100%|█| 2000/2000 [00:01<00:00, 1313.82 examples/s\n",
      "Running tokenizer on dataset: 100%|███| 100/100 [00:00<00:00, 989.01 examples/s]\n",
      "Running tokenizer on dataset: 100%|██| 200/200 [00:00<00:00, 1131.56 examples/s]\n",
      "/usr/local/lib/python3.10/dist-packages/accelerate/accelerator.py:488: FutureWarning: `torch.cuda.amp.GradScaler(args...)` is deprecated. Please use `torch.amp.GradScaler('cuda', args...)` instead.\n",
      "  self.scaler = torch.cuda.amp.GradScaler(**kwargs)\n",
      "Initialisation complete.\n",
      "\n",
      "\n",
      "Training classifier...\n",
      "  0%|                                                   | 0/315 [00:00<?, ?it/s][WARNING|modeling_utils.py:1239] 2024-08-30 10:50:49,938 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n",
      "{'loss': 3.7034, 'grad_norm': 3.995910167694092, 'learning_rate': 4.015873015873016e-05, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 63/315 [00:54<03:08,  1.34it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  4.03it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:00<00:00,  2.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.105900764465332, 'eval_precision': 0.5833333333333334, 'eval_recall': 0.9747474747474747, 'eval_f1': 0.6299037749814952, 'eval_accuracy': 0.95, 'eval_runtime': 1.7322, 'eval_samples_per_second': 57.73, 'eval_steps_per_second': 2.309, 'epoch': 1.0}\n",
      " 20%|████████▍                                 | 63/315 [00:56<03:08,  1.34it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      "{'loss': 3.4803, 'grad_norm': 0.0798908993601799, 'learning_rate': 3.0158730158730158e-05, 'epoch': 2.0}\n",
      " 40%|████████████████▍                        | 126/315 [02:01<02:21,  1.33it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  4.01it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:00<00:00,  2.95it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.223165512084961, 'eval_precision': 0.5625, 'eval_recall': 0.9646464646464646, 'eval_f1': 0.5927865037812682, 'eval_accuracy': 0.93, 'eval_runtime': 1.7321, 'eval_samples_per_second': 57.735, 'eval_steps_per_second': 2.309, 'epoch': 2.0}\n",
      " 40%|████████████████▍                        | 126/315 [02:03<02:21,  1.33it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.63it/s]\u001b[A\n",
      "{'loss': 3.4509, 'grad_norm': 0.013715633191168308, 'learning_rate': 2.015873015873016e-05, 'epoch': 3.0}\n",
      " 60%|████████████████████████▌                | 189/315 [03:08<01:34,  1.33it/s]\n",
      "  0%|                                                     | 0/4 [00:00<?, ?it/s]\u001b[A\n",
      " 50%|██████████████████████▌                      | 2/4 [00:00<00:00,  4.00it/s]\u001b[A\n",
      " 75%|█████████████████████████████████▊           | 3/4 [00:00<00:00,  2.94it/s]\u001b[A\n",
      "                                                                                \u001b[A\n",
      "\u001b[A{'eval_loss': 4.3070454597473145, 'eval_precision': 0.5833333333333334, 'eval_recall': 0.9747474747474747, 'eval_f1': 0.6299037749814952, 'eval_accuracy': 0.95, 'eval_runtime': 1.7358, 'eval_samples_per_second': 57.611, 'eval_steps_per_second': 2.304, 'epoch': 3.0}\n",
      " 60%|████████████████████████▌                | 189/315 [03:10<01:34,  1.33it/s]\n",
      "100%|█████████████████████████████████████████████| 4/4 [00:01<00:00,  3.62it/s]\u001b[A\n",
      " 68%|███████████████████████████▉             | 215/315 [03:47<01:40,  1.01s/it]\u001b[A"
     ]
    }
   ],
   "source": [
    "!python train.py \\\n",
    "    --model FastFit \\\n",
    "    --data-path=../../data/level-0.5/data.json \\\n",
    "    --embedding-model=avsolatorio/GIST-large-Embedding-v0 \\\n",
    "    --output-path=./models/level-2/ \\\n",
    "    --chunk-size 512 \\\n",
    "    --batch-size 32 \\\n",
    "    --samples-per-label 1000 \\\n",
    "    --timer"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# %load_ext cudf.pandas\n",
    "# import pandas as pd\n",
    "import cudf as pd\n",
    "from tqdm.auto import tqdm,trange\n",
    "import torch"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/workspace/GreyLit/venv/lib/python3.10/site-packages/cudf/io/json.py:108: UserWarning: Using CPU via Pandas to read JSON dataset, this may be GPU accelerated in the future\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data loaded.\n",
      "<class 'cudf.core.dataframe.DataFrame'>\n",
      "RangeIndex: 19284 entries, 0 to 19283\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype\n",
      "---  ------        --------------  -----\n",
      " 0   url           19284 non-null  object\n",
      " 1   text          19284 non-null  object\n",
      " 2   relevance     19284 non-null  object\n",
      " 3   multiclasses  19284 non-null  list\n",
      "dtypes: list(1), object(3)\n",
      "memory usage: 1.3+ GB\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>multiclasses</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://mglsd.go.ug/wp-content/uploads/2023/07...</td>\n",
       "      <td>THE REPUBLIC OF UGANDA MINISTRY OF GENDER, LAB...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>Plant new hedges Provide or retain set‐aside a...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Insects]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>http://www.world-psi.org/sites/default/files/d...</td>\n",
       "      <td>RAPPORT SPÉCIAL DE LA PSI : l’Accord sur le co...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://yanceyfamilygenealogy.org/Family_Bible...</td>\n",
       "      <td>The Family Bible Preservation Project's - Fami...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://budget.finance.go.ug/sites/default/fil...</td>\n",
       "      <td>LG Draft Budget Estimates 2024/25 VOTE: 609 Mb...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://mglsd.go.ug/wp-content/uploads/2023/07...   \n",
       "1  https://www.conservationevidence.com/individua...   \n",
       "2  http://www.world-psi.org/sites/default/files/d...   \n",
       "3  https://yanceyfamilygenealogy.org/Family_Bible...   \n",
       "4  https://budget.finance.go.ug/sites/default/fil...   \n",
       "\n",
       "                                                text   relevance multiclasses  \n",
       "0  THE REPUBLIC OF UGANDA MINISTRY OF GENDER, LAB...  irrelevant           []  \n",
       "1  Plant new hedges Provide or retain set‐aside a...    relevant    [Insects]  \n",
       "2  RAPPORT SPÉCIAL DE LA PSI : l’Accord sur le co...  irrelevant           []  \n",
       "3  The Family Bible Preservation Project's - Fami...  irrelevant           []  \n",
       "4  LG Draft Budget Estimates 2024/25 VOTE: 609 Mb...  irrelevant           []  "
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/labelled/data.json\", group_relevant=True):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    if group_relevant:\n",
    "        data[\"class\"] = data[\"class\"].apply(\n",
    "            lambda x: \"relevant\" if x != \"irrelevant\" else x\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "data = import_labelled_data(\n",
    "    path=\"../../data/level-0.5/data.json\", group_relevant=False\n",
    ")\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "# data = data.sample(2500)\n",
    "\n",
    "data.info()\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train_test_split(x, y=None, test_size=0.2,shuffle=False):\n",
    "    split = int(len(x) * (1 - test_size))\n",
    "    xTrain, xTest = x.iloc[:split], x.iloc[split:]\n",
    "    if y is not None:\n",
    "        yTrain, yTest = y.iloc[:split], y.iloc[split:]\n",
    "        return xTrain, xTest, yTrain, yTest\n",
    "    return xTrain, xTest\n",
    "\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data[\"text\"], data[\"relevance\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from sklearn.feature_extraction.text import HashingVectorizer\n",
    "from cuml.feature_extraction.text import HashingVectorizer\n",
    "\n",
    "vectorizer = HashingVectorizer(\n",
    "    stop_words=\"english\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.39 ms ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Vectorization took 0.00 seconds.\n",
      "Vectorization of the whole dataset is estimated to take 0.27 seconds.\n"
     ]
    }
   ],
   "source": [
    "time = %timeit -n1 -r1 -o vectorizer.fit(data['text'].iloc[:100])\n",
    "\n",
    "print(f\"Vectorization took {time.average:.2f} seconds.\")\n",
    "print(f\"Vectorization of the whole dataset is estimated to take {time.average * len(data) / 100:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "33.3 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Vectorization took 33.29 seconds.\n",
      "Vectorization of the whole dataset is estimated to take 3209.45 seconds.\n"
     ]
    }
   ],
   "source": [
    "time = %timeit -n1 -r1 -o vectorizer.transform(data['text'].iloc[:200])\n",
    "\n",
    "print(f\"Vectorization took {time.average:.2f} seconds.\")\n",
    "print(f\"Vectorization of the whole dataset is estimated to take {time.average * len(data) / 200:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n",
      "Fitting vectorizer...\n",
      "Vectorizer fit.\n",
      "Vectorizing data...\n",
      "Training data vectorized.\n",
      "Testing data vectorized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Vectorizing data...\")\n",
    "\n",
    "#* took 1min30s in cudf+cuml, then up to 1min40 for yTrain\n",
    "#* and 1min13s in pandas+cuml, then up to 1min47 for yTrain\n",
    "\n",
    "vectorizer = HashingVectorizer(\n",
    "    stop_words=\"english\"\n",
    ")\n",
    "\n",
    "print(\"Fitting vectorizer...\")\n",
    "vectorizer.fit(data[\"text\"])\n",
    "print(\"Vectorizer fit.\")\n",
    "\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "xTrainVector = vectorizer.transform(xTrain)\n",
    "print(\"Training data vectorized.\")\n",
    "xTestVector = vectorizer.transform(xTest)\n",
    "print(\"Testing data vectorized.\")\n",
    "\n",
    "yTrainVector = (yTrain=='relevant').astype(int)\n",
    "yTestVector = (yTest=='relevant').astype(int)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(15427, 1048576)"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "xTrainVector.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "from LazyTextDataset import LazyTextDataset\n",
    "\n",
    "train_dataset = LazyTextDataset(xTrainVector, yTrainVector,device='cuda:0')\n",
    "test_dataset = LazyTextDataset(xTestVector, yTestVector,device='cuda:0')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "if vectorizer.__class__.__name__ == \"HashingVectorizer\":\n",
    "    VOCAB_SIZE = vectorizer.n_features\n",
    "else: # tfidf or count\n",
    "    VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
    "\n",
    "NUM_CLASSES = len(data[\"relevance\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "torch.multiprocessing.set_start_method('spawn')\n",
    "torch.set_default_device('cuda:0')\n",
    "torch.cuda.set_device(0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader\n",
    "\n",
    "def sparse_collate_fn(batch):\n",
    "    print(batch)\n",
    "    x = torch.stack([x for x, y in batch])\n",
    "    y = torch.stack([y for x, y in batch])\n",
    "    return x, y\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=128, collate_fn=sparse_collate_fn, shuffle=True,generator=torch.Generator(device='cuda'),num_workers=0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionClassifier(\n",
      "  (linear): Linear(in_features=1048576, out_features=5, bias=True)\n",
      "  (linear2): Linear(in_features=5, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "from LogisticRegressionClassifier import LogisticRegressionClassifier\n",
    "\n",
    "model = LogisticRegressionClassifier(vocab_size=VOCAB_SIZE,hidden_dim=5)\n",
    "model.to('cuda:0')\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.BCEWithLogitsLoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "6037ee0d056041da99e374d28f8e01e8",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "616d809817d44bf48bc7c9f20206f834",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/25:   0%|          | 0/121 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[(tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  17987,   44647,   56144,   59409,   62855,   70993,\n",
      "                          72242,   83224,  118442,  129889,  172875,  180823,\n",
      "                         192373,  202578,  241976,  244234,  260032,  261287,\n",
      "                         261426,  273260,  274879,  299161,  302597,  306588,\n",
      "                         331110,  333066,  341769,  385698,  416655,  451849,\n",
      "                         453171,  463390,  518126,  533850,  560470,  562857,\n",
      "                         579331,  611746,  617166,  634572,  643213,  649727,\n",
      "                         666308,  688111,  691802,  778214,  778857,  781946,\n",
      "                         859593,  861893,  866718,  873283,  885193,  891788,\n",
      "                         894091,  898813,  914583,  973290,  982773,  985691,\n",
      "                         990225,  995598, 1031473]]),\n",
      "       values=tensor([0.0400, 0.0801, 0.0801, 0.2402, 0.0400, 0.0801, 0.0400,\n",
      "                      0.0801, 0.0801, 0.2802, 0.0400, 0.0801, 0.0801, 0.0801,\n",
      "                      0.0400, 0.0801, 0.0801, 0.0400, 0.2402, 0.3203, 0.3203,\n",
      "                      0.0801, 0.0801, 0.0801, 0.0801, 0.0801, 0.0801, 0.0801,\n",
      "                      0.2402, 0.1601, 0.0801, 0.0400, 0.0801, 0.0801, 0.0801,\n",
      "                      0.0801, 0.0801, 0.0801, 0.0801, 0.0801, 0.0801, 0.2402,\n",
      "                      0.0801, 0.0400, 0.0801, 0.1601, 0.1601, 0.2402, 0.0801,\n",
      "                      0.0801, 0.0801, 0.2402, 0.1601, 0.0801, 0.0801, 0.0801,\n",
      "                      0.0801, 0.0400, 0.0400, 0.0801, 0.0801, 0.0801, 0.0400]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=63, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   2623,    5415,    6641,  ..., 1045733, 1047825,\n",
      "                        1048572]]),\n",
      "       values=tensor([0.2139, 0.0153, 0.0038, 0.0038, 0.0038, 0.2674, 0.0038,\n",
      "                      0.0115, 0.0038, 0.0726, 0.0191, 0.0115, 0.0076, 0.0115,\n",
      "                      0.0038, 0.0038, 0.0191, 0.0038, 0.0076, 0.0038, 0.0038,\n",
      "                      0.4049, 0.0038, 0.0267, 0.0038, 0.0076, 0.0497, 0.0115,\n",
      "                      0.0038, 0.0038, 0.0038, 0.1604, 0.0153, 0.0993, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0153, 0.0191, 0.0115, 0.0038,\n",
      "                      0.0267, 0.0115, 0.0076, 0.0038, 0.0038, 0.1337, 0.0115,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0229, 0.0917, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0306, 0.0267, 0.0115, 0.0076, 0.0191, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0267, 0.0153, 0.0267, 0.0038, 0.0153,\n",
      "                      0.0076, 0.0038, 0.0115, 0.0076, 0.0153, 0.0076, 0.0038,\n",
      "                      0.0115, 0.0076, 0.0038, 0.0076, 0.0038, 0.0153, 0.0267,\n",
      "                      0.0726, 0.0076, 0.0038, 0.1490, 0.1375, 0.0076, 0.0038,\n",
      "                      0.0153, 0.0076, 0.0115, 0.0038, 0.0038, 0.0038, 0.0076,\n",
      "                      0.0076, 0.0229, 0.0076, 0.0153, 0.0038, 0.0420, 0.0038,\n",
      "                      0.0115, 0.0038, 0.0038, 0.0038, 0.0038, 0.0038, 0.0458,\n",
      "                      0.0382, 0.0038, 0.0076, 0.0229, 0.1070, 0.0038, 0.0076,\n",
      "                      0.0076, 0.0229, 0.0038, 0.0038, 0.0038, 0.0076, 0.0153,\n",
      "                      0.1413, 0.0076, 0.0038, 0.0153, 0.0076, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0153, 0.0038, 0.0038, 0.0038, 0.0038, 0.0076,\n",
      "                      0.0153, 0.0076, 0.0038, 0.0115, 0.0191, 0.1108, 0.0038,\n",
      "                      0.0153, 0.0038, 0.0038, 0.0306, 0.0115, 0.0038, 0.0076,\n",
      "                      0.0879, 0.0038, 0.0076, 0.0038, 0.0229, 0.0038, 0.0038,\n",
      "                      0.0573, 0.0191, 0.0076, 0.0497, 0.0076, 0.0038, 0.0038,\n",
      "                      0.0153, 0.0038, 0.0344, 0.0191, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0038, 0.0153, 0.0038,\n",
      "                      0.0153, 0.0115, 0.0115, 0.0115, 0.0153, 0.0038, 0.0229,\n",
      "                      0.0038, 0.0038, 0.0076, 0.0038, 0.0229, 0.0153, 0.0076,\n",
      "                      0.0076, 0.0038, 0.0115, 0.0267, 0.0076, 0.0611, 0.0229,\n",
      "                      0.0038, 0.0038, 0.0076, 0.0038, 0.0038, 0.0038, 0.0076,\n",
      "                      0.0535, 0.0191, 0.0038, 0.1413, 0.0153, 0.0076, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0038, 0.0076, 0.0115,\n",
      "                      0.0038, 0.0076, 0.0076, 0.0382, 0.1490, 0.0229, 0.0306,\n",
      "                      0.0115, 0.0115, 0.0038, 0.0802, 0.0229, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0038, 0.2980, 0.0038, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0153, 0.0458, 0.0344, 0.0306, 0.0038, 0.0115, 0.0038,\n",
      "                      0.0038, 0.0076, 0.1910, 0.0038, 0.0153, 0.0076, 0.1070,\n",
      "                      0.0688, 0.0038, 0.0115, 0.0115, 0.0076, 0.0038, 0.0076,\n",
      "                      0.0229, 0.0038, 0.0038, 0.0076, 0.0076, 0.0382, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0382, 0.0229, 0.0038,\n",
      "                      0.0038, 0.0306, 0.0038, 0.0038, 0.0573, 0.0038, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0458, 0.0115, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0153, 0.0076, 0.0038, 0.0993, 0.0153, 0.0076, 0.0535,\n",
      "                      0.0038, 0.0535, 0.0153, 0.0038, 0.0153, 0.0115, 0.0076,\n",
      "                      0.0038, 0.0306, 0.0038, 0.0038, 0.0153, 0.0038, 0.0115,\n",
      "                      0.0038, 0.0076, 0.1910, 0.0382, 0.0153, 0.0038, 0.0076,\n",
      "                      0.0115, 0.0535, 0.0038, 0.0153, 0.0115, 0.0038, 0.0076,\n",
      "                      0.0229, 0.0076, 0.0076, 0.0038, 0.0076, 0.0038, 0.0076,\n",
      "                      0.0038, 0.0076, 0.0153, 0.0115, 0.1452, 0.0038, 0.0038,\n",
      "                      0.0115, 0.0764, 0.0076, 0.0038, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0153, 0.0076, 0.0115, 0.0038, 0.0573, 0.1413, 0.0611,\n",
      "                      0.0076, 0.0038, 0.0038, 0.0038, 0.0038, 0.0153, 0.0153,\n",
      "                      0.0076, 0.0115, 0.0076, 0.0076, 0.0306, 0.0038, 0.0306,\n",
      "                      0.0038, 0.0115, 0.0038, 0.0191, 0.0076, 0.0038, 0.0229,\n",
      "                      0.0497, 0.0458, 0.0076, 0.0038, 0.0038, 0.0611, 0.0038,\n",
      "                      0.0038, 0.0573, 0.0344, 0.0229, 0.0076, 0.0076, 0.0038,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0229, 0.0649, 0.0038, 0.0038,\n",
      "                      0.0649, 0.0038, 0.0306, 0.0420, 0.0038, 0.0038, 0.0076,\n",
      "                      0.0076, 0.0038, 0.0038, 0.0038, 0.0076, 0.0038, 0.0038,\n",
      "                      0.0649, 0.0038, 0.0076, 0.0229, 0.0076, 0.0153, 0.0076,\n",
      "                      0.0076, 0.0076, 0.0115, 0.0076, 0.0038, 0.0038, 0.0115,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0076, 0.0115, 0.0229, 0.0038,\n",
      "                      0.0267, 0.0153, 0.0115, 0.0153, 0.0076, 0.1261, 0.0038,\n",
      "                      0.0038, 0.0191, 0.0115, 0.0038, 0.0191, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0191, 0.0076, 0.0076, 0.0038, 0.0038, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0115, 0.0076, 0.0038, 0.0038, 0.0611,\n",
      "                      0.0306, 0.0153, 0.1146, 0.0382, 0.0076, 0.0076, 0.0038,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0076, 0.0076, 0.0038, 0.0153,\n",
      "                      0.0076, 0.0038, 0.0038, 0.0038, 0.0076, 0.0038, 0.0038,\n",
      "                      0.0153, 0.0038, 0.0076, 0.0153, 0.0535, 0.0038, 0.0115,\n",
      "                      0.0115, 0.0038, 0.0076, 0.0076, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0076, 0.0115, 0.0038, 0.0076, 0.0076, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0191, 0.0076, 0.0076, 0.0191, 0.0076, 0.0038,\n",
      "                      0.0153, 0.0038, 0.0038, 0.0153, 0.0649, 0.0076, 0.0038,\n",
      "                      0.0038, 0.0038, 0.0535, 0.0038, 0.0229, 0.0038, 0.0726,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0038, 0.2368, 0.0153,\n",
      "                      0.0038, 0.0879, 0.0038, 0.0038, 0.0076, 0.0191, 0.0076,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0306, 0.1146, 0.0038,\n",
      "                      0.0153, 0.0076, 0.0038, 0.0038, 0.0038, 0.0038, 0.0038,\n",
      "                      0.0038, 0.0038, 0.0038, 0.0038, 0.0076, 0.0115, 0.0344,\n",
      "                      0.0076, 0.0420, 0.0076, 0.0038, 0.0726, 0.0038, 0.0038]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=567, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [  13196,   13205,   27956,   31906,   34211,   39838,\n",
      "                          59409,   71032,   76580,   79765,   86889,   93119,\n",
      "                          99833,  102865,  104712,  124763,  128430,  133013,\n",
      "                         134541,  134676,  148410,  148594,  152350,  152416,\n",
      "                         155011,  155319,  160635,  172293,  172527,  173748,\n",
      "                         173941,  180414,  182887,  185628,  186346,  200093,\n",
      "                         202578,  206475,  213222,  216382,  219696,  220579,\n",
      "                         222184,  222373,  253179,  260032,  265144,  268438,\n",
      "                         279355,  284011,  286447,  299655,  300592,  311111,\n",
      "                         313131,  330208,  331110,  333927,  335594,  338413,\n",
      "                         342567,  344946,  345617,  349680,  356327,  356647,\n",
      "                         370795,  370827,  377888,  378098,  379247,  380201,\n",
      "                         381013,  385892,  387429,  387864,  392739,  402535,\n",
      "                         412708,  429014,  432391,  433291,  441925,  444736,\n",
      "                         455007,  457593,  463258,  464609,  468659,  475392,\n",
      "                         482293,  483314,  487622,  489141,  489375,  494507,\n",
      "                         503308,  513664,  517024,  517232,  528632,  530812,\n",
      "                         536463,  541775,  548553,  549376,  551646,  561849,\n",
      "                         561926,  564511,  565561,  566251,  586944,  587173,\n",
      "                         588395,  594744,  604136,  604812,  607058,  613616,\n",
      "                         619417,  621384,  623618,  627601,  628143,  631554,\n",
      "                         631827,  635910,  636528,  638894,  640251,  650193,\n",
      "                         659143,  663846,  666657,  668510,  670091,  670165,\n",
      "                         679168,  679464,  680788,  682045,  686373,  688983,\n",
      "                         692053,  706231,  709318,  710615,  711627,  712954,\n",
      "                         716889,  726481,  729497,  739074,  739778,  741769,\n",
      "                         750395,  765371,  767345,  767686,  771135,  772187,\n",
      "                         776834,  778188,  782068,  784179,  789080,  794144,\n",
      "                         798860,  799928,  802214,  807003,  810818,  812069,\n",
      "                         815053,  818259,  821862,  824273,  825495,  830727,\n",
      "                         848005,  850599,  853005,  853665,  859752,  859812,\n",
      "                         867441,  868411,  873938,  874639,  879401,  884866,\n",
      "                         887504,  891788,  892458,  892822,  895123,  898740,\n",
      "                         913074,  915333,  916617,  916983,  917153,  936389,\n",
      "                         938177,  948638,  952543,  955511,  960803,  962419,\n",
      "                         963031,  969771,  970513,  971651,  972295,  975280,\n",
      "                         978068,  982395,  989457,  990237,  995135,  995585,\n",
      "                         999238, 1002674, 1013264, 1025814, 1027345, 1030372,\n",
      "                        1030485, 1031297, 1035735, 1037885, 1039313, 1041634]]),\n",
      "       values=tensor([0.0272, 0.0545, 0.0272, 0.0272, 0.0272, 0.0545, 0.0272,\n",
      "                      0.0272, 0.2180, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0545, 0.0272, 0.0272, 0.1635, 0.0272,\n",
      "                      0.1635, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.1635, 0.0545, 0.0272, 0.0272, 0.1635, 0.0272, 0.0272,\n",
      "                      0.0272, 0.2452, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0817,\n",
      "                      0.0545, 0.0817, 0.0272, 0.0272, 0.0272, 0.0272, 0.0817,\n",
      "                      0.0545, 0.1362, 0.0272, 0.0272, 0.0272, 0.0817, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0545, 0.0272, 0.0272,\n",
      "                      0.0817, 0.0272, 0.0272, 0.0272, 0.0272, 0.0545, 0.0272,\n",
      "                      0.0272, 0.0545, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0545, 0.0272, 0.0272, 0.0817, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0545, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.1090, 0.0272, 0.0817,\n",
      "                      0.0272, 0.0545, 0.0272, 0.0545, 0.0272, 0.0817, 0.0817,\n",
      "                      0.0545, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.3815, 0.0272, 0.0545, 0.0272, 0.0272,\n",
      "                      0.0545, 0.0272, 0.0272, 0.0545, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0817, 0.0817, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0817, 0.0545, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.1362, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0817, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0545, 0.0817, 0.2180, 0.0272, 0.0272,\n",
      "                      0.0272, 0.0272, 0.3815, 0.0272, 0.0272, 0.1635, 0.0272,\n",
      "                      0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272, 0.0272,\n",
      "                      0.0545, 0.0272, 0.0272, 0.0272, 0.0545, 0.2452, 0.0272,\n",
      "                      0.0272, 0.0545, 0.0272, 0.0545, 0.0272, 0.0272, 0.0545,\n",
      "                      0.0272, 0.0272, 0.0545, 0.0545, 0.0272, 0.0272, 0.0545,\n",
      "                      0.0272, 0.0545, 0.0545]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=234, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   1592,   13196,   13205,   14962,   72242,   79753,\n",
      "                          83224,  100631,  119937,  122534,  134541,  138055,\n",
      "                         147803,  158118,  183122,  185618,  202578,  220450,\n",
      "                         223495,  236858,  237464,  261287,  274000,  289799,\n",
      "                         302597,  321113,  323109,  400949,  452941,  463390,\n",
      "                         533850,  581096,  654930,  662515,  666813,  688111,\n",
      "                         699311,  762342,  782597,  796902,  845609,  887177,\n",
      "                         891788,  899662,  955129,  957170,  967444,  982041,\n",
      "                         982773, 1001248, 1031773]]),\n",
      "       values=tensor([0.0956, 0.0956, 0.0956, 0.0956, 0.0478, 0.0956, 0.0956,\n",
      "                      0.0956, 0.0956, 0.0956, 0.1911, 0.2867, 0.1911, 0.1911,\n",
      "                      0.0956, 0.0956, 0.0956, 0.0956, 0.0956, 0.0956, 0.0956,\n",
      "                      0.1433, 0.4778, 0.0956, 0.0956, 0.1911, 0.0956, 0.0956,\n",
      "                      0.0956, 0.0478, 0.0956, 0.0956, 0.0956, 0.0956, 0.1911,\n",
      "                      0.0478, 0.0956, 0.1911, 0.1911, 0.0478, 0.0956, 0.0956,\n",
      "                      0.0956, 0.0956, 0.0956, 0.2867, 0.0956, 0.0956, 0.0478,\n",
      "                      0.0956, 0.0956]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=51, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [   4993,   13196,   13666,   19433,   72242,   82618,\n",
      "                          83339,   86356,   93216,   94446,   95531,  102865,\n",
      "                         109992,  116557,  119937,  131210,  134541,  160635,\n",
      "                         171719,  179579,  218949,  244038,  256425,  260450,\n",
      "                         261287,  270167,  285570,  287022,  289350,  293038,\n",
      "                         311856,  457113,  463390,  478084,  483567,  487857,\n",
      "                         492068,  513664,  555935,  608527,  610959,  611672,\n",
      "                         619417,  627191,  664073,  666856,  667368,  676606,\n",
      "                         688111,  688676,  690661,  711543,  728086,  754424,\n",
      "                         797633,  810044,  822200,  822945,  847848,  849899,\n",
      "                         878520,  881252,  909456,  973101,  979016,  982773,\n",
      "                         995585, 1005750, 1007574, 1033635]]),\n",
      "       values=tensor([0.0530, 0.0530, 0.0530, 0.0530, 0.1060, 0.0530, 0.0530,\n",
      "                      0.1060, 0.0530, 0.0530, 0.4770, 0.0530, 0.0530, 0.0530,\n",
      "                      0.2120, 0.1060, 0.0530, 0.0530, 0.0530, 0.0530, 0.1590,\n",
      "                      0.0530, 0.0530, 0.0530, 0.1060, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.1060, 0.3710, 0.2120, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.1060, 0.0530, 0.0530, 0.0530, 0.0530, 0.1590,\n",
      "                      0.0530, 0.1590, 0.0530, 0.0530, 0.1060, 0.1590, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.1060, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.3710, 0.0530, 0.0530, 0.0530, 0.2650]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=70, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4634,   10428,   13434,   19433,   30079,   31339,\n",
      "                          36053,   43212,   49115,   51509,   55764,   58737,\n",
      "                          59409,   60810,   68267,   71501,   86889,   88435,\n",
      "                          98582,   99366,  100742,  104403,  109690,  111087,\n",
      "                         113783,  118575,  120060,  122688,  124438,  138883,\n",
      "                         139627,  142720,  145228,  151344,  155441,  159360,\n",
      "                         163727,  164600,  164964,  172475,  172529,  174436,\n",
      "                         177319,  177590,  183122,  189998,  190055,  195066,\n",
      "                         207087,  214816,  216661,  223620,  227819,  231953,\n",
      "                         233752,  235279,  236418,  243399,  250128,  256142,\n",
      "                         256417,  259348,  267074,  270545,  277274,  277545,\n",
      "                         278631,  281957,  284011,  287194,  288082,  288502,\n",
      "                         296127,  297337,  307303,  318112,  322650,  330658,\n",
      "                         341771,  344134,  346500,  349596,  350158,  351284,\n",
      "                         358903,  364184,  365870,  369407,  376638,  379610,\n",
      "                         385592,  387529,  387975,  389282,  390172,  390537,\n",
      "                         397384,  402535,  408591,  418185,  420104,  428247,\n",
      "                         440835,  456374,  456592,  457728,  460508,  463532,\n",
      "                         476318,  479373,  487030,  489861,  498539,  505043,\n",
      "                         506462,  508654,  513664,  518585,  523658,  534719,\n",
      "                         542414,  555254,  562081,  564039,  565760,  568431,\n",
      "                         592316,  592418,  592652,  594372,  598357,  598741,\n",
      "                         606855,  607058,  613709,  614373,  622022,  623753,\n",
      "                         626799,  627392,  630779,  631674,  638428,  639114,\n",
      "                         649636,  652127,  659405,  664752,  668251,  669723,\n",
      "                         670113,  673390,  673543,  675598,  675973,  678169,\n",
      "                         680788,  682190,  688574,  688890,  691802,  692053,\n",
      "                         693649,  718125,  726541,  732556,  732961,  737566,\n",
      "                         740730,  746855,  748916,  754424,  760118,  767413,\n",
      "                         771654,  781220,  785022,  785883,  789996,  792408,\n",
      "                         800951,  805662,  806225,  809193,  809718,  821145,\n",
      "                         823851,  835473,  844679,  846332,  851931,  863286,\n",
      "                         866696,  875460,  876033,  880722,  887020,  893467,\n",
      "                         898260,  898740,  900647,  901430,  908405,  909450,\n",
      "                         911715,  912690,  917696,  919479,  922873,  926952,\n",
      "                         927833,  929081,  929268,  936007,  943599,  946039,\n",
      "                         948143,  948851,  949320,  968513,  980439,  981247,\n",
      "                         982395,  983027,  986861, 1000069, 1000438, 1007989,\n",
      "                        1008851, 1020063, 1024954, 1036359, 1043325]]),\n",
      "       values=tensor([0.0318, 0.0318, 0.0318, 0.0318, 0.0636, 0.0318, 0.0318,\n",
      "                      0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0636, 0.3500, 0.0318, 0.0318, 0.0636, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0954, 0.0318, 0.0318, 0.0954,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0636, 0.0636, 0.0318, 0.1273,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0636, 0.0636, 0.0318, 0.0636,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.1273, 0.0318, 0.0954, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0636, 0.0636, 0.0318, 0.0636, 0.0954,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.1909, 0.0318, 0.0636,\n",
      "                      0.0318, 0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0954, 0.0318, 0.0318, 0.1591, 0.0318, 0.0318,\n",
      "                      0.0318, 0.1273, 0.0318, 0.0318, 0.2545, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0636, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0636, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0954, 0.0318, 0.0954, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.1273,\n",
      "                      0.0318, 0.0954, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0954, 0.0318, 0.0636, 0.0318,\n",
      "                      0.0318, 0.2227, 0.0954, 0.0636, 0.0318, 0.0318, 0.1909,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0636, 0.0318, 0.0318, 0.0318, 0.4136, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0636,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0636, 0.0318,\n",
      "                      0.0318, 0.0636, 0.0318, 0.0318, 0.0318, 0.0318, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0636, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0636, 0.0318,\n",
      "                      0.0318, 0.0318, 0.0318, 0.0318, 0.0318, 0.0954, 0.0318,\n",
      "                      0.1591, 0.0318]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=233, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   3111,    7251,   15482,   16903,   18117,   22508,\n",
      "                          26160,   26233,   26624,   28933,   30566,   36588,\n",
      "                          39596,   40327,   43797,   52522,   55941,   56144,\n",
      "                          60550,   60651,   61657,   64187,   67409,   69302,\n",
      "                          70929,   78083,   83126,   88437,   88497,   88530,\n",
      "                          88661,   89986,   93996,   95131,   99903,  102812,\n",
      "                         102865,  105271,  108799,  108824,  114884,  121563,\n",
      "                         121811,  122688,  126850,  129965,  131533,  131706,\n",
      "                         132070,  135978,  137895,  138249,  139980,  141362,\n",
      "                         144995,  147277,  147730,  155163,  161918,  166200,\n",
      "                         166598,  172544,  174436,  176356,  177421,  180827,\n",
      "                         181182,  183122,  185169,  185628,  190055,  193963,\n",
      "                         194937,  195557,  196095,  201625,  204537,  207111,\n",
      "                         207615,  213145,  214435,  214591,  222499,  222984,\n",
      "                         228376,  230177,  231439,  233090,  240459,  250486,\n",
      "                         250596,  251800,  253138,  253881,  254032,  256948,\n",
      "                         258813,  261850,  264061,  264942,  266056,  268797,\n",
      "                         276988,  281691,  287627,  289425,  290493,  293952,\n",
      "                         294815,  295735,  297814,  299608,  305872,  308848,\n",
      "                         310870,  311048,  314106,  314754,  322552,  326148,\n",
      "                         326661,  327033,  327219,  328173,  328958,  330386,\n",
      "                         331110,  331634,  332221,  333939,  334076,  339624,\n",
      "                         339941,  342267,  344166,  354229,  355353,  363541,\n",
      "                         364304,  364511,  366210,  366862,  373665,  373756,\n",
      "                         377615,  377670,  378098,  378312,  379610,  381013,\n",
      "                         383527,  388877,  388891,  390701,  394617,  397252,\n",
      "                         401339,  402553,  406157,  408903,  409697,  412833,\n",
      "                         416552,  420315,  421316,  429958,  430610,  432823,\n",
      "                         439019,  444887,  449975,  454518,  454602,  454961,\n",
      "                         463390,  464609,  465335,  471904,  472362,  472582,\n",
      "                         479398,  479754,  489141,  489172,  494507,  499325,\n",
      "                         499636,  503168,  503718,  506047,  509876,  511454,\n",
      "                         512656,  514090,  518985,  519037,  522112,  522633,\n",
      "                         524685,  529736,  530108,  536772,  539111,  542971,\n",
      "                         543152,  546572,  548668,  555853,  562041,  572693,\n",
      "                         579342,  579621,  581475,  583699,  583981,  588833,\n",
      "                         592382,  593657,  593881,  595079,  595335,  599350,\n",
      "                         600464,  603221,  607524,  608037,  614178,  615150,\n",
      "                         615761,  617100,  621371,  625167,  634220,  637253,\n",
      "                         637706,  638684,  638794,  639342,  641547,  647082,\n",
      "                         648833,  651389,  653745,  664290,  665539,  665745,\n",
      "                         665760,  665941,  667368,  668705,  672398,  673805,\n",
      "                         674175,  674520,  676458,  682190,  688880,  691412,\n",
      "                         697244,  697515,  697745,  699490,  701891,  704222,\n",
      "                         704865,  710506,  713540,  715340,  715817,  716856,\n",
      "                         717521,  717905,  723685,  725407,  725426,  729684,\n",
      "                         730969,  737008,  738006,  743770,  743926,  745722,\n",
      "                         752945,  753955,  754321,  754424,  756941,  758388,\n",
      "                         763631,  765290,  766914,  769685,  771353,  772022,\n",
      "                         776577,  778270,  786051,  790261,  791694,  793966,\n",
      "                         796637,  799112,  800770,  815690,  816473,  818259,\n",
      "                         820903,  821596,  823800,  824784,  829717,  832047,\n",
      "                         832150,  842188,  845609,  848220,  849614,  851843,\n",
      "                         853300,  856092,  857474,  857560,  858069,  859413,\n",
      "                         859577,  860467,  861016,  861546,  864458,  865902,\n",
      "                         871469,  873375,  873785,  874762,  881460,  892163,\n",
      "                         895376,  895934,  896537,  898816,  900097,  902875,\n",
      "                         905275,  905744,  916358,  916707,  917258,  922005,\n",
      "                         925857,  932901,  935807,  938077,  940631,  942572,\n",
      "                         945684,  945959,  947008,  949216,  955514,  960592,\n",
      "                         960605,  960770,  961694,  962792,  977550,  978282,\n",
      "                         981292,  983911,  987642,  988070,  990978,  991617,\n",
      "                         994998,  995302, 1001780, 1002193, 1007772, 1008000,\n",
      "                        1011569, 1012586, 1012839, 1014935, 1016743, 1018236,\n",
      "                        1022775, 1026189, 1026196, 1030128, 1031297, 1035735,\n",
      "                        1037154, 1037529, 1043423, 1043786, 1044823]]),\n",
      "       values=tensor([0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167, 0.0670,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0502, 0.0167, 0.0167, 0.1004,\n",
      "                      0.0335, 0.0335, 0.0335, 0.0335, 0.0167, 0.0167, 0.0837,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0335, 0.0167, 0.0502, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0670, 0.0670, 0.0335, 0.0167, 0.0335, 0.0335,\n",
      "                      0.0335, 0.0335, 0.0167, 0.1004, 0.0167, 0.0167, 0.0837,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0335, 0.0167, 0.0335, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167, 0.1507,\n",
      "                      0.0670, 0.1339, 0.0335, 0.0335, 0.0335, 0.0167, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.4855, 0.0335, 0.0335,\n",
      "                      0.0167, 0.0502, 0.0335, 0.0502, 0.0335, 0.0167, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0670, 0.0167, 0.0335, 0.0335, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0335, 0.0167, 0.0167, 0.0167, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0670, 0.0502, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0335, 0.0167, 0.0335, 0.0670, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.1507, 0.0335, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0502, 0.0167, 0.0167, 0.0670, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0335, 0.0335, 0.0167, 0.0167, 0.0335,\n",
      "                      0.0167, 0.0670, 0.0335, 0.0502, 0.0167, 0.0335, 0.0670,\n",
      "                      0.0670, 0.0335, 0.1339, 0.0167, 0.0167, 0.0335, 0.0167,\n",
      "                      0.0167, 0.0167, 0.2511, 0.0167, 0.0335, 0.0167, 0.0167,\n",
      "                      0.0502, 0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0335, 0.1339, 0.0335, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0167, 0.0335, 0.0335, 0.0335, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0167, 0.0167, 0.0502, 0.0670, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167, 0.0167, 0.0670, 0.0167, 0.0167,\n",
      "                      0.0335, 0.0167, 0.0167, 0.0167, 0.0670, 0.0167, 0.0335,\n",
      "                      0.0335, 0.0167, 0.0167, 0.0167, 0.0335, 0.0502, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0335, 0.0335, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.0167, 0.0670, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0335, 0.1674, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0335, 0.0335,\n",
      "                      0.0167, 0.0502, 0.0335, 0.1004, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0335, 0.0167, 0.0670, 0.0167, 0.1172,\n",
      "                      0.0167, 0.0335, 0.0335, 0.0167, 0.0167, 0.0502, 0.0335,\n",
      "                      0.0335, 0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0335,\n",
      "                      0.1674, 0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167, 0.0167, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.0335, 0.0335, 0.0502,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.0335, 0.0167, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0167, 0.0167, 0.0335, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0335, 0.0335, 0.0167, 0.0335,\n",
      "                      0.0670, 0.0167, 0.0335, 0.0335, 0.1507, 0.0502, 0.0167,\n",
      "                      0.0335, 0.0335, 0.0167, 0.0167, 0.0335, 0.0167, 0.0167,\n",
      "                      0.0837, 0.0167, 0.0335, 0.0335, 0.0167, 0.0167, 0.0167,\n",
      "                      0.0335, 0.0670, 0.0167, 0.0167, 0.0335, 0.0167, 0.0335,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.0502, 0.1507, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0335, 0.0335, 0.0167, 0.0335, 0.0670,\n",
      "                      0.0335, 0.0167, 0.0670, 0.0335, 0.0167, 0.0502, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.0670, 0.0167, 0.0167,\n",
      "                      0.0167, 0.0167, 0.0167, 0.0167, 0.3348, 0.0502, 0.0167,\n",
      "                      0.0167, 0.0335, 0.0167]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=395, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4747,    4993,    7663,   33618,   41680,   44157,\n",
      "                          65164,   72242,   77995,   83224,  104542,  110325,\n",
      "                         129255,  132263,  134239,  134676,  160635,  170127,\n",
      "                         181854,  183503,  202578,  248602,  261287,  271626,\n",
      "                         277274,  284011,  286447,  288082,  291058,  311856,\n",
      "                         315111,  335594,  378098,  388241,  388892,  414385,\n",
      "                         427197,  462769,  463390,  489141,  541775,  545024,\n",
      "                         555935,  558500,  562081,  576996,  595785,  604812,\n",
      "                         629795,  633966,  654930,  670091,  676606,  680788,\n",
      "                         682190,  688111,  690661,  709707,  724861,  729805,\n",
      "                         738519,  795166,  799928,  813363,  817497,  822200,\n",
      "                         847848,  848005,  850599,  853005,  887069,  888151,\n",
      "                         891788,  897215,  926952,  935777,  963512,  977484,\n",
      "                         992603, 1019906, 1031773]]),\n",
      "       values=tensor([0.1227, 0.0409, 0.0409, 0.0409, 0.0409, 0.0409, 0.0409,\n",
      "                      0.0818, 0.0409, 0.0409, 0.0818, 0.0818, 0.0409, 0.0409,\n",
      "                      0.0409, 0.1227, 0.0818, 0.0818, 0.0409, 0.0409, 0.0409,\n",
      "                      0.4498, 0.0818, 0.0409, 0.1227, 0.0818, 0.0409, 0.0409,\n",
      "                      0.0409, 0.0409, 0.0818, 0.0818, 0.0409, 0.0409, 0.0409,\n",
      "                      0.0409, 0.1227, 0.0818, 0.0409, 0.0409, 0.0409, 0.0409,\n",
      "                      0.0409, 0.0409, 0.0818, 0.0409, 0.0409, 0.0409, 0.0409,\n",
      "                      0.0409, 0.1227, 0.0818, 0.0409, 0.0409, 0.0818, 0.0409,\n",
      "                      0.0409, 0.0409, 0.0409, 0.0409, 0.2454, 0.1227, 0.0409,\n",
      "                      0.0818, 0.0818, 0.0818, 0.2454, 0.0409, 0.0409, 0.1227,\n",
      "                      0.0409, 0.1227, 0.0409, 0.3680, 0.4498, 0.0409, 0.1636,\n",
      "                      0.0409, 0.0409, 0.0409, 0.0409]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=81, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   4262,    7663,   11458,   13205,   13425,   15643,\n",
      "                          18269,   20996,   32646,   33566,   33753,   37263,\n",
      "                          41865,   42391,   43310,   43837,   45751,   47864,\n",
      "                          48860,   49809,   56144,   58931,   59272,   66563,\n",
      "                          66612,   68306,   77202,   77425,   82101,   83422,\n",
      "                          84348,   93916,   94335,   96833,  104637,  104712,\n",
      "                         105440,  106263,  108799,  109777,  109992,  111914,\n",
      "                         113965,  115069,  116623,  118086,  119120,  123096,\n",
      "                         123392,  124440,  126638,  128620,  128769,  130398,\n",
      "                         130536,  134541,  136177,  137151,  144002,  151106,\n",
      "                         156819,  158190,  161910,  165757,  172409,  172529,\n",
      "                         175618,  176660,  180999,  181182,  183122,  184875,\n",
      "                         191765,  192544,  195278,  198241,  200226,  204537,\n",
      "                         207087,  208093,  208787,  211024,  211934,  215687,\n",
      "                         216202,  219696,  220644,  223290,  223768,  225255,\n",
      "                         227394,  228234,  228305,  231953,  235017,  235742,\n",
      "                         240671,  242907,  245649,  246291,  246802,  249381,\n",
      "                         250301,  255979,  260032,  265319,  267587,  269032,\n",
      "                         270757,  270854,  279603,  282072,  284011,  284390,\n",
      "                         285932,  286033,  287665,  288082,  290308,  298123,\n",
      "                         299161,  300145,  301002,  302885,  303804,  303805,\n",
      "                         304057,  305298,  312820,  314036,  314106,  315004,\n",
      "                         316254,  317825,  329548,  337763,  338517,  342874,\n",
      "                         344746,  347214,  349959,  353335,  355080,  356650,\n",
      "                         357780,  360982,  364511,  365735,  369407,  369543,\n",
      "                         373689,  376681,  376749,  378098,  378184,  378457,\n",
      "                         379610,  379789,  385695,  385825,  391143,  403023,\n",
      "                         403622,  406527,  416346,  417793,  419425,  419655,\n",
      "                         419763,  420005,  420651,  422011,  423931,  427197,\n",
      "                         428259,  438849,  439736,  440655,  441191,  444013,\n",
      "                         445173,  449356,  451608,  453323,  455313,  456672,\n",
      "                         459271,  460797,  462792,  463390,  465469,  468028,\n",
      "                         469940,  471814,  472719,  475048,  479676,  481580,\n",
      "                         483314,  487471,  489141,  491896,  496272,  497646,\n",
      "                         499894,  501598,  502020,  506551,  508189,  508256,\n",
      "                         513664,  516492,  518275,  520755,  520965,  522034,\n",
      "                         522112,  526710,  528451,  532768,  533220,  534719,\n",
      "                         536463,  539127,  545183,  550125,  555936,  562081,\n",
      "                         562478,  563741,  564000,  564612,  564799,  568749,\n",
      "                         571415,  572914,  573374,  574289,  575579,  578108,\n",
      "                         579713,  581096,  584374,  589784,  589830,  592652,\n",
      "                         593617,  598053,  603007,  604812,  606278,  607102,\n",
      "                         608138,  609410,  609898,  611141,  611288,  615761,\n",
      "                         617660,  619417,  622862,  623231,  623503,  623845,\n",
      "                         628764,  630014,  630779,  632461,  633535,  636876,\n",
      "                         637200,  637377,  639222,  639342,  639770,  644552,\n",
      "                         644630,  645039,  645140,  645214,  645438,  649791,\n",
      "                         651137,  652867,  652875,  656688,  656912,  659143,\n",
      "                         659236,  659405,  660819,  665935,  667812,  670445,\n",
      "                         675722,  676703,  680788,  682190,  685036,  685639,\n",
      "                         685930,  686725,  686889,  691102,  691724,  692291,\n",
      "                         694791,  696171,  696366,  700093,  700850,  702096,\n",
      "                         702689,  704486,  707447,  712964,  714935,  716061,\n",
      "                         717803,  718125,  723441,  725407,  730180,  730378,\n",
      "                         734364,  736108,  736782,  739803,  742073,  748479,\n",
      "                         750451,  750530,  751870,  753213,  754424,  757243,\n",
      "                         757430,  762396,  763631,  765129,  766932,  767441,\n",
      "                         768404,  768504,  769381,  770317,  772187,  772924,\n",
      "                         783250,  784902,  789441,  789626,  789635,  790209,\n",
      "                         792819,  798640,  799059,  803084,  811455,  813363,\n",
      "                         815287,  815801,  816233,  816470,  817621,  821862,\n",
      "                         823364,  823397,  824478,  825319,  827737,  828576,\n",
      "                         828733,  831090,  832913,  835473,  836404,  836442,\n",
      "                         837586,  839238,  842815,  845483,  845609,  845632,\n",
      "                         848943,  850415,  851238,  852092,  856427,  857691,\n",
      "                         866837,  868832,  869408,  870214,  871381,  874094,\n",
      "                         874762,  876844,  878377,  880389,  883318,  885113,\n",
      "                         885528,  887127,  887612,  898740,  900611,  901523,\n",
      "                         905165,  907553,  908408,  910556,  915333,  917338,\n",
      "                         918617,  921488,  922868,  924777,  932304,  933080,\n",
      "                         933642,  936007,  936662,  938379,  938692,  938905,\n",
      "                         939858,  942049,  949320,  954683,  955142,  956223,\n",
      "                         957152,  961291,  961694,  962588,  969233,  969771,\n",
      "                         970200,  972033,  972279,  972880,  976983,  982005,\n",
      "                         983052,  986861,  990035,  990225,  994053,  994487,\n",
      "                         995638, 1000438, 1006860, 1008826, 1009981, 1011350,\n",
      "                        1017263, 1019448, 1021103, 1024034, 1025814, 1026760,\n",
      "                        1027359, 1028225, 1031372, 1032060, 1035084, 1035735,\n",
      "                        1036203, 1037924, 1038630, 1039680, 1040169, 1043771,\n",
      "                        1045345, 1045494]]),\n",
      "       values=tensor([0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0338, 0.0507, 0.0844,\n",
      "                      0.0507, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0507, 0.2364, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0507, 0.0338, 0.0844, 0.0507, 0.0169, 0.0338,\n",
      "                      0.0507, 0.0169, 0.0169, 0.0507, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0507, 0.0338, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0507, 0.0169, 0.1520, 0.0338, 0.0169,\n",
      "                      0.1013, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.1182,\n",
      "                      0.0338, 0.0169, 0.0338, 0.0169, 0.1182, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0507,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0507, 0.0169, 0.0507, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0338, 0.0507, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0169, 0.0675, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0675, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0338, 0.0169,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0338, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.1689, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0844, 0.3377,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0675, 0.0169, 0.0338, 0.0169, 0.0169, 0.0338, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0844, 0.0169, 0.0338, 0.0338, 0.0338, 0.0169, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.3884,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0675, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0338, 0.0169, 0.0507, 0.0844, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0675, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169,\n",
      "                      0.0507, 0.0169, 0.0169, 0.2195, 0.0169, 0.0169, 0.0507,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.1013, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0507, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0675,\n",
      "                      0.0169, 0.0338, 0.0338, 0.0169, 0.0169, 0.0507, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0507, 0.0169,\n",
      "                      0.0338, 0.0507, 0.0507, 0.0169, 0.0169, 0.0169, 0.1351,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0338, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0675, 0.0169, 0.0507, 0.0169, 0.2195,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0844, 0.0169, 0.0338, 0.0169, 0.0507, 0.0169, 0.1520,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0507, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0169,\n",
      "                      0.0338, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0507, 0.0169, 0.0338,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0338, 0.0169, 0.0507, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.3377, 0.0169, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0169, 0.0169, 0.0169, 0.0507, 0.0507,\n",
      "                      0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0675, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0338, 0.0169, 0.0338, 0.0169,\n",
      "                      0.0169, 0.0169, 0.0338, 0.0169, 0.0169, 0.0169, 0.0169]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=476, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   1364,    7214,    7698,   14175,   15518,   17575,\n",
      "                          19610,   20261,   26131,   28764,   30256,   31712,\n",
      "                          33036,   36611,   37722,   51295,   52595,   56799,\n",
      "                          63427,   70905,   73732,   74305,   74327,   75066,\n",
      "                          77362,   82900,   85586,   89993,   90342,   97540,\n",
      "                          98052,  100005,  100564,  100742,  101800,  111914,\n",
      "                         120086,  123946,  125532,  130435,  133764,  144928,\n",
      "                         147631,  153905,  154664,  158307,  159274,  167071,\n",
      "                         167891,  167997,  169246,  171331,  171836,  172218,\n",
      "                         172875,  175618,  176645,  176660,  177590,  177821,\n",
      "                         180414,  185628,  186346,  197806,  200954,  201093,\n",
      "                         204537,  207313,  207456,  213222,  220398,  221466,\n",
      "                         227928,  228597,  231149,  240617,  247599,  249247,\n",
      "                         253939,  254973,  256417,  256425,  260069,  260089,\n",
      "                         263780,  266312,  275192,  278877,  287665,  291431,\n",
      "                         292602,  296127,  299161,  302487,  303805,  306270,\n",
      "                         306588,  308651,  316254,  317837,  322776,  323069,\n",
      "                         327678,  331110,  331146,  333033,  333248,  337399,\n",
      "                         338688,  346050,  348528,  356306,  365309,  368578,\n",
      "                         370120,  370730,  373756,  395455,  398895,  409369,\n",
      "                         409865,  411831,  413614,  414384,  416210,  416552,\n",
      "                         422077,  424396,  425005,  428734,  430098,  430184,\n",
      "                         430466,  430501,  431143,  436818,  439855,  443057,\n",
      "                         455007,  458355,  460508,  466472,  467797,  472033,\n",
      "                         476585,  477587,  479650,  482559,  483079,  489448,\n",
      "                         491896,  492029,  493098,  495765,  497864,  499325,\n",
      "                         505146,  505649,  506970,  512021,  515488,  520880,\n",
      "                         524031,  532155,  532768,  533944,  535530,  536463,\n",
      "                         539708,  541105,  553760,  555920,  558544,  566251,\n",
      "                         567860,  572163,  572693,  577442,  577669,  579342,\n",
      "                         588209,  593995,  594744,  602217,  606574,  609173,\n",
      "                         613432,  617376,  618510,  629933,  637178,  639342,\n",
      "                         655027,  662483,  665783,  671591,  673533,  685185,\n",
      "                         689560,  691802,  694791,  707942,  710615,  716738,\n",
      "                         719852,  720145,  723999,  726369,  726524,  746855,\n",
      "                         747031,  747392,  748220,  751728,  754097,  756731,\n",
      "                         761302,  763631,  766687,  771367,  771785,  772022,\n",
      "                         772203,  773988,  777942,  780244,  785295,  793684,\n",
      "                         802847,  804641,  806225,  812539,  814017,  815598,\n",
      "                         819643,  821862,  823364,  823542,  823851,  829380,\n",
      "                         830193,  831214,  839238,  840305,  851663,  854015,\n",
      "                         857474,  858598,  859812,  861172,  863387,  869857,\n",
      "                         881374,  882054,  889531,  890515,  892163,  898740,\n",
      "                         910486,  914583,  915275,  916173,  916358,  925624,\n",
      "                         931626,  934978,  937018,  944787,  944800,  947363,\n",
      "                         947445,  952439,  956945,  964022,  970200,  970849,\n",
      "                         976211,  978757,  981398,  982773,  982937,  983384,\n",
      "                         986519,  988071,  990467,  991094,  996371,  997238,\n",
      "                         999331, 1007482, 1008826, 1009915, 1012579, 1014125,\n",
      "                        1015431, 1015734, 1019047, 1019624, 1025814, 1026035,\n",
      "                        1028588, 1029450, 1035735, 1037791, 1037885, 1041084,\n",
      "                        1042162, 1046787]]),\n",
      "       values=tensor([0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0721, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0481, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0481, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.1202, 0.0240, 0.0240, 0.0240, 0.0962, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0962, 0.0481, 0.0240, 0.0240, 0.1443, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0962, 0.0240, 0.0240, 0.0721, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.2645, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0481, 0.0240, 0.0240, 0.0962,\n",
      "                      0.0481, 0.0240, 0.0240, 0.0481, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0721, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0721, 0.0240, 0.0962,\n",
      "                      0.0240, 0.0240, 0.0481, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0481, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0481,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0481, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0481, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0721, 0.0240, 0.0240, 0.0481, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0962, 0.0240, 0.0481, 0.0240, 0.0962,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0481,\n",
      "                      0.0240, 0.0240, 0.0481, 0.0240, 0.0481, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0481, 0.0240, 0.0240, 0.0481, 0.0240, 0.0721, 0.0240,\n",
      "                      0.0240, 0.0481, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0962,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0962, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0962, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0481, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.1443, 0.0962, 0.0240,\n",
      "                      0.0240, 0.0240, 0.5049, 0.0240, 0.0240, 0.0240, 0.0481,\n",
      "                      0.0240, 0.0240, 0.0240, 0.1443, 0.0481, 0.0240, 0.4328,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0481, 0.0481, 0.0240,\n",
      "                      0.0240, 0.0481, 0.0240, 0.0240, 0.0240, 0.0240, 0.0962,\n",
      "                      0.0240, 0.0240, 0.0721, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0481, 0.0240, 0.0240, 0.0240, 0.0481, 0.0240, 0.1202,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0481, 0.0240, 0.0240, 0.0481, 0.0962, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0240, 0.1683, 0.0240, 0.0240]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=308, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   1364,    3437,    4993,  ..., 1045345, 1045494,\n",
      "                        1048029]]),\n",
      "       values=tensor([0.0710, 0.0037, 0.0187,  ..., 0.0262, 0.0262, 0.0075]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1044, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  11274,   23523,  190089,  218065,  249247,  313062,\n",
      "                         487160,  605960,  613709,  623824,  646243,  675598,\n",
      "                         680619,  757243,  806225,  808224,  844679,  986861,\n",
      "                        1014259, 1025814, 1038762, 1045546]]),\n",
      "       values=tensor([0.3112, 0.3112, 0.0389, 0.0778, 0.0389, 0.0389, 0.0389,\n",
      "                      0.0389, 0.3112, 0.0389, 0.0778, 0.3112, 0.0389, 0.3112,\n",
      "                      0.0389, 0.0778, 0.3112, 0.3112, 0.0389, 0.3112, 0.3112,\n",
      "                      0.3112]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=22, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [  31712,   43829,   45291,   72242,   75066,   86889,\n",
      "                         116557,  129001,  132835,  157513,  162917,  171836,\n",
      "                         180414,  202578,  202750,  230201,  241309,  250337,\n",
      "                         261287,  283977,  284163,  293038,  302597,  351700,\n",
      "                         353478,  365382,  391060,  463390,  494507,  518126,\n",
      "                         549676,  569221,  583408,  585733,  595054,  617280,\n",
      "                         638498,  666657,  676693,  688111,  695058,  703380,\n",
      "                         710853,  723999,  748708,  750433,  762380,  799928,\n",
      "                         820638,  871194,  891788,  896891,  907489,  912690,\n",
      "                         915747,  941540,  945014,  957178,  962588,  968435,\n",
      "                         977772,  989394,  989457,  995358, 1019906]]),\n",
      "       values=tensor([0.0167, 0.1167, 0.0333, 0.0167, 0.0333, 0.1501, 0.1167,\n",
      "                      0.2334, 0.1167, 0.0167, 0.0667, 0.2334, 0.0167, 0.1000,\n",
      "                      0.0834, 0.0834, 0.0333, 0.1000, 0.1334, 0.0333, 0.1167,\n",
      "                      0.0834, 0.1000, 0.1167, 0.0333, 0.0167, 0.0834, 0.0167,\n",
      "                      0.1167, 0.1167, 0.1167, 0.2334, 0.0333, 0.0167, 0.0333,\n",
      "                      0.1167, 0.1167, 0.0834, 0.1501, 0.0167, 0.2501, 0.0834,\n",
      "                      0.0333, 0.0167, 0.0167, 0.1167, 0.1501, 0.0834, 0.0333,\n",
      "                      0.1167, 0.1000, 0.2334, 0.0167, 0.0333, 0.0167, 0.0834,\n",
      "                      0.1167, 0.1167, 0.1167, 0.0834, 0.1834, 0.4835, 0.1167,\n",
      "                      0.1167, 0.1167]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=65, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[     0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0],\n",
      "                       [  3607,  45962, 127846, 133387, 138566, 175827, 234230,\n",
      "                        247599, 264573, 277563, 298123, 348566, 370780, 468830,\n",
      "                        548553, 551393, 575313, 601145, 628764, 628961, 698519,\n",
      "                        703380, 768202, 843661, 848491, 851503, 876882, 937304,\n",
      "                        958870, 973566]]),\n",
      "       values=tensor([0.1782, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782,\n",
      "                      0.1782, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782,\n",
      "                      0.1782, 0.1782, 0.1782, 0.0891, 0.1782, 0.1782, 0.0891,\n",
      "                      0.3563, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782, 0.1782,\n",
      "                      0.1782, 0.1782]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=30, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    649,    2787,    2904,  ..., 1047802, 1048291,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0016, 0.0016, 0.0016,  ..., 0.0292, 0.0016, 0.0081]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1800, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  72242,  109992,  111914,  117922,  177127,  179131,\n",
      "                         180414,  181854,  183122,  186237,  204128,  208093,\n",
      "                         212119,  218949,  227600,  233597,  258821,  261287,\n",
      "                         311111,  312408,  313691,  439491,  463390,  478794,\n",
      "                         483567,  484243,  497646,  607352,  609898,  631802,\n",
      "                         643423,  674972,  676606,  682190,  688111,  699750,\n",
      "                         718393,  754424,  756012,  796613,  798002,  813363,\n",
      "                         825319,  838113,  871737,  891097,  899093,  920960,\n",
      "                         936625,  968425, 1031773]]),\n",
      "       values=tensor([0.0403, 0.1610, 0.0805, 0.0805, 0.1610, 0.1208, 0.1610,\n",
      "                      0.2416, 0.0805, 0.0805, 0.0805, 0.1610, 0.0805, 0.1610,\n",
      "                      0.0805, 0.1610, 0.0805, 0.1208, 0.0805, 0.0805, 0.0805,\n",
      "                      0.1610, 0.0403, 0.0805, 0.0805, 0.0805, 0.0805, 0.0403,\n",
      "                      0.0805, 0.5636, 0.0805, 0.0805, 0.0805, 0.0805, 0.0403,\n",
      "                      0.0805, 0.0805, 0.1610, 0.0403, 0.0805, 0.0805, 0.1610,\n",
      "                      0.0805, 0.0805, 0.0403, 0.0805, 0.2416, 0.0403, 0.0805,\n",
      "                      0.0805, 0.3221]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=51, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [     16,      59,     281,  ..., 1048189, 1048291,\n",
      "                        1048553]]),\n",
      "       values=tensor([3.6356e-04, 9.0890e-05, 9.0890e-04,  ...,\n",
      "                      1.8178e-04, 1.2725e-03, 8.1801e-04]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=8168, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4310,    4889,    4993,   13205,   18025,   24582,\n",
      "                          26745,   27956,   28629,   28753,   35071,   40548,\n",
      "                          47163,   47334,   47399,   51833,   56144,   56836,\n",
      "                          64008,   75066,   81097,   86760,   86889,   91636,\n",
      "                          97299,  102865,  104343,  104347,  108294,  125462,\n",
      "                         127846,  132733,  134524,  134676,  143699,  144612,\n",
      "                         148594,  150336,  151203,  151691,  152637,  155441,\n",
      "                         160635,  172168,  172475,  174155,  180414,  181854,\n",
      "                         183122,  190517,  195278,  199544,  205749,  208522,\n",
      "                         209687,  215194,  245421,  250337,  252762,  256425,\n",
      "                         261287,  265772,  272529,  279900,  280856,  282085,\n",
      "                         284011,  286620,  287665,  291252,  297915,  313824,\n",
      "                         316970,  320802,  323681,  324827,  331110,  335771,\n",
      "                         345966,  349796,  357454,  357897,  364285,  365508,\n",
      "                         366662,  366912,  370268,  373758,  377670,  378098,\n",
      "                         381838,  390168,  392071,  398532,  398738,  401992,\n",
      "                         410776,  414030,  414110,  414353,  420474,  422011,\n",
      "                         430909,  432823,  433755,  463390,  465452,  471472,\n",
      "                         472021,  477333,  477713,  480454,  489141,  489963,\n",
      "                         501737,  502723,  507438,  507879,  507884,  510025,\n",
      "                         514467,  517462,  517578,  522112,  527653,  534038,\n",
      "                         534719,  535621,  538520,  544092,  544502,  551371,\n",
      "                         554671,  584374,  584494,  592920,  594744,  595785,\n",
      "                         599889,  603284,  604812,  611075,  613452,  619113,\n",
      "                         623066,  625238,  629528,  631674,  639716,  644552,\n",
      "                         660320,  663052,  665678,  670091,  674972,  676606,\n",
      "                         680623,  683025,  683992,  686106,  689676,  690661,\n",
      "                         693196,  697582,  698875,  704261,  706440,  710325,\n",
      "                         716911,  723208,  723924,  727903,  733169,  737566,\n",
      "                         738341,  740097,  748220,  752882,  753187,  754077,\n",
      "                         754424,  756882,  773019,  776577,  778918,  785295,\n",
      "                         788003,  789080,  791693,  793928,  795109,  796613,\n",
      "                         799771,  806546,  806978,  812720,  813696,  814112,\n",
      "                         817497,  818687,  820455,  827672,  831499,  836404,\n",
      "                         839265,  845352,  851769,  851804,  852687,  857691,\n",
      "                         858552,  858730,  859976,  863827,  865972,  875791,\n",
      "                         883158,  884070,  885911,  892458,  893493,  893771,\n",
      "                         894997,  896903,  907791,  910749,  912518,  914157,\n",
      "                         914882,  916173,  922760,  926952,  936007,  954143,\n",
      "                         956214,  958092,  961694,  968224,  969355,  970849,\n",
      "                         971388,  978398,  982773,  988070,  993654, 1001646,\n",
      "                        1005358, 1009746, 1011917, 1014259, 1016024, 1017367,\n",
      "                        1023869, 1029750, 1031773, 1033052, 1036050, 1037764,\n",
      "                        1041348, 1043771, 1048303]]),\n",
      "       values=tensor([0.0230, 0.0230, 0.0691, 0.0461, 0.0461, 0.0461, 0.0230,\n",
      "                      0.0230, 0.0461, 0.0461, 0.0461, 0.0230, 0.0461, 0.0230,\n",
      "                      0.0230, 0.0922, 0.0691, 0.0922, 0.0461, 0.0230, 0.0461,\n",
      "                      0.0461, 0.0461, 0.0461, 0.1844, 0.0461, 0.2074, 0.0230,\n",
      "                      0.0230, 0.0230, 0.1613, 0.0922, 0.0461, 0.3457, 0.0230,\n",
      "                      0.0691, 0.0461, 0.0461, 0.0230, 0.0461, 0.0461, 0.0230,\n",
      "                      0.0691, 0.0230, 0.0230, 0.0230, 0.0461, 0.0922, 0.0230,\n",
      "                      0.0461, 0.0461, 0.0230, 0.0230, 0.0230, 0.0230, 0.0230,\n",
      "                      0.0461, 0.0230, 0.0461, 0.0230, 0.0691, 0.0461, 0.1383,\n",
      "                      0.0461, 0.0461, 0.0230, 0.0230, 0.0230, 0.0230, 0.0461,\n",
      "                      0.0461, 0.0461, 0.0461, 0.0230, 0.0230, 0.0461, 0.0230,\n",
      "                      0.0230, 0.0230, 0.0230, 0.0922, 0.0461, 0.0691, 0.0461,\n",
      "                      0.0461, 0.0922, 0.0230, 0.0461, 0.0230, 0.0461, 0.0230,\n",
      "                      0.0230, 0.0230, 0.0461, 0.0230, 0.0230, 0.1383, 0.0230,\n",
      "                      0.0230, 0.0922, 0.0230, 0.0691, 0.0461, 0.0230, 0.0461,\n",
      "                      0.0922, 0.0230, 0.0230, 0.0461, 0.0461, 0.0230, 0.0230,\n",
      "                      0.0230, 0.0230, 0.0461, 0.0230, 0.0230, 0.0230, 0.0461,\n",
      "                      0.0461, 0.0230, 0.0461, 0.0461, 0.1152, 0.0230, 0.0230,\n",
      "                      0.0461, 0.0230, 0.0461, 0.0461, 0.0922, 0.0230, 0.0230,\n",
      "                      0.0230, 0.0461, 0.1383, 0.0461, 0.1152, 0.0461, 0.0230,\n",
      "                      0.0461, 0.0230, 0.0461, 0.0230, 0.0230, 0.0461, 0.0461,\n",
      "                      0.0230, 0.0230, 0.0461, 0.0461, 0.0461, 0.0461, 0.0461,\n",
      "                      0.0230, 0.0230, 0.0461, 0.0230, 0.0230, 0.0230, 0.0230,\n",
      "                      0.0230, 0.0230, 0.0922, 0.0230, 0.0230, 0.0922, 0.0230,\n",
      "                      0.0230, 0.0461, 0.0461, 0.0230, 0.0230, 0.0461, 0.0461,\n",
      "                      0.0461, 0.0461, 0.0461, 0.0691, 0.0922, 0.0691, 0.0230,\n",
      "                      0.0230, 0.0230, 0.0230, 0.1152, 0.1383, 0.0922, 0.0230,\n",
      "                      0.0461, 0.1613, 0.0461, 0.0461, 0.0230, 0.0230, 0.0230,\n",
      "                      0.0461, 0.0461, 0.0922, 0.0230, 0.0461, 0.0230, 0.0461,\n",
      "                      0.0230, 0.0922, 0.0230, 0.1383, 0.0461, 0.0461, 0.0230,\n",
      "                      0.0461, 0.0230, 0.0691, 0.0691, 0.0230, 0.1383, 0.0691,\n",
      "                      0.0230, 0.0461, 0.0230, 0.0230, 0.0461, 0.0230, 0.0922,\n",
      "                      0.0230, 0.0922, 0.0230, 0.0461, 0.2996, 0.0461, 0.0230,\n",
      "                      0.1383, 0.0461, 0.0230, 0.0461, 0.0461, 0.0230, 0.0230,\n",
      "                      0.0230, 0.0461, 0.0691, 0.0230, 0.0461, 0.0461, 0.0461,\n",
      "                      0.0461, 0.0230, 0.0922, 0.0230, 0.0230, 0.0461, 0.0461,\n",
      "                      0.0230, 0.0461, 0.0461, 0.0230, 0.0461, 0.0461, 0.0461,\n",
      "                      0.0461, 0.0461]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=261, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [  13196,   28711,   35476,   44647,   66546,   72242,\n",
      "                          77469,   81097,   83224,  104343,  131210,  143957,\n",
      "                         149970,  202578,  261287,  280728,  302597,  318613,\n",
      "                         324827,  331110,  357954,  401992,  403622,  413561,\n",
      "                         442183,  463020,  463390,  477333,  497599,  515242,\n",
      "                         520755,  529674,  533850,  544092,  561926,  619417,\n",
      "                         671665,  673190,  684696,  688111,  691802,  697605,\n",
      "                         701891,  711629,  733961,  750279,  753213,  755578,\n",
      "                         765613,  778214,  778883,  786953,  788397,  813369,\n",
      "                         871194,  872998,  873973,  891788,  892458,  896903,\n",
      "                         919814,  938177,  957152,  982773,  983403, 1005517]]),\n",
      "       values=tensor([0.0971, 0.0971, 0.0971, 0.0971, 0.0486, 0.0486, 0.0971,\n",
      "                      0.0971, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971,\n",
      "                      0.0486, 0.0971, 0.0971, 0.2914, 0.0971, 0.0971, 0.0971,\n",
      "                      0.0486, 0.0971, 0.2914, 0.0971, 0.0971, 0.0486, 0.0971,\n",
      "                      0.0971, 0.0971, 0.0971, 0.2914, 0.0971, 0.2914, 0.0971,\n",
      "                      0.0971, 0.0971, 0.0971, 0.0971, 0.0486, 0.0971, 0.0971,\n",
      "                      0.0971, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971,\n",
      "                      0.0971, 0.0971, 0.2428, 0.0971, 0.0971, 0.0971, 0.1943,\n",
      "                      0.1457, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971, 0.0971,\n",
      "                      0.1943, 0.0971, 0.0971]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=66, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    218,     229,     457,  ..., 1047847, 1047855,\n",
      "                        1048572]]),\n",
      "       values=tensor([0.0021, 0.0021, 0.0014,  ..., 0.0007, 0.0014, 0.0007]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=4597, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   2290,   23026,   44371,   72242,   77995,   83224,\n",
      "                          86356,   88377,  101378,  102865,  106521,  122692,\n",
      "                         124896,  128620,  129255,  134541,  138396,  150957,\n",
      "                         160635,  170200,  171836,  174569,  181854,  193995,\n",
      "                         202578,  250486,  256417,  256425,  257613,  258821,\n",
      "                         261287,  271188,  284011,  284125,  286636,  300606,\n",
      "                         306588,  314106,  334634,  338517,  343358,  347442,\n",
      "                         352161,  370389,  401992,  406627,  406752,  426530,\n",
      "                         434197,  436818,  451795,  451849,  463390,  482690,\n",
      "                         484354,  489267,  494082,  497646,  521557,  526232,\n",
      "                         526453,  533850,  543301,  551393,  561926,  599399,\n",
      "                         609898,  611746,  619417,  620996,  654930,  662483,\n",
      "                         666856,  676313,  676606,  677206,  688111,  689509,\n",
      "                         696949,  700786,  725665,  726272,  728117,  736686,\n",
      "                         737240,  748708,  756012,  776312,  800123,  816699,\n",
      "                         817497,  840228,  846556,  887069,  888506,  891788,\n",
      "                         893290,  894997,  895122,  899662,  905954,  906218,\n",
      "                         914583,  915333,  920021,  957999,  958944,  973725,\n",
      "                         989394,  992806,  993256,  994998, 1019906, 1023442,\n",
      "                        1034292]]),\n",
      "       values=tensor([0.0455, 0.0455, 0.0455, 0.0911, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0455, 0.0455, 0.0911, 0.0455, 0.0911, 0.0911, 0.0455,\n",
      "                      0.0455, 0.0455, 0.0911, 0.0455, 0.0911, 0.0455, 0.0455,\n",
      "                      0.0455, 0.0455, 0.0455, 0.0911, 0.0455, 0.0455, 0.0455,\n",
      "                      0.2277, 0.0455, 0.0911, 0.0455, 0.0911, 0.0455, 0.0455,\n",
      "                      0.0911, 0.1366, 0.0455, 0.0455, 0.0455, 0.0455, 0.0911,\n",
      "                      0.2277, 0.0455, 0.0455, 0.0455, 0.1366, 0.0911, 0.0455,\n",
      "                      0.2277, 0.0455, 0.0455, 0.0455, 0.0911, 0.0455, 0.2277,\n",
      "                      0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0911, 0.0455, 0.0455, 0.0911, 0.0911, 0.0455, 0.0455,\n",
      "                      0.0911, 0.2733, 0.0455, 0.5010, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0911, 0.1822, 0.1366, 0.0911, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0911, 0.0911, 0.0455, 0.0455, 0.0455, 0.0455, 0.0911,\n",
      "                      0.0455, 0.0455, 0.0455, 0.0455, 0.0911, 0.0455, 0.0455,\n",
      "                      0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0455, 0.0455, 0.1366, 0.0455, 0.0455, 0.0455, 0.0455,\n",
      "                      0.0455, 0.0455, 0.0455]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=115, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4993,    5237,    7663,    8015,    8356,   20366,\n",
      "                          25335,   36322,   41680,   42127,   46651,   72242,\n",
      "                          83224,   86805,   86937,  102865,  134260,  138566,\n",
      "                         150336,  154623,  157024,  169246,  175827,  179245,\n",
      "                         181854,  185917,  190150,  196296,  202578,  202893,\n",
      "                         204615,  207087,  219854,  220530,  230201,  244234,\n",
      "                         248539,  257581,  261287,  269530,  270167,  284011,\n",
      "                         298102,  298424,  345617,  357780,  386104,  389719,\n",
      "                         401412,  403294,  416069,  425164,  440710,  444013,\n",
      "                         463390,  465452,  484683,  489141,  497646,  503017,\n",
      "                         506462,  506479,  521953,  534719,  538357,  561590,\n",
      "                         575268,  584505,  588209,  590331,  592652,  627392,\n",
      "                         628961,  643518,  649727,  650384,  654044,  655285,\n",
      "                         665539,  688111,  692291,  695066,  701891,  702425,\n",
      "                         710331,  710880,  725164,  737068,  739946,  745514,\n",
      "                         746265,  781946,  786746,  799339,  804729,  809082,\n",
      "                         822949,  825495,  826208,  827473,  867767,  868920,\n",
      "                         891788,  893771,  898816,  899093,  906038,  933164,\n",
      "                         943695,  943723,  949527,  957152,  963633,  971070,\n",
      "                         971278,  985093,  986799,  989394,  992783,  995598,\n",
      "                         999983, 1020063, 1025299, 1034292, 1040609, 1045494,\n",
      "                        1045546]]),\n",
      "       values=tensor([0.0333, 0.0333, 0.0665, 0.0665, 0.0333, 0.1330, 0.0665,\n",
      "                      0.0333, 0.0998, 0.1663, 0.0665, 0.0333, 0.0998, 0.0333,\n",
      "                      0.0333, 0.0998, 0.0665, 0.0998, 0.0333, 0.0333, 0.0333,\n",
      "                      0.0333, 0.1330, 0.0665, 0.0333, 0.0333, 0.0665, 0.0665,\n",
      "                      0.1330, 0.0665, 0.0665, 0.0333, 0.0333, 0.0333, 0.0333,\n",
      "                      0.0333, 0.0333, 0.0333, 0.1330, 0.0333, 0.2661, 0.0665,\n",
      "                      0.2328, 0.0333, 0.0333, 0.0333, 0.0998, 0.0333, 0.0665,\n",
      "                      0.0333, 0.0665, 0.2661, 0.0333, 0.0665, 0.0998, 0.0333,\n",
      "                      0.1330, 0.0333, 0.0333, 0.0333, 0.0665, 0.0665, 0.0665,\n",
      "                      0.0665, 0.0333, 0.0665, 0.0665, 0.0333, 0.0333, 0.0665,\n",
      "                      0.0333, 0.0333, 0.0333, 0.0333, 0.2328, 0.0333, 0.0333,\n",
      "                      0.0333, 0.0665, 0.0333, 0.0665, 0.0333, 0.0665, 0.0665,\n",
      "                      0.0998, 0.0333, 0.0665, 0.0333, 0.0333, 0.0998, 0.0998,\n",
      "                      0.1330, 0.0665, 0.0665, 0.0998, 0.0665, 0.0333, 0.0998,\n",
      "                      0.0665, 0.0998, 0.0333, 0.0998, 0.1330, 0.0333, 0.0665,\n",
      "                      0.0333, 0.0998, 0.0665, 0.2328, 0.0333, 0.0333, 0.0333,\n",
      "                      0.0665, 0.0665, 0.0998, 0.0665, 0.0665, 0.0665, 0.1996,\n",
      "                      0.0333, 0.0998, 0.0665, 0.2993, 0.1330, 0.0665, 0.0333,\n",
      "                      0.0665]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=127, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [  28007,   33618,   41168,   71032,   72242,   83224,\n",
      "                         116557,  129001,  199714,  221929,  244038,  250486,\n",
      "                         261287,  267292,  284011,  306588,  311856,  338517,\n",
      "                         361492,  383531,  401412,  425679,  463390,  467952,\n",
      "                         470449,  474674,  478794,  483567,  533850,  564210,\n",
      "                         581096,  606366,  615924,  620996,  676813,  680788,\n",
      "                         688111,  706231,  748220,  752031,  753187,  754424,\n",
      "                         758917,  780319,  781819,  817497,  820231,  849465,\n",
      "                         860866,  879743,  885344,  888797,  888897,  894997,\n",
      "                         915747,  966647,  971070,  973725,  982773,  990225,\n",
      "                        1002391, 1011259]]),\n",
      "       values=tensor([0.0792, 0.0792, 0.0792, 0.0792, 0.0396, 0.0792, 0.0792,\n",
      "                      0.0792, 0.0792, 0.0792, 0.0792, 0.0792, 0.1189, 0.0792,\n",
      "                      0.0792, 0.1585, 0.3170, 0.1585, 0.0792, 0.0792, 0.0792,\n",
      "                      0.1585, 0.0396, 0.3170, 0.0792, 0.0792, 0.0792, 0.0792,\n",
      "                      0.0792, 0.0792, 0.0792, 0.0792, 0.0792, 0.0792, 0.0396,\n",
      "                      0.1585, 0.0396, 0.0792, 0.0792, 0.0792, 0.0792, 0.0792,\n",
      "                      0.2377, 0.0396, 0.0792, 0.0792, 0.1585, 0.0792, 0.0792,\n",
      "                      0.0396, 0.2377, 0.0792, 0.4358, 0.0792, 0.1585, 0.0792,\n",
      "                      0.0792, 0.2377, 0.0396, 0.0792, 0.0792, 0.0792]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=62, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [     46,      60,      97,  ..., 1048301, 1048456,\n",
      "                        1048461]]),\n",
      "       values=tensor([0.0003, 0.0003, 0.0003,  ..., 0.0006, 0.0003, 0.0003]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=9457, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [   4187,   13205,   16374,   65164,   72242,   81998,\n",
      "                          83224,   96677,  104343,  128430,  147277,  149970,\n",
      "                         161347,  190333,  201625,  202578,  209899,  214012,\n",
      "                         231149,  232925,  236858,  261287,  261798,  263756,\n",
      "                         272625,  274689,  306588,  342424,  360283,  361492,\n",
      "                         371551,  373756,  378799,  396224,  403294,  406752,\n",
      "                         426530,  463390,  497646,  533850,  543301,  544092,\n",
      "                         593468,  604812,  637354,  646726,  657653,  682190,\n",
      "                         688111,  708052,  720228,  748708,  750360,  758474,\n",
      "                         763865,  781220,  804474,  838370,  842698,  891788,\n",
      "                         895122,  898816,  900272,  910749,  910977,  914583,\n",
      "                         924956,  962543,  964076,  978028,  981289,  990225,\n",
      "                        1017368, 1028115, 1031773, 1048517]]),\n",
      "       values=tensor([0.1322, 0.0661, 0.0661, 0.1322, 0.1322, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.3304, 0.0661,\n",
      "                      0.1322, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.1982, 0.1982, 0.0661, 0.1322,\n",
      "                      0.0661, 0.0661, 0.1322, 0.0661, 0.0661, 0.0661, 0.1982,\n",
      "                      0.1322, 0.0661, 0.0661, 0.0661, 0.5287, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.1322,\n",
      "                      0.2643, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.0661,\n",
      "                      0.0661, 0.0661, 0.0661, 0.0661, 0.0661, 0.1322]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=76, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [     85,     174,     243,  ..., 1047813, 1048456,\n",
      "                        1048506]]),\n",
      "       values=tensor([1.6519e-05, 1.6519e-05, 3.3038e-05,  ...,\n",
      "                      3.3038e-05, 1.6519e-05, 1.6519e-05]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=7910, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   7663,   12964,   23852,   37299,   44647,   51738,\n",
      "                          65164,   72242,   75066,   83224,   96677,  102044,\n",
      "                         104343,  108799,  122688,  134676,  156310,  160635,\n",
      "                         168438,  180414,  189070,  192533,  198194,  199905,\n",
      "                         202578,  209687,  233834,  247659,  249893,  260032,\n",
      "                         261287,  306588,  325733,  332873,  333249,  335594,\n",
      "                         343713,  354977,  363402,  365508,  378098,  379610,\n",
      "                         387864,  401992,  409865,  420474,  423505,  424131,\n",
      "                         431615,  449974,  457988,  463390,  464484,  466563,\n",
      "                         480547,  492905,  505726,  507438,  520755,  521557,\n",
      "                         530027,  533850,  610906,  611746,  613877,  632017,\n",
      "                         634572,  642260,  667024,  673862,  684696,  688111,\n",
      "                         696949,  703380,  721669,  748087,  782487,  789080,\n",
      "                         798860,  799928,  806225,  817497,  822945,  825319,\n",
      "                         847848,  851931,  852769,  865475,  879904,  891788,\n",
      "                         895122,  912518,  937400,  949320,  949974,  971070,\n",
      "                         978028,  981289,  982041,  982773,  993256,  994376,\n",
      "                         999983, 1014259, 1038762]]),\n",
      "       values=tensor([0.0462, 0.0462, 0.0462, 0.1387, 0.0462, 0.0462, 0.0925,\n",
      "                      0.0925, 0.0462, 0.0462, 0.0925, 0.0462, 0.0462, 0.0462,\n",
      "                      0.2774, 0.0462, 0.0462, 0.0462, 0.0462, 0.0925, 0.0462,\n",
      "                      0.0462, 0.0925, 0.0462, 0.0462, 0.0925, 0.0462, 0.0925,\n",
      "                      0.0462, 0.0462, 0.0925, 0.0925, 0.0925, 0.0462, 0.0462,\n",
      "                      0.0462, 0.0462, 0.0462, 0.0462, 0.0462, 0.0462, 0.0462,\n",
      "                      0.0462, 0.0925, 0.0462, 0.0925, 0.0925, 0.1849, 0.0462,\n",
      "                      0.0925, 0.1387, 0.0462, 0.0925, 0.0925, 0.0462, 0.1387,\n",
      "                      0.0462, 0.0462, 0.0462, 0.0462, 0.0462, 0.0462, 0.0462,\n",
      "                      0.0462, 0.0462, 0.0925, 0.0462, 0.0462, 0.0462, 0.1849,\n",
      "                      0.3236, 0.0462, 0.0462, 0.0925, 0.0462, 0.3698, 0.0462,\n",
      "                      0.0462, 0.0925, 0.0462, 0.0925, 0.2774, 0.0925, 0.0462,\n",
      "                      0.0462, 0.0462, 0.0925, 0.2774, 0.0462, 0.0462, 0.0925,\n",
      "                      0.0462, 0.0462, 0.0925, 0.0462, 0.0925, 0.0462, 0.0462,\n",
      "                      0.0462, 0.0462, 0.0925, 0.1849, 0.0462, 0.0462, 0.0462]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=105, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4993,   30089,   37299,   56144,   72242,   83224,\n",
      "                         123158,  126714,  129255,  134676,  143719,  149895,\n",
      "                         160635,  171836,  172527,  180652,  181854,  189774,\n",
      "                         202578,  203193,  204128,  219854,  244038,  248539,\n",
      "                         250486,  252762,  256425,  261287,  264768,  282085,\n",
      "                         284011,  290039,  341769,  342567,  343837,  349680,\n",
      "                         360380,  365508,  367697,  372974,  377412,  391421,\n",
      "                         419708,  421465,  444746,  463390,  477587,  483314,\n",
      "                         507438,  527676,  541775,  555920,  562081,  573374,\n",
      "                         577428,  583245,  600521,  612602,  634572,  666657,\n",
      "                         684696,  688111,  730608,  742242,  821169,  843846,\n",
      "                         847877,  856162,  879743,  891788,  896891,  901611,\n",
      "                         906532,  938177,  943000,  973303,  982773,  983046,\n",
      "                         989394,  996215, 1012100, 1014259, 1031773]]),\n",
      "       values=tensor([0.0373, 0.0373, 0.0373, 0.0373, 0.0746, 0.0373, 0.0373,\n",
      "                      0.0373, 0.0746, 0.1865, 0.1119, 0.0373, 0.1119, 0.0746,\n",
      "                      0.0373, 0.0746, 0.2238, 0.0373, 0.0373, 0.0373, 0.1119,\n",
      "                      0.0373, 0.0373, 0.2238, 0.0373, 0.0373, 0.0373, 0.0746,\n",
      "                      0.1492, 0.0373, 0.0746, 0.0373, 0.0746, 0.0373, 0.0373,\n",
      "                      0.0373, 0.0373, 0.0373, 0.0373, 0.0373, 0.0373, 0.0373,\n",
      "                      0.1492, 0.1492, 0.0373, 0.0373, 0.0373, 0.0373, 0.0373,\n",
      "                      0.0373, 0.0373, 0.0746, 0.0373, 0.0373, 0.0373, 0.0373,\n",
      "                      0.0373, 0.2238, 0.0746, 0.0746, 0.2238, 0.0373, 0.5221,\n",
      "                      0.0373, 0.0373, 0.0373, 0.0746, 0.0373, 0.1119, 0.0373,\n",
      "                      0.0746, 0.0373, 0.0373, 0.4475, 0.0746, 0.0373, 0.0373,\n",
      "                      0.0373, 0.0373, 0.1492, 0.0373, 0.0373, 0.0746]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=83, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   3111,    4993,    7698,  ..., 1034292, 1039087,\n",
      "                        1045737]]),\n",
      "       values=tensor([0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144,\n",
      "                      0.0289, 0.0144, 0.0144, 0.0144, 0.0289, 0.0722, 0.0144,\n",
      "                      0.1444, 0.0144, 0.0289, 0.0144, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0289, 0.1011, 0.0144, 0.0144,\n",
      "                      0.0578, 0.0144, 0.0144, 0.0289, 0.0144, 0.0289, 0.0289,\n",
      "                      0.0144, 0.1300, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144,\n",
      "                      0.0578, 0.0144, 0.0289, 0.0144, 0.0289, 0.0144, 0.0289,\n",
      "                      0.0289, 0.0144, 0.0144, 0.0144, 0.0289, 0.0289, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0722, 0.0144, 0.0289, 0.0144, 0.0144, 0.1011, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0578, 0.0144, 0.0144,\n",
      "                      0.0289, 0.0289, 0.0144, 0.0289, 0.0144, 0.0144, 0.0578,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0289, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0289, 0.0144, 0.0289, 0.0433, 0.0144, 0.0144, 0.3322,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0144, 0.0867,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0144, 0.0289, 0.0289, 0.3611,\n",
      "                      0.0433, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.1155,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0578, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0433, 0.0144, 0.0433, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0289, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0144, 0.0433, 0.0144, 0.0144,\n",
      "                      0.0433, 0.0144, 0.0144, 0.0144, 0.0433, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0867, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0867, 0.0144, 0.1300, 0.0144,\n",
      "                      0.0144, 0.0867, 0.0144, 0.0289, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0578, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0867, 0.0578, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0289, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0722, 0.0433, 0.0144, 0.0144, 0.0289, 0.0433, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0433, 0.0144, 0.0289, 0.0433, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0867, 0.0144, 0.0578,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.3322,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0433, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0433, 0.0144, 0.0144, 0.0289, 0.0289,\n",
      "                      0.0144, 0.0433, 0.0433, 0.0144, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0289, 0.0289, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0289, 0.0144, 0.0144, 0.0289, 0.0578,\n",
      "                      0.0144, 0.0578, 0.0144, 0.0144, 0.0433, 0.0144, 0.0289,\n",
      "                      0.1011, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0144, 0.0578,\n",
      "                      0.0289, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0144, 0.0144, 0.0433, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0433, 0.0144, 0.0144, 0.1011,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0289, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0289, 0.0433, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0578, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0144, 0.0578,\n",
      "                      0.0289, 0.0144, 0.0722, 0.0144, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.0867, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0578, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0722, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0578, 0.0144, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0144, 0.0433, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0433,\n",
      "                      0.0144, 0.0144, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0578, 0.0144, 0.0433, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.1155, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0867, 0.0144, 0.0722, 0.0144, 0.0289,\n",
      "                      0.0144, 0.2022, 0.0144, 0.0144, 0.0289, 0.0289, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0433, 0.0144, 0.0722, 0.0289, 0.0289, 0.0289, 0.0144,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0144, 0.0433, 0.0289, 0.0144,\n",
      "                      0.0433, 0.0289, 0.0289, 0.0144, 0.0144, 0.0722, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0722, 0.0144, 0.0433, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0289, 0.0433, 0.0578,\n",
      "                      0.0289, 0.0722, 0.0144, 0.0433, 0.0144, 0.0867, 0.0144,\n",
      "                      0.0289, 0.1444, 0.0144, 0.0144, 0.0578, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0433, 0.0433, 0.0144,\n",
      "                      0.0144, 0.0289, 0.0144, 0.0289, 0.0144, 0.0144, 0.0578,\n",
      "                      0.0289, 0.0433, 0.0289, 0.0144, 0.0144, 0.0144, 0.0289,\n",
      "                      0.0144, 0.0144, 0.0433, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0578, 0.0144, 0.0144, 0.0289, 0.0289, 0.0144,\n",
      "                      0.0289, 0.0289, 0.0289, 0.0144, 0.0144, 0.0867, 0.0144,\n",
      "                      0.0867, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0722, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0144, 0.0144, 0.0144, 0.0578, 0.0144,\n",
      "                      0.0144, 0.0144, 0.0578, 0.0144, 0.0289, 0.0289, 0.0722,\n",
      "                      0.0144, 0.0289, 0.0433, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0433, 0.0144, 0.0144, 0.0144, 0.0289, 0.0144,\n",
      "                      0.0289, 0.0289, 0.0144, 0.0144, 0.0144, 0.0144, 0.0144,\n",
      "                      0.0144, 0.0144]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=590, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,     333,     366,  ..., 1047825, 1047885,\n",
      "                        1048137]]),\n",
      "       values=tensor([0.0009, 0.0009, 0.0188,  ..., 0.0043, 0.0009, 0.0043]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=4117, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  50793,   72242,   83224,  109992,  116557,  122878,\n",
      "                         124600,  131210,  134396,  150501,  169408,  179131,\n",
      "                         181182,  183122,  202578,  210071,  220895,  221086,\n",
      "                         261287,  262997,  284611,  290665,  302597,  302747,\n",
      "                         306588,  333066,  343799,  350117,  353478,  387864,\n",
      "                         389282,  406627,  440710,  455086,  463390,  476114,\n",
      "                         483878,  492029,  499894,  520755,  533850,  604812,\n",
      "                         608527,  619417,  620996,  644938,  647864,  659405,\n",
      "                         666813,  676378,  686604,  688111,  689676,  710615,\n",
      "                         720228,  728086,  748220,  752031,  756012,  796613,\n",
      "                         799928,  806225,  845609,  849465,  891788,  907997,\n",
      "                         910457,  912518,  914583,  936007,  943000,  982773,\n",
      "                         986508,  989394, 1029044, 1031773]]),\n",
      "       values=tensor([0.0549, 0.1098, 0.0549, 0.0549, 0.0549, 0.4391, 0.0549,\n",
      "                      0.1646, 0.3842, 0.1098, 0.0549, 0.0549, 0.0549, 0.0549,\n",
      "                      0.0549, 0.0549, 0.0549, 0.0549, 0.1098, 0.2195, 0.1098,\n",
      "                      0.0549, 0.0549, 0.0549, 0.0549, 0.1098, 0.0549, 0.1646,\n",
      "                      0.0549, 0.1646, 0.1098, 0.0549, 0.0549, 0.0549, 0.0549,\n",
      "                      0.0549, 0.0549, 0.0549, 0.0549, 0.0549, 0.0549, 0.1098,\n",
      "                      0.0549, 0.0549, 0.1098, 0.0549, 0.0549, 0.0549, 0.1098,\n",
      "                      0.1098, 0.1098, 0.0549, 0.1098, 0.0549, 0.0549, 0.0549,\n",
      "                      0.0549, 0.1098, 0.0549, 0.0549, 0.1098, 0.2195, 0.0549,\n",
      "                      0.1098, 0.0549, 0.2744, 0.1646, 0.0549, 0.0549, 0.1098,\n",
      "                      0.0549, 0.0549, 0.1098, 0.1646, 0.0549, 0.0549]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=76, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   8356,   15588,   25138,   40137,   42516,   42610,\n",
      "                          43518,   44743,   60810,   72242,   83224,   85919,\n",
      "                          86356,  101071,  116557,  124600,  127526,  150957,\n",
      "                         160635,  162675,  182887,  188762,  200342,  230483,\n",
      "                         231149,  236037,  236858,  237435,  246056,  248539,\n",
      "                         261287,  302597,  303929,  311856,  314478,  332445,\n",
      "                         332497,  335594,  342154,  347624,  349124,  349136,\n",
      "                         360380,  361492,  373347,  373756,  381737,  386462,\n",
      "                         428569,  453049,  463390,  469369,  483314,  488225,\n",
      "                         512021,  520880,  533850,  540129,  636941,  639716,\n",
      "                         639768,  649791,  654930,  663846,  684696,  688111,\n",
      "                         710325,  720228,  720765,  725332,  737524,  787801,\n",
      "                         822200,  827446,  833922,  841877,  842342,  847848,\n",
      "                         888897,  889766,  894091,  907339,  925705,  945916,\n",
      "                         950245,  965993,  982345,  989789,  990799, 1021446,\n",
      "                        1043981]]),\n",
      "       values=tensor([0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0956, 0.0478, 0.0956, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0956, 0.1433, 0.0478, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478,\n",
      "                      0.0478, 0.3823, 0.0956, 0.0478, 0.1433, 0.4300, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478,\n",
      "                      0.0478, 0.0478, 0.2389, 0.0478, 0.0478, 0.0478, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.1433,\n",
      "                      0.0478, 0.0478, 0.0478, 0.1911, 0.0478, 0.1433, 0.0478,\n",
      "                      0.0956, 0.3345, 0.0478, 0.0478, 0.0956, 0.0956, 0.0478,\n",
      "                      0.1911, 0.0956, 0.0478, 0.0956, 0.0478, 0.0478, 0.0478,\n",
      "                      0.0478, 0.2389, 0.0478, 0.0478, 0.0478, 0.0956, 0.0478,\n",
      "                      0.0478, 0.0478, 0.0478, 0.0478, 0.0478, 0.1433, 0.1433]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=91, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    213,     741,    1891,  ..., 1046011, 1048419,\n",
      "                        1048572]]),\n",
      "       values=tensor([0.0744, 0.0057, 0.0057,  ..., 0.0057, 0.0057, 0.0057]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2355, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [    820,    4307,    4747,    4993,    7166,    8608,\n",
      "                          14175,   24711,   25621,   27956,   40982,   43363,\n",
      "                          44647,   46486,   47399,   49723,   50210,   53397,\n",
      "                          55531,   56144,   59671,   60787,   61802,   74987,\n",
      "                          77129,   77362,   88330,   97092,  119937,  120754,\n",
      "                         122688,  124946,  138701,  142761,  144165,  144973,\n",
      "                         153604,  158483,  163377,  163761,  166664,  169246,\n",
      "                         171836,  172293,  173748,  177319,  177590,  180246,\n",
      "                         181182,  183147,  185307,  190065,  194259,  196296,\n",
      "                         201191,  207457,  211210,  222666,  224509,  225027,\n",
      "                         230201,  231149,  235017,  238120,  240459,  253939,\n",
      "                         257154,  257892,  263511,  266125,  269671,  271004,\n",
      "                         284254,  284619,  286738,  291006,  291460,  292391,\n",
      "                         299655,  311111,  315004,  328695,  331110,  333033,\n",
      "                         337399,  340215,  342526,  353478,  357034,  357954,\n",
      "                         359482,  364511,  365309,  369265,  371983,  373694,\n",
      "                         377670,  382925,  388891,  399327,  400089,  402191,\n",
      "                         406752,  408903,  409474,  409865,  411540,  414385,\n",
      "                         422011,  422077,  425005,  430184,  432823,  433749,\n",
      "                         435014,  435510,  436512,  437552,  438849,  444736,\n",
      "                         447812,  453022,  455007,  461326,  463165,  465151,\n",
      "                         476504,  484255,  486132,  487022,  489141,  512021,\n",
      "                         513664,  517405,  520306,  530848,  535288,  546177,\n",
      "                         548716,  551698,  555920,  556492,  558982,  569221,\n",
      "                         571393,  572172,  577442,  579733,  582844,  585734,\n",
      "                         589652,  592311,  599584,  600646,  605027,  605697,\n",
      "                         610636,  613452,  613463,  614694,  615817,  617684,\n",
      "                         630331,  633809,  635828,  636524,  639342,  640762,\n",
      "                         646960,  665332,  666424,  667330,  667368,  673390,\n",
      "                         680495,  680619,  682045,  682387,  685100,  686004,\n",
      "                         686889,  687125,  694550,  694791,  696946,  699933,\n",
      "                         700162,  701958,  703270,  706760,  709886,  711616,\n",
      "                         712954,  716047,  716738,  725407,  725665,  726710,\n",
      "                         735274,  738127,  743770,  743972,  745356,  751728,\n",
      "                         752412,  752530,  754424,  757243,  761599,  763631,\n",
      "                         765697,  767686,  769360,  789080,  794144,  798491,\n",
      "                         799147,  799529,  803669,  804474,  806225,  807003,\n",
      "                         813537,  814017,  817944,  820231,  821862,  822949,\n",
      "                         825123,  826251,  826759,  831214,  835473,  836437,\n",
      "                         837119,  837172,  844352,  845632,  849465,  851238,\n",
      "                         851663,  853599,  857691,  858163,  863387,  866997,\n",
      "                         876035,  879904,  892163,  894356,  901412,  906532,\n",
      "                         907003,  908010,  908405,  908959,  915102,  917258,\n",
      "                         922951,  924042,  925977,  937864,  937938,  940923,\n",
      "                         942529,  945765,  946814,  954855,  958967,  965988,\n",
      "                         967651,  969375,  969771,  969848,  970200,  982773,\n",
      "                         983423,  990225,  990542,  997154,  999153, 1006860,\n",
      "                        1008826, 1009981, 1012100, 1020886, 1023688, 1025814,\n",
      "                        1027359, 1032060, 1037529, 1040169, 1045494, 1045496,\n",
      "                        1047114]]),\n",
      "       values=tensor([0.0560, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.1119, 0.0373, 0.0746, 0.0560, 0.0187, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0933, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0373, 0.0373, 0.0373, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0560, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0373, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0373, 0.0187, 0.0187, 0.0187, 0.0560, 0.0187,\n",
      "                      0.0187, 0.0187, 0.1492, 0.0373, 0.0373, 0.0373, 0.0187,\n",
      "                      0.0187, 0.0560, 0.0187, 0.5036, 0.0187, 0.0560, 0.0187,\n",
      "                      0.0187, 0.0746, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0373, 0.0746, 0.0187,\n",
      "                      0.0187, 0.0373, 0.0373, 0.0187, 0.0373, 0.0187, 0.0373,\n",
      "                      0.3730, 0.0187, 0.0373, 0.0187, 0.0187, 0.0187, 0.0746,\n",
      "                      0.0187, 0.0373, 0.0187, 0.0187, 0.0933, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0560, 0.0373, 0.0187,\n",
      "                      0.0746, 0.1119, 0.0560, 0.0373, 0.0187, 0.1119, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.1119, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0746, 0.0187, 0.0746, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0746, 0.0187, 0.0187, 0.0187, 0.0373, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0373, 0.0373, 0.0560, 0.0560, 0.0187, 0.0560, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0560, 0.0187, 0.2984, 0.0373, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0373, 0.0187, 0.0560, 0.0746,\n",
      "                      0.0373, 0.0560, 0.0187, 0.0373, 0.0373, 0.0373, 0.0187,\n",
      "                      0.0187, 0.1119, 0.0187, 0.0187, 0.0187, 0.0560, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0373, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.2984, 0.0187, 0.0187, 0.0187, 0.0373, 0.0187, 0.0560,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0560, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0373, 0.0187,\n",
      "                      0.0373, 0.0187, 0.1119, 0.0373, 0.0187, 0.0187, 0.1119,\n",
      "                      0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0373, 0.0187, 0.0187, 0.0187,\n",
      "                      0.0187, 0.0373, 0.0187, 0.0373, 0.0187, 0.0373, 0.0373,\n",
      "                      0.0560, 0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0560, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.2052, 0.0187, 0.0560, 0.0187, 0.0373,\n",
      "                      0.0187, 0.0187, 0.0187, 0.0187, 0.0187, 0.0560, 0.0560,\n",
      "                      0.0187]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=295, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   2602,   41680,   44157,   56144,   72242,   81097,\n",
      "                          83224,   87586,  110325,  124185,  134676,  136425,\n",
      "                         144109,  148594,  182887,  186535,  202578,  244516,\n",
      "                         261287,  278741,  290308,  302140,  342043,  342316,\n",
      "                         365508,  377412,  391060,  401412,  407525,  426717,\n",
      "                         444746,  457113,  462769,  463390,  470194,  497646,\n",
      "                         525761,  576996,  598695,  633966,  688111,  689560,\n",
      "                         701891,  706231,  709707,  724861,  729805,  751444,\n",
      "                         818597,  822945,  839265,  861607,  891788,  926952,\n",
      "                         933965,  935777,  936007,  963512,  988070,  990225,\n",
      "                        1014734, 1045737]]),\n",
      "       values=tensor([0.0687, 0.1374, 0.0687, 0.0687, 0.1374, 0.0687, 0.0687,\n",
      "                      0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 0.3434,\n",
      "                      0.0687, 0.0687, 0.0687, 0.0687, 0.2060, 0.0687, 0.0687,\n",
      "                      0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687,\n",
      "                      0.0687, 0.1374, 0.0687, 0.0687, 0.3434, 0.0687, 0.0687,\n",
      "                      0.0687, 0.0687, 0.0687, 0.0687, 0.1374, 0.0687, 0.0687,\n",
      "                      0.0687, 0.0687, 0.0687, 0.2060, 0.2060, 0.0687, 0.0687,\n",
      "                      0.0687, 0.1374, 0.0687, 0.0687, 0.5494, 0.0687, 0.0687,\n",
      "                      0.0687, 0.0687, 0.0687, 0.0687, 0.0687, 0.0687]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=62, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    296,     422,     682,  ..., 1048291, 1048548,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0002, 0.0005, 0.0002,  ..., 0.0002, 0.0002, 0.0071]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=7446, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   3607,   65164,   72242,   73454,  104542,  106683,\n",
      "                         108799,  119937,  122688,  157024,  166844,  181775,\n",
      "                         202893,  204466,  210402,  220644,  237464,  261287,\n",
      "                         270167,  314106,  333199,  349928,  364511,  379135,\n",
      "                         380221,  383531,  386462,  391596,  400949,  403294,\n",
      "                         414310,  416069,  428236,  430909,  433749,  454377,\n",
      "                         463390,  483314,  484243,  514025,  547172,  548553,\n",
      "                         560310,  583245,  608496,  620678,  623149,  623856,\n",
      "                         667879,  674312,  677283,  682190,  688111,  692272,\n",
      "                         693618,  716186,  742242,  746988,  754424,  797503,\n",
      "                         804729,  822945,  823697,  845609,  847877,  859593,\n",
      "                         861445,  868920,  869920,  882660,  883158,  896059,\n",
      "                         908398,  915333,  917258,  941540,  973725,  974076,\n",
      "                         979016,  989640,  999983, 1031679, 1037154, 1037989,\n",
      "                        1038762, 1045546, 1048219]]),\n",
      "       values=tensor([0.0657, 0.0657, 0.1313, 0.1313, 0.0657, 0.0657, 0.0657,\n",
      "                      0.1970, 0.1313, 0.1313, 0.0657, 0.0657, 0.1970, 0.1313,\n",
      "                      0.0657, 0.0657, 0.0657, 0.1313, 0.1313, 0.0657, 0.0657,\n",
      "                      0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657,\n",
      "                      0.1970, 0.1313, 0.0657, 0.0657, 0.1313, 0.0657, 0.1313,\n",
      "                      0.1970, 0.1313, 0.0657, 0.0657, 0.0657, 0.3939, 0.1313,\n",
      "                      0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.1313,\n",
      "                      0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657,\n",
      "                      0.0657, 0.0657, 0.0657, 0.1970, 0.0657, 0.0657, 0.0657,\n",
      "                      0.0657, 0.1970, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657,\n",
      "                      0.0657, 0.0657, 0.1970, 0.0657, 0.1313, 0.1313, 0.1313,\n",
      "                      0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657, 0.0657,\n",
      "                      0.0657, 0.1313, 0.1313]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=87, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  13196,   25335,   31712,   37453,   44157,   59824,\n",
      "                          72242,   86356,   88386,  102865,  156270,  171836,\n",
      "                         172527,  180414,  192544,  202578,  213222,  261287,\n",
      "                         341769,  351843,  365508,  371723,  372858,  376113,\n",
      "                         462085,  463390,  513664,  518126,  518554,  552646,\n",
      "                         565349,  575579,  576996,  594435,  626799,  645475,\n",
      "                         659143,  661456,  688111,  693562,  718616,  724861,\n",
      "                         739468,  748708,  788622,  798860,  799059,  820231,\n",
      "                         823364,  891788,  922873,  926558,  926952,  936007,\n",
      "                         955038,  995585,  997154, 1011084, 1017126, 1030915,\n",
      "                        1031758, 1042448, 1043771, 1045494]]),\n",
      "       values=tensor([0.0720, 0.1440, 0.0720, 0.0720, 0.0720, 0.0720, 0.1440,\n",
      "                      0.1440, 0.1440, 0.1440, 0.0720, 0.1440, 0.0720, 0.1440,\n",
      "                      0.0720, 0.0720, 0.1440, 0.1440, 0.1440, 0.0720, 0.0720,\n",
      "                      0.0720, 0.0720, 0.0720, 0.0720, 0.0720, 0.0720, 0.0720,\n",
      "                      0.0720, 0.1440, 0.0720, 0.0720, 0.0720, 0.0720, 0.0720,\n",
      "                      0.0720, 0.4319, 0.1440, 0.0720, 0.0720, 0.1440, 0.2159,\n",
      "                      0.2879, 0.0720, 0.0720, 0.0720, 0.0720, 0.0720, 0.0720,\n",
      "                      0.0720, 0.1440, 0.1440, 0.2159, 0.0720, 0.0720, 0.0720,\n",
      "                      0.2879, 0.0720, 0.0720, 0.1440, 0.0720, 0.0720, 0.0720,\n",
      "                      0.0720]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=64, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4993,   13196,   28007,   40548,   46055,   47697,\n",
      "                          61944,   75066,   76865,   81097,   86356,   89986,\n",
      "                          94254,  100564,  102865,  104762,  115069,  116199,\n",
      "                         121978,  127887,  128262,  134541,  140490,  141198,\n",
      "                         142842,  148594,  157431,  172475,  180414,  183580,\n",
      "                         185374,  195158,  200342,  211900,  219517,  233752,\n",
      "                         244038,  244107,  253557,  256417,  261287,  261462,\n",
      "                         267650,  267807,  268018,  269707,  278245,  284611,\n",
      "                         286447,  290665,  292399,  309677,  311856,  327219,\n",
      "                         330919,  332445,  332873,  342183,  342526,  343799,\n",
      "                         345545,  345966,  348530,  349124,  353478,  359658,\n",
      "                         371276,  372525,  377412,  383511,  386462,  387864,\n",
      "                         389199,  390990,  398738,  402535,  428569,  432152,\n",
      "                         432528,  435205,  436494,  454194,  462085,  462145,\n",
      "                         477713,  483314,  497597,  506909,  507438,  507879,\n",
      "                         513726,  516086,  516649,  517428,  520880,  530921,\n",
      "                         544889,  547874,  564496,  567498,  570047,  578971,\n",
      "                         581096,  584282,  584374,  588282,  592295,  592311,\n",
      "                         596938,  603388,  606366,  619113,  623066,  632102,\n",
      "                         639716,  660042,  667522,  670091,  676606,  677283,\n",
      "                         684495,  686106,  689294,  693196,  694222,  698875,\n",
      "                         704261,  706231,  720228,  737617,  737742,  756637,\n",
      "                         758474,  761760,  763656,  766687,  766777,  773019,\n",
      "                         776577,  777475,  782539,  785396,  798227,  798377,\n",
      "                         806546,  808455,  808754,  813363,  814112,  816717,\n",
      "                         820231,  822945,  825495,  854092,  858863,  861445,\n",
      "                         861694,  870239,  871194,  874639,  876018,  886737,\n",
      "                         887833,  888049,  893493,  895310,  899662,  906532,\n",
      "                         908010,  910749,  911902,  917258,  920646,  926952,\n",
      "                         930539,  933164,  940919,  950245,  958967,  961694,\n",
      "                         961927,  967272,  968224,  972033,  978028,  988319,\n",
      "                         989394,  996895, 1004270, 1005358, 1005750, 1011917,\n",
      "                        1014259, 1022866, 1023442, 1027278, 1027345, 1032156,\n",
      "                        1033052, 1039313, 1041634]]),\n",
      "       values=tensor([0.0806, 0.0538, 0.0269, 0.0269, 0.0538, 0.0269, 0.0269,\n",
      "                      0.2688, 0.0538, 0.0269, 0.0806, 0.1344, 0.1075, 0.0538,\n",
      "                      0.0538, 0.0538, 0.0269, 0.0269, 0.0538, 0.0269, 0.1075,\n",
      "                      0.0269, 0.0269, 0.0269, 0.0538, 0.1075, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0269, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0538, 0.0269, 0.0269, 0.1344, 0.2688,\n",
      "                      0.0269, 0.0538, 0.0806, 0.0269, 0.0269, 0.0269, 0.0538,\n",
      "                      0.0269, 0.0269, 0.0269, 0.0269, 0.0269, 0.0538, 0.0269,\n",
      "                      0.0269, 0.0269, 0.0538, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.2957, 0.0538, 0.0538, 0.0538, 0.0269, 0.0269, 0.0538,\n",
      "                      0.0269, 0.0269, 0.0269, 0.0269, 0.0269, 0.0269, 0.0806,\n",
      "                      0.0269, 0.0269, 0.0269, 0.2150, 0.0538, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0269, 0.0538, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0806, 0.0269, 0.0269, 0.0269, 0.0269, 0.0269, 0.0538,\n",
      "                      0.0538, 0.0538, 0.0269, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0538, 0.2957, 0.0269, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0269, 0.1075, 0.0269, 0.0538, 0.1075, 0.0538,\n",
      "                      0.0538, 0.0538, 0.0269, 0.0269, 0.0269, 0.0538, 0.0269,\n",
      "                      0.0269, 0.0269, 0.1344, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.1075, 0.0538, 0.0538, 0.0269, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0806, 0.0269, 0.1075, 0.0269, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0269, 0.0269, 0.0538, 0.0269, 0.0538,\n",
      "                      0.0538, 0.0538, 0.0538, 0.0269, 0.0538, 0.0538, 0.1882,\n",
      "                      0.0806, 0.0538, 0.0538, 0.0269, 0.0269, 0.0538, 0.1075,\n",
      "                      0.0269, 0.0269, 0.0538, 0.0538, 0.0269, 0.1882, 0.0538,\n",
      "                      0.0269, 0.0269, 0.0269, 0.1075, 0.0269, 0.0538, 0.0806,\n",
      "                      0.0538, 0.0538, 0.0538, 0.1882, 0.2688, 0.0538, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0269, 0.0269, 0.0538, 0.0269, 0.0269,\n",
      "                      0.0269, 0.0538, 0.0269, 0.0538, 0.0269]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=201, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   4187,    4993,   11565,  ..., 1045496, 1045536,\n",
      "                        1047144]]),\n",
      "       values=tensor([0.0116, 0.0348, 0.0116, 0.0116, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0232, 0.0348, 0.0232, 0.0232, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0232, 0.0580, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0348, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0348, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0232, 0.0232, 0.0116, 0.0116, 0.0464, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0348, 0.0116, 0.0116, 0.0232,\n",
      "                      0.1740, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0232, 0.0232, 0.1044,\n",
      "                      0.0580, 0.0116, 0.0116, 0.0232, 0.3016, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0464, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.2204, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.2784, 0.0116, 0.0232, 0.0116, 0.0464, 0.0116, 0.0116,\n",
      "                      0.0232, 0.0116, 0.0464, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0348, 0.0116, 0.0232, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0812, 0.0116, 0.0348, 0.0232, 0.1508,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0348, 0.0232, 0.0116, 0.0232, 0.0232, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0464, 0.0116, 0.0232, 0.0116, 0.0232, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0580, 0.0580,\n",
      "                      0.0116, 0.0116, 0.0812, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0232, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0580, 0.0116, 0.0116,\n",
      "                      0.0348, 0.0116, 0.0580, 0.0696, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0464, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232, 0.0232,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0232, 0.0232, 0.0116, 0.0116, 0.0116, 0.0464, 0.0232,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0348, 0.0232, 0.0116, 0.0464, 0.0232, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0348,\n",
      "                      0.0348, 0.0116, 0.0116, 0.0928, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.1508, 0.0116, 0.0348, 0.0232,\n",
      "                      0.1624, 0.0232, 0.0116, 0.0348, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0232, 0.0232, 0.0116,\n",
      "                      0.0696, 0.0116, 0.0232, 0.0116, 0.0232, 0.0116, 0.0464,\n",
      "                      0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0232, 0.0232, 0.0232, 0.0232,\n",
      "                      0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0696, 0.0116,\n",
      "                      0.0116, 0.0348, 0.0116, 0.0116, 0.0232, 0.0232, 0.0116,\n",
      "                      0.0348, 0.0116, 0.0812, 0.0116, 0.0116, 0.0348, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.2552, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0232, 0.0116, 0.0116, 0.0348, 0.0116, 0.0232, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0348, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0580, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0580, 0.0348, 0.0232, 0.0116, 0.0232, 0.0116, 0.0232,\n",
      "                      0.0464, 0.0116, 0.0116, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0580, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0580, 0.0116, 0.0116, 0.0348, 0.0116,\n",
      "                      0.0232, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0348, 0.0464, 0.0232, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0232, 0.0232, 0.0116, 0.1160, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0232, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0696, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0464, 0.0348, 0.0232, 0.0116, 0.0696, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0348, 0.0116, 0.0232, 0.0232, 0.0116,\n",
      "                      0.0232, 0.0116, 0.0116, 0.0232, 0.0464, 0.0116, 0.0116,\n",
      "                      0.0116, 0.1856, 0.0116, 0.0348, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0464, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0348, 0.0232, 0.0580, 0.0116,\n",
      "                      0.2900, 0.0116, 0.0116, 0.0116, 0.0232, 0.0464, 0.0116,\n",
      "                      0.0464, 0.0116, 0.0464, 0.0116, 0.0116, 0.1044, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0348, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0232, 0.2204, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0232, 0.0232, 0.0116, 0.0232, 0.0116, 0.0116,\n",
      "                      0.0232, 0.1508, 0.0116, 0.0232, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0232, 0.0116, 0.0580, 0.0116, 0.0116,\n",
      "                      0.2088, 0.0580, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0232, 0.0116, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.0116, 0.0116, 0.0116, 0.0116, 0.0464, 0.0232,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0348,\n",
      "                      0.0232, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0116, 0.0116, 0.0116, 0.0232,\n",
      "                      0.0116, 0.0116, 0.0464, 0.0232, 0.0116, 0.1276, 0.0116,\n",
      "                      0.0116, 0.0232, 0.0116, 0.0232, 0.0116, 0.0232, 0.0116,\n",
      "                      0.0116, 0.1392, 0.0116, 0.0116, 0.0116, 0.0116]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=692, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     215,     235,  ..., 1047825, 1048462,\n",
      "                        1048572]]),\n",
      "       values=tensor([0.0015, 0.0015, 0.0015,  ..., 0.0015, 0.0023, 0.0008]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=4378, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4993,    5237,   13196,   13205,   25335,   36060,\n",
      "                          52153,   72242,   73732,   83224,   86356,   93860,\n",
      "                          94039,  103911,  106683,  152833,  160635,  184116,\n",
      "                         188122,  218949,  220895,  230201,  236037,  241656,\n",
      "                         261287,  276503,  279249,  313691,  353478,  358163,\n",
      "                         366000,  366799,  379610,  390172,  421640,  430127,\n",
      "                         438129,  440835,  444746,  463390,  470393,  483314,\n",
      "                         484243,  489141,  497646,  525432,  525567,  544891,\n",
      "                         555920,  569221,  573374,  574229,  586305,  601654,\n",
      "                         620996,  623618,  643867,  644065,  645475,  649227,\n",
      "                         669342,  669458,  674972,  688111,  696949,  699565,\n",
      "                         703380,  705444,  706231,  710615,  736833,  741217,\n",
      "                         754424,  778315,  781946,  788108,  797633,  800415,\n",
      "                         827473,  838113,  839867,  842439,  851931,  860529,\n",
      "                         865612,  871194,  907489,  913870,  922873,  944378,\n",
      "                         958387,  961291,  970543,  970944,  972095,  974564,\n",
      "                         990225,  998969, 1001534, 1025557, 1030915]]),\n",
      "       values=tensor([0.0443, 0.0443, 0.3321, 0.0664, 0.0221, 0.1329, 0.0221,\n",
      "                      0.0111, 0.0443, 0.0443, 0.0886, 0.1329, 0.0111, 0.1107,\n",
      "                      0.0221, 0.0886, 0.0443, 0.1107, 0.0886, 0.2214, 0.0886,\n",
      "                      0.0221, 0.0443, 0.0111, 0.0996, 0.0443, 0.2214, 0.0221,\n",
      "                      0.0443, 0.1107, 0.0664, 0.0111, 0.0886, 0.2879, 0.0886,\n",
      "                      0.0443, 0.0111, 0.2436, 0.0886, 0.0554, 0.0886, 0.0221,\n",
      "                      0.0886, 0.0443, 0.0221, 0.1550, 0.0886, 0.0221, 0.0664,\n",
      "                      0.0443, 0.0221, 0.0664, 0.0221, 0.1329, 0.0443, 0.1882,\n",
      "                      0.0886, 0.0886, 0.1550, 0.1329, 0.0554, 0.0221, 0.1550,\n",
      "                      0.0111, 0.0443, 0.1329, 0.0221, 0.0886, 0.0886, 0.0996,\n",
      "                      0.0221, 0.0664, 0.0221, 0.0886, 0.0443, 0.0886, 0.0443,\n",
      "                      0.0221, 0.0886, 0.0664, 0.0443, 0.0443, 0.0443, 0.0221,\n",
      "                      0.1550, 0.0443, 0.1993, 0.1107, 0.0221, 0.0221, 0.0443,\n",
      "                      0.0221, 0.0886, 0.0443, 0.0111, 0.0664, 0.0221, 0.2768,\n",
      "                      0.0111, 0.0443, 0.1107]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=101, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,     352,    3607,  ..., 1044417, 1045494,\n",
      "                        1045496]]),\n",
      "       values=tensor([0.0377, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0283, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0283, 0.0188, 0.0094, 0.0094, 0.0188, 0.0565,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0188,\n",
      "                      0.0094, 0.0188, 0.0471, 0.0094, 0.0188, 0.0094, 0.0283,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0565, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0094, 0.2637, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0188, 0.0188, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0283, 0.0188, 0.0094, 0.0283, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0094, 0.0188, 0.1695, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0377, 0.0942, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0377, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0471, 0.0094, 0.0094, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0377, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0283, 0.0094, 0.0094, 0.0377, 0.0094,\n",
      "                      0.0377, 0.0094, 0.0283, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.1130, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0283, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0565, 0.0094, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0094, 0.0283, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0188, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0471, 0.0753, 0.0094, 0.0188,\n",
      "                      0.0188, 0.0471, 0.0094, 0.0094, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0565, 0.0094, 0.0188, 0.0094, 0.0094, 0.4426, 0.0094,\n",
      "                      0.0283, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0377, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.5557,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0848,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0377, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0377, 0.0094, 0.0188, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0377, 0.0471, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0188, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0283, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0283, 0.0094, 0.0094, 0.0188, 0.0188, 0.0283,\n",
      "                      0.0471, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0188, 0.0188, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0377, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0283, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.3390, 0.0094, 0.0377, 0.0094, 0.0283,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0188, 0.0283,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0377, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0094, 0.0188, 0.0094, 0.0188,\n",
      "                      0.0283, 0.0283, 0.0094, 0.0094, 0.0283, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0471, 0.0188, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0283, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0283, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0188, 0.0283, 0.0094, 0.0094, 0.0377, 0.0188,\n",
      "                      0.0094, 0.0283, 0.0283, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0283, 0.0094, 0.0188, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0377, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0377, 0.0094, 0.0188, 0.0283, 0.0188,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0659,\n",
      "                      0.0094, 0.0094, 0.0188, 0.0094, 0.1130, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0283, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0283, 0.0377, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.1507, 0.0094,\n",
      "                      0.0094, 0.0377, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0094, 0.0283, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0565, 0.0188, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0283, 0.0094,\n",
      "                      0.0094, 0.0565, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0471, 0.0094, 0.0188, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0471, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0188, 0.0094, 0.0188, 0.0094, 0.0094, 0.0377,\n",
      "                      0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0283, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0283, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0283, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0283, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094, 0.0188,\n",
      "                      0.0188, 0.0094, 0.0188, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094, 0.0094,\n",
      "                      0.0659, 0.0094, 0.0094, 0.0094, 0.0094, 0.0753, 0.0377,\n",
      "                      0.0094, 0.0094, 0.0094, 0.0188, 0.0094, 0.0094, 0.0283,\n",
      "                      0.0565, 0.0094]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=793, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,    1151,    2547,  ..., 1045737, 1047114,\n",
      "                        1047144]]),\n",
      "       values=tensor([0.0074, 0.0074, 0.0074,  ..., 0.0148, 0.0074, 0.0221]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1051, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   4993,   25335,   33618,   37299,   72242,   75066,\n",
      "                          77469,   83224,  102865,  104542,  108799,  116557,\n",
      "                         124600,  135242,  144109,  180414,  181121,  181854,\n",
      "                         202578,  207087,  225521,  237464,  261287,  284011,\n",
      "                         286447,  306588,  311856,  363668,  380983,  387864,\n",
      "                         441925,  463390,  483314,  489861,  513664,  533814,\n",
      "                         533850,  548716,  555920,  562081,  604812,  606366,\n",
      "                         620870,  649227,  656850,  662533,  666657,  679663,\n",
      "                         682190,  688111,  706231,  720228,  720288,  724105,\n",
      "                         747377,  758474,  777475,  798860,  813020,  825437,\n",
      "                         842188,  845609,  847848,  879743,  887069,  891788,\n",
      "                         919814,  968224,  971070,  996949, 1005750, 1011474,\n",
      "                        1029044, 1031773]]),\n",
      "       values=tensor([0.0549, 0.1646, 0.1098, 0.0549, 0.1098, 0.0549, 0.1098,\n",
      "                      0.0549, 0.1098, 0.0549, 0.0549, 0.0549, 0.1098, 0.0549,\n",
      "                      0.1098, 0.0549, 0.0549, 0.3842, 0.0549, 0.1098, 0.1098,\n",
      "                      0.0549, 0.1098, 0.0549, 0.2195, 0.0549, 0.1646, 0.0549,\n",
      "                      0.0549, 0.1098, 0.0549, 0.0549, 0.0549, 0.0549, 0.1098,\n",
      "                      0.1098, 0.0549, 0.0549, 0.0549, 0.0549, 0.0549, 0.0549,\n",
      "                      0.0549, 0.0549, 0.0549, 0.4939, 0.0549, 0.0549, 0.0549,\n",
      "                      0.0549, 0.0549, 0.0549, 0.0549, 0.2744, 0.1098, 0.0549,\n",
      "                      0.0549, 0.0549, 0.0549, 0.0549, 0.0549, 0.0549, 0.2195,\n",
      "                      0.0549, 0.1098, 0.0549, 0.0549, 0.1098, 0.0549, 0.0549,\n",
      "                      0.0549, 0.2195, 0.0549, 0.0549]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=74, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  13205,   15564,   42127,   72242,   73454,   92300,\n",
      "                         126586,  199544,  202893,  218949,  220644,  236159,\n",
      "                         260140,  261287,  306180,  312820,  315286,  342345,\n",
      "                         348936,  350158,  380221,  380611,  390168,  403294,\n",
      "                         409518,  410496,  428236,  463390,  492068,  499584,\n",
      "                         514025,  532647,  542727,  590331,  649727,  671752,\n",
      "                         688111,  692051,  702425,  799339,  800066,  815971,\n",
      "                         817497,  868920,  888506,  893290,  897961,  931633,\n",
      "                         961885,  988070, 1031297]]),\n",
      "       values=tensor([0.0917, 0.0917, 0.0917, 0.1833, 0.1833, 0.1833, 0.0917,\n",
      "                      0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.1833, 0.1833,\n",
      "                      0.1833, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917,\n",
      "                      0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917,\n",
      "                      0.2750, 0.1833, 0.0917, 0.2750, 0.1833, 0.1833, 0.1833,\n",
      "                      0.0917, 0.0917, 0.1833, 0.0917, 0.0917, 0.0917, 0.2750,\n",
      "                      0.0917, 0.2750, 0.0917, 0.0917, 0.0917, 0.0917, 0.0917,\n",
      "                      0.1833, 0.0917]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=51, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [   1616,    7821,   18781,   18973,   19433,   30924,\n",
      "                          31180,   50706,   55536,   55925,   72718,   77447,\n",
      "                          89219,   90194,   93119,   98974,  113223,  114093,\n",
      "                         123950,  127284,  134966,  157841,  179131,  184878,\n",
      "                         194568,  243399,  251193,  252766,  258306,  267474,\n",
      "                         274689,  286027,  290739,  293749,  306270,  327723,\n",
      "                         339126,  341398,  365456,  366000,  380196,  387791,\n",
      "                         396690,  410316,  424130,  425205,  431619,  432588,\n",
      "                         434653,  440710,  454602,  473012,  499383,  500872,\n",
      "                         507182,  520070,  527858,  554751,  562478,  564039,\n",
      "                         581096,  581315,  588372,  591328,  595225,  617684,\n",
      "                         623567,  637071,  641929,  659403,  660644,  663669,\n",
      "                         670091,  682190,  691802,  726121,  731435,  732272,\n",
      "                         745258,  750162,  754176,  760913,  779216,  782957,\n",
      "                         817703,  821088,  831360,  839866,  846972,  857691,\n",
      "                         874361,  880217,  884905,  903201,  910749,  925977,\n",
      "                         926252,  930913,  948638,  949530,  952232,  956430,\n",
      "                         957152,  962529,  962588,  977484,  982773,  983463,\n",
      "                         985997,  998850,  999682, 1000538, 1000926, 1002193,\n",
      "                        1009184, 1009252, 1010799, 1014259, 1015614, 1024725,\n",
      "                        1026788, 1027888, 1031758, 1031773]]),\n",
      "       values=tensor([0.1567, 0.0783, 0.0783, 0.0783, 0.1567, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.1567, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.1567, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.3916, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.1567, 0.0783, 0.0783, 0.0783,\n",
      "                      0.0783, 0.0783, 0.0783, 0.0783, 0.0783]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=124, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4993,   13583,   26887,   28007,   44157,   72242,\n",
      "                          77469,   83224,   89986,  100875,  104712,  116557,\n",
      "                         122692,  126154,  129255,  132977,  134676,  145887,\n",
      "                         148594,  150192,  160635,  171499,  171836,  172527,\n",
      "                         181854,  183122,  191057,  192373,  202578,  220895,\n",
      "                         222127,  236037,  245421,  249022,  256417,  256425,\n",
      "                         261287,  269076,  271626,  283449,  284011,  286447,\n",
      "                         293798,  302597,  311111,  311856,  313062,  326616,\n",
      "                         327219,  333255,  335594,  336478,  337898,  353478,\n",
      "                         356488,  366799,  367697,  368409,  388892,  390172,\n",
      "                         398532,  404216,  406627,  440866,  451849,  457113,\n",
      "                         463390,  466337,  471501,  473703,  478084,  483567,\n",
      "                         497646,  507438,  512021,  522112,  525432,  526300,\n",
      "                         526453,  532357,  533850,  534719,  541775,  554976,\n",
      "                         576996,  595785,  611672,  618399,  618813,  627191,\n",
      "                         627392,  634572,  643213,  646791,  654930,  657736,\n",
      "                         666657,  676606,  688111,  690661,  691802,  706231,\n",
      "                         718167,  720228,  736823,  752031,  752393,  754424,\n",
      "                         758474,  778214,  782597,  798860,  808269,  851663,\n",
      "                         851804,  851931,  856162,  857571,  871194,  891788,\n",
      "                         896891,  906973,  909668,  919814,  921200,  926952,\n",
      "                         935585,  940916,  956214,  960199,  964894,  977772,\n",
      "                         978350,  978789,  982773, 1005750, 1014259, 1031297,\n",
      "                        1033635, 1037785, 1042159]]),\n",
      "       values=tensor([0.0495, 0.0248, 0.0248, 0.0495, 0.0248, 0.0495, 0.0991,\n",
      "                      0.0495, 0.0248, 0.0495, 0.0248, 0.0495, 0.0743, 0.0248,\n",
      "                      0.0248, 0.0248, 0.0991, 0.0248, 0.1734, 0.1238, 0.1734,\n",
      "                      0.0248, 0.0248, 0.1238, 0.2229, 0.0495, 0.0248, 0.0248,\n",
      "                      0.0495, 0.0248, 0.0248, 0.0743, 0.0248, 0.0495, 0.0248,\n",
      "                      0.0495, 0.1486, 0.3963, 0.0495, 0.1238, 0.0495, 0.1238,\n",
      "                      0.0495, 0.0248, 0.0248, 0.1982, 0.0248, 0.0495, 0.0743,\n",
      "                      0.0495, 0.0743, 0.0248, 0.0495, 0.0248, 0.0248, 0.0248,\n",
      "                      0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248,\n",
      "                      0.0248, 0.0248, 0.3220, 0.0743, 0.1238, 0.0248, 0.0248,\n",
      "                      0.0248, 0.0248, 0.0991, 0.0248, 0.0248, 0.1238, 0.0495,\n",
      "                      0.0495, 0.0495, 0.0991, 0.0743, 0.0743, 0.0743, 0.0248,\n",
      "                      0.0248, 0.1238, 0.0743, 0.0495, 0.0248, 0.0248, 0.0248,\n",
      "                      0.0991, 0.0248, 0.0248, 0.0495, 0.0248, 0.0248, 0.0248,\n",
      "                      0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0495,\n",
      "                      0.0248, 0.0248, 0.0248, 0.0248, 0.0248, 0.0743, 0.0248,\n",
      "                      0.0248, 0.0495, 0.0495, 0.0248, 0.0495, 0.0495, 0.0248,\n",
      "                      0.0495, 0.0248, 0.3468, 0.0743, 0.0495, 0.0248, 0.1982,\n",
      "                      0.0248, 0.0248, 0.0495, 0.0495, 0.0248, 0.1238, 0.1486,\n",
      "                      0.0248, 0.0248, 0.0495, 0.0248, 0.0248, 0.1734, 0.0248,\n",
      "                      0.0495]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=141, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  12260,   13196,   29667,   49110,   61899,   72242,\n",
      "                          86937,   90681,   99593,  102933,  109067,  116557,\n",
      "                         124896,  157574,  161760,  165922,  172875,  186603,\n",
      "                         199575,  220895,  223620,  226807,  232925,  235267,\n",
      "                         250486,  260327,  261287,  262307,  269029,  270167,\n",
      "                         306588,  325733,  332445,  335594,  365508,  433291,\n",
      "                         436762,  439115,  440125,  463390,  484243,  489008,\n",
      "                         497646,  506577,  514293,  517428,  530361,  544092,\n",
      "                         555920,  561926,  583245,  594744,  597113,  631227,\n",
      "                         640251,  648542,  654930,  659143,  676606,  688111,\n",
      "                         689676,  690661,  712555,  728086,  742242,  743770,\n",
      "                         747186,  754424,  758474,  798860,  799928,  813369,\n",
      "                         822138,  827737,  846781,  874094,  887069,  914583,\n",
      "                         915263,  932301,  961580,  973101,  986508,  989394,\n",
      "                         990225,  994136, 1031297, 1038762]]),\n",
      "       values=tensor([0.0531, 0.0531, 0.0531, 0.0531, 0.0531, 0.0531, 0.0265,\n",
      "                      0.0531, 0.0531, 0.1062, 0.0531, 0.0531, 0.0531, 0.0531,\n",
      "                      0.0265, 0.0531, 0.0265, 0.0531, 0.1593, 0.0531, 0.0531,\n",
      "                      0.2655, 0.0531, 0.0531, 0.0531, 0.0531, 0.0796, 0.0531,\n",
      "                      0.1062, 0.1593, 0.0531, 0.2655, 0.0531, 0.0531, 0.1062,\n",
      "                      0.0531, 0.0531, 0.0265, 0.0531, 0.0265, 0.0531, 0.0531,\n",
      "                      0.0531, 0.0531, 0.0531, 0.0531, 0.0531, 0.3982, 0.0531,\n",
      "                      0.0531, 0.0265, 0.0531, 0.0265, 0.0531, 0.0531, 0.0531,\n",
      "                      0.0531, 0.0531, 0.0531, 0.0265, 0.1593, 0.0531, 0.0531,\n",
      "                      0.1062, 0.0265, 0.0531, 0.0265, 0.1062, 0.0531, 0.0531,\n",
      "                      0.0531, 0.0531, 0.0531, 0.2920, 0.1062, 0.2655, 0.0531,\n",
      "                      0.0531, 0.2389, 0.0531, 0.1593, 0.0531, 0.0796, 0.2655,\n",
      "                      0.1593, 0.1593, 0.1062, 0.1062]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=88, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4993,   15170,   28007,   36060,   37299,   37722,\n",
      "                          44157,   72242,   83224,  109992,  116557,  122692,\n",
      "                         129255,  132443,  144109,  148594,  150192,  160635,\n",
      "                         172527,  177100,  180414,  181854,  182354,  184116,\n",
      "                         192373,  202578,  216752,  219854,  220895,  244516,\n",
      "                         252701,  261287,  269076,  283449,  284011,  286447,\n",
      "                         286463,  314106,  335594,  338688,  340937,  341769,\n",
      "                         342648,  346029,  353478,  356488,  357780,  375128,\n",
      "                         391060,  391421,  401412,  438129,  439468,  444013,\n",
      "                         458222,  463229,  463390,  484243,  513664,  525432,\n",
      "                         525761,  576996,  578971,  594744,  604187,  619672,\n",
      "                         645475,  654930,  676606,  688111,  689560,  690661,\n",
      "                         691802,  692291,  721886,  724105,  768534,  799928,\n",
      "                         818597,  853005,  856162,  857571,  881252,  883440,\n",
      "                         886657,  891788,  902989,  906973,  909668,  915333,\n",
      "                         919814,  926952,  951805,  970543,  979016,  982773,\n",
      "                         990225, 1018409, 1031773]]),\n",
      "       values=tensor([0.0729, 0.0365, 0.0365, 0.4376, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0729, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0365, 0.2553, 0.0365, 0.0729, 0.0365, 0.0365, 0.0365,\n",
      "                      0.2188, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0729, 0.0365, 0.0365, 0.0729, 0.4376, 0.0729, 0.0729,\n",
      "                      0.1459, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0729,\n",
      "                      0.0365, 0.0365, 0.0365, 0.4011, 0.0729, 0.0365, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0365, 0.0365, 0.0729, 0.0365, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0729, 0.0365, 0.0365, 0.0729, 0.0365,\n",
      "                      0.0365, 0.0365, 0.0365, 0.1823, 0.0365, 0.0365, 0.0729,\n",
      "                      0.3282, 0.0365, 0.0365, 0.0729, 0.0365, 0.0729, 0.0365,\n",
      "                      0.0365]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=99, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   7663,   14962,   44561,   61772,   64103,   65895,\n",
      "                          72242,   83224,   89986,  107669,  108799,  160635,\n",
      "                         177642,  182713,  200342,  216640,  236858,  256425,\n",
      "                         261287,  306588,  311856,  332445,  342567,  345545,\n",
      "                         356488,  361492,  373756,  374365,  376113,  386887,\n",
      "                         401412,  426204,  426530,  434197,  463390,  472246,\n",
      "                         499894,  500590,  525761,  533850,  542931,  547294,\n",
      "                         551393,  561007,  562081,  570465,  602115,  604306,\n",
      "                         611095,  611746,  636575,  643423,  676669,  688111,\n",
      "                         708385,  717779,  718360,  720228,  750530,  758917,\n",
      "                         780733,  817497,  825319,  825376,  825495,  852769,\n",
      "                         853662,  891232,  894091,  894997,  922760,  922951,\n",
      "                         926952,  932901,  936007,  940322,  942665,  945916,\n",
      "                         961291,  963925,  976344,  987200, 1005750, 1019906,\n",
      "                        1037792]]),\n",
      "       values=tensor([0.0390, 0.0390, 0.0390, 0.0390, 0.0780, 0.0390, 0.0390,\n",
      "                      0.0780, 0.0780, 0.0390, 0.0780, 0.0390, 0.3119, 0.0390,\n",
      "                      0.1559, 0.1170, 0.0780, 0.0390, 0.0390, 0.0390, 0.1170,\n",
      "                      0.0390, 0.0390, 0.0390, 0.1559, 0.0780, 0.0390, 0.0390,\n",
      "                      0.0390, 0.0390, 0.1170, 0.1170, 0.1170, 0.0390, 0.0390,\n",
      "                      0.1170, 0.0780, 0.0390, 0.1170, 0.0390, 0.0780, 0.0390,\n",
      "                      0.0390, 0.0390, 0.0390, 0.0390, 0.0780, 0.0390, 0.0390,\n",
      "                      0.0390, 0.0390, 0.0390, 0.0390, 0.0390, 0.1170, 0.0780,\n",
      "                      0.0390, 0.1170, 0.0390, 0.1170, 0.0780, 0.0780, 0.0780,\n",
      "                      0.1170, 0.6627, 0.0390, 0.0780, 0.0390, 0.0390, 0.0390,\n",
      "                      0.1170, 0.0390, 0.2729, 0.0390, 0.0390, 0.0390, 0.0780,\n",
      "                      0.0780, 0.0390, 0.0390, 0.0390, 0.0390, 0.0390, 0.0390,\n",
      "                      0.0390]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=85, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    281,    2022,    2057,  ..., 1047218, 1048548,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0007, 0.0007, 0.0007,  ..., 0.0021, 0.0007, 0.0035]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2471, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4187,    4835,   13196,   25335,   26070,   31857,\n",
      "                          39427,   48841,   72242,   77469,   79765,   83224,\n",
      "                         102865,  109777,  111914,  124600,  129255,  131210,\n",
      "                         144109,  148594,  157431,  164954,  171836,  174818,\n",
      "                         181854,  183790,  186346,  198642,  215001,  225521,\n",
      "                         244038,  246004,  250337,  261287,  266125,  273063,\n",
      "                         284011,  284241,  286447,  290665,  302597,  306588,\n",
      "                         311856,  314106,  323109,  332445,  343799,  346831,\n",
      "                         356488,  357780,  363668,  365508,  370023,  380221,\n",
      "                         380983,  388877,  393319,  400949,  401412,  404216,\n",
      "                         426841,  438129,  444013,  463229,  463390,  470449,\n",
      "                         473369,  483314,  483567,  484243,  491820,  495918,\n",
      "                         499894,  501295,  524302,  529481,  533814,  533850,\n",
      "                         538425,  550075,  555920,  561926,  578971,  583245,\n",
      "                         604812,  606366,  611746,  619417,  627392,  638916,\n",
      "                         647355,  664087,  666657,  666813,  670289,  671989,\n",
      "                         676606,  682190,  685491,  688111,  689560,  706231,\n",
      "                         716000,  742242,  743770,  752031,  754424,  757430,\n",
      "                         777038,  777475,  780733,  799928,  801262,  805000,\n",
      "                         813020,  818890,  824394,  825437,  831499,  847355,\n",
      "                         851931,  864985,  871194,  887177,  891703,  892822,\n",
      "                         895310,  896891,  897148,  915333,  927621,  950245,\n",
      "                         952227,  955266,  990225,  995365, 1005750, 1005809,\n",
      "                        1011474, 1031297, 1031773, 1032162, 1045762]]),\n",
      "       values=tensor([0.0391, 0.2218, 0.0522, 0.0783, 0.0261, 0.0522, 0.0261,\n",
      "                      0.0261, 0.0391, 0.1435, 0.0261, 0.0783, 0.0652, 0.0261,\n",
      "                      0.0783, 0.0261, 0.0522, 0.1305, 0.0391, 0.1044, 0.0783,\n",
      "                      0.0130, 0.0652, 0.0130, 0.3523, 0.0522, 0.0783, 0.0261,\n",
      "                      0.0261, 0.1305, 0.0391, 0.1044, 0.0522, 0.1174, 0.0261,\n",
      "                      0.0261, 0.0391, 0.0261, 0.0261, 0.0261, 0.0261, 0.0391,\n",
      "                      0.3393, 0.0391, 0.0261, 0.0522, 0.0261, 0.2349, 0.0130,\n",
      "                      0.0130, 0.0130, 0.0522, 0.0783, 0.0652, 0.1957, 0.0261,\n",
      "                      0.0130, 0.1957, 0.0913, 0.0261, 0.0913, 0.0130, 0.0261,\n",
      "                      0.0913, 0.0130, 0.0391, 0.0130, 0.0391, 0.0130, 0.0652,\n",
      "                      0.0783, 0.0130, 0.0522, 0.1305, 0.0130, 0.0391, 0.1566,\n",
      "                      0.0783, 0.0130, 0.0261, 0.0522, 0.0522, 0.0261, 0.0130,\n",
      "                      0.0130, 0.0261, 0.0261, 0.0130, 0.0130, 0.0261, 0.0391,\n",
      "                      0.0261, 0.0522, 0.0130, 0.0261, 0.0261, 0.0522, 0.0261,\n",
      "                      0.0652, 0.0130, 0.0130, 0.0261, 0.0783, 0.0130, 0.0130,\n",
      "                      0.0522, 0.0130, 0.0130, 0.0130, 0.0261, 0.2740, 0.0261,\n",
      "                      0.0130, 0.1305, 0.0391, 0.0261, 0.1566, 0.0130, 0.0130,\n",
      "                      0.0261, 0.0130, 0.0913, 0.0391, 0.0783, 0.0130, 0.3132,\n",
      "                      0.0261, 0.0391, 0.0261, 0.0652, 0.0130, 0.0522, 0.0261,\n",
      "                      0.0261, 0.0522, 0.0652, 0.0522, 0.0522, 0.0391, 0.0261,\n",
      "                      0.0783, 0.0261, 0.0391]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=143, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [  13583,   19433,   33621,   43938,   62855,   71032,\n",
      "                          72242,   73512,   83224,   86937,   93944,  116557,\n",
      "                         129001,  130191,  169846,  177755,  179642,  204128,\n",
      "                         217694,  237464,  246118,  261287,  306588,  322650,\n",
      "                         353478,  379135,  380351,  416210,  429628,  463390,\n",
      "                         483567,  488225,  497646,  534719,  574938,  584494,\n",
      "                         594744,  611746,  617280,  620542,  627392,  664073,\n",
      "                         688111,  698083,  703380,  738358,  762380,  776247,\n",
      "                         786953,  796613,  799928,  847848,  851931,  914583,\n",
      "                         915747,  933886,  949320,  964283,  982773,  989394,\n",
      "                         990225, 1031473]]),\n",
      "       values=tensor([0.0584, 0.1753, 0.0876, 0.1168, 0.2921, 0.1168, 0.0292,\n",
      "                      0.0584, 0.1168, 0.0876, 0.1168, 0.1168, 0.0876, 0.0584,\n",
      "                      0.1168, 0.0292, 0.0584, 0.1168, 0.0584, 0.0584, 0.1168,\n",
      "                      0.2045, 0.0584, 0.0584, 0.0584, 0.0584, 0.2337, 0.0584,\n",
      "                      0.1168, 0.0292, 0.1753, 0.0584, 0.0584, 0.1168, 0.1168,\n",
      "                      0.1753, 0.0584, 0.1168, 0.0584, 0.1168, 0.1168, 0.1168,\n",
      "                      0.0292, 0.1168, 0.1168, 0.0292, 0.0584, 0.5258, 0.0584,\n",
      "                      0.0584, 0.0584, 0.1168, 0.0584, 0.0584, 0.1461, 0.1753,\n",
      "                      0.0584, 0.1168, 0.0876, 0.1461, 0.0584, 0.0584]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=62, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[     0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0],\n",
      "                       [  6160,  18779,  28007,  69906,  72242,  83224, 107750,\n",
      "                        115864, 116557, 131210, 131377, 134757, 164964, 177590,\n",
      "                        177642, 178029, 202578, 202893, 217711, 221874, 232946,\n",
      "                        261287, 302597, 303929, 323618, 327219, 332445, 403294,\n",
      "                        450109, 463390, 467952, 474475, 478794, 533850, 555935,\n",
      "                        591868, 611746, 632166, 685587, 688111, 723018, 729358,\n",
      "                        741866, 745183, 789626, 793704, 799059, 822439, 846611,\n",
      "                        865475, 869920, 891788, 893771, 903554, 913381, 947273,\n",
      "                        986799, 987200, 999983]]),\n",
      "       values=tensor([0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.3980, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.1990, 0.0995, 0.1990, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.0995,\n",
      "                      0.0995, 0.0995, 0.0995, 0.0995, 0.0995, 0.1990, 0.3980,\n",
      "                      0.0995, 0.0995, 0.1990]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=59, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  24582,   30043,   30518,   51833,   66506,   72242,\n",
      "                          81322,   86356,  116557,  119937,  183426,  202578,\n",
      "                         218949,  225521,  230050,  261287,  293482,  306588,\n",
      "                         357467,  359145,  359151,  361492,  379135,  463390,\n",
      "                         494082,  496474,  501245,  538425,  542212,  549313,\n",
      "                         581096,  612900,  620992,  620996,  623618,  653589,\n",
      "                         666657,  676606,  688111,  690661,  699827,  732319,\n",
      "                         776587,  795936,  810818,  840228,  847848,  886575,\n",
      "                         887069,  891788,  905014,  922358,  926172,  958878,\n",
      "                         963495, 1008796, 1030915]]),\n",
      "       values=tensor([0.1678, 0.0839, 0.0839, 0.0839, 0.0839, 0.1678, 0.0839,\n",
      "                      0.0839, 0.0839, 0.0839, 0.1678, 0.0839, 0.1678, 0.3357,\n",
      "                      0.0839, 0.1678, 0.0839, 0.1678, 0.0839, 0.0839, 0.0839,\n",
      "                      0.0839, 0.0839, 0.0839, 0.0839, 0.0839, 0.0839, 0.0839,\n",
      "                      0.0839, 0.1678, 0.0839, 0.0839, 0.1678, 0.0839, 0.0839,\n",
      "                      0.0839, 0.0839, 0.0839, 0.0839, 0.0839, 0.4196, 0.0839,\n",
      "                      0.0839, 0.2518, 0.0839, 0.0839, 0.1678, 0.0839, 0.1678,\n",
      "                      0.0839, 0.0839, 0.0839, 0.0839, 0.0839, 0.2518, 0.0839,\n",
      "                      0.0839]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=57, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     366,     655,  ..., 1046698, 1047448,\n",
      "                        1047878]]),\n",
      "       values=tensor([0.0035, 0.0105, 0.0035,  ..., 0.0035, 0.0035, 0.0175]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1310, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [    101,    4993,   26887,   31712,   37941,   42127,\n",
      "                          55531,   64467,   67475,   72242,   72578,   73454,\n",
      "                          77469,   86356,   94335,  100005,  102865,  102933,\n",
      "                         108799,  118442,  125462,  126372,  126714,  134309,\n",
      "                         134541,  136177,  142070,  151305,  157024,  166844,\n",
      "                         168461,  171719,  171836,  181182,  181422,  185811,\n",
      "                         186237,  186603,  198626,  200886,  202893,  204382,\n",
      "                         226071,  236037,  237566,  257643,  261287,  264573,\n",
      "                         283746,  293729,  306588,  307907,  312408,  312820,\n",
      "                         318680,  323021,  327219,  331110,  342345,  342689,\n",
      "                         345734,  346523,  348530,  352195,  358163,  364511,\n",
      "                         365525,  380221,  390172,  400949,  403294,  410275,\n",
      "                         421496,  427197,  428236,  429655,  430963,  432823,\n",
      "                         433749,  439491,  440835,  441308,  454377,  463390,\n",
      "                         465452,  472021,  482353,  484243,  494507,  497646,\n",
      "                         511818,  517901,  520755,  521557,  525567,  526427,\n",
      "                         529481,  541712,  544092,  544891,  546908,  548716,\n",
      "                         562081,  597113,  604812,  606320,  609898,  613787,\n",
      "                         620825,  623918,  627392,  628948,  633994,  638510,\n",
      "                         655285,  660584,  674972,  675345,  676606,  678320,\n",
      "                         688111,  694199,  700745,  701891,  702425,  710325,\n",
      "                         716911,  723879,  739468,  752406,  754077,  754424,\n",
      "                         763631,  767389,  768380,  775923,  782397,  786746,\n",
      "                         795560,  804729,  815971,  816717,  819728,  822949,\n",
      "                         827473,  846611,  851503,  859066,  859593,  868920,\n",
      "                         869920,  872633,  883158,  888506,  922873,  924382,\n",
      "                         925886,  933164,  941540,  982773,  988070,  991645,\n",
      "                         997154, 1005669, 1023442, 1031297, 1034292, 1034821,\n",
      "                        1043771, 1045494]]),\n",
      "       values=tensor([0.1144, 0.0572, 0.0286, 0.0286, 0.0286, 0.1860, 0.0143,\n",
      "                      0.0143, 0.0286, 0.0286, 0.0143, 0.0143, 0.1430, 0.0715,\n",
      "                      0.0715, 0.0286, 0.1430, 0.0286, 0.0715, 0.0286, 0.0715,\n",
      "                      0.0143, 0.0143, 0.0286, 0.1860, 0.1430, 0.0286, 0.0286,\n",
      "                      0.0143, 0.0572, 0.0143, 0.0715, 0.0143, 0.0572, 0.1430,\n",
      "                      0.0429, 0.0715, 0.0429, 0.0143, 0.0143, 0.2003, 0.0286,\n",
      "                      0.1001, 0.0286, 0.0143, 0.0858, 0.1144, 0.0143, 0.1287,\n",
      "                      0.0286, 0.0286, 0.0715, 0.0286, 0.0429, 0.0143, 0.0286,\n",
      "                      0.0572, 0.0286, 0.1574, 0.0286, 0.0286, 0.0715, 0.0143,\n",
      "                      0.0143, 0.0143, 0.0858, 0.0715, 0.1717, 0.0286, 0.1144,\n",
      "                      0.0572, 0.0143, 0.0143, 0.0286, 0.0286, 0.0143, 0.2003,\n",
      "                      0.0143, 0.0286, 0.0286, 0.0286, 0.1001, 0.0143, 0.0429,\n",
      "                      0.0286, 0.0286, 0.0286, 0.0572, 0.0572, 0.0572, 0.0143,\n",
      "                      0.1430, 0.0286, 0.0429, 0.0286, 0.0143, 0.0143, 0.0286,\n",
      "                      0.0143, 0.0715, 0.0143, 0.0143, 0.0286, 0.0286, 0.0858,\n",
      "                      0.0286, 0.0429, 0.0572, 0.0143, 0.0286, 0.0286, 0.0715,\n",
      "                      0.0429, 0.0858, 0.0572, 0.0572, 0.0286, 0.0286, 0.0143,\n",
      "                      0.0858, 0.0143, 0.0286, 0.0286, 0.0286, 0.0143, 0.0715,\n",
      "                      0.0572, 0.0286, 0.0572, 0.0286, 0.0143, 0.0572, 0.0143,\n",
      "                      0.0715, 0.0429, 0.0143, 0.0286, 0.1144, 0.0429, 0.0286,\n",
      "                      0.0143, 0.1860, 0.1144, 0.0286, 0.0715, 0.0143, 0.0143,\n",
      "                      0.0143, 0.0143, 0.1430, 0.0286, 0.0286, 0.0286, 0.0143,\n",
      "                      0.0715, 0.0715, 0.0286, 0.0715, 0.1287, 0.0143, 0.0143,\n",
      "                      0.4864, 0.0572, 0.0572, 0.1717, 0.0858, 0.0286, 0.0143,\n",
      "                      0.0429, 0.0286]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=170, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4993,    7698,    8442,   13196,   18575,   31712,\n",
      "                          37299,   50201,   64103,   71105,   72242,   86937,\n",
      "                          88697,   95170,   98410,  103692,  131377,  134541,\n",
      "                         139627,  140090,  166793,  170127,  181422,  182730,\n",
      "                         188122,  195278,  202893,  231149,  234049,  260327,\n",
      "                         261287,  264573,  266136,  268484,  276714,  280064,\n",
      "                         283746,  311111,  314106,  314742,  319456,  323802,\n",
      "                         340937,  341308,  342526,  353478,  365508,  383259,\n",
      "                         387864,  393650,  394295,  421730,  437253,  440835,\n",
      "                         455212,  462085,  463390,  466909,  472932,  478794,\n",
      "                         484243,  497646,  502723,  506102,  512926,  516492,\n",
      "                         518362,  521557,  533256,  538883,  541712,  548544,\n",
      "                         573155,  586305,  589262,  594744,  599331,  604462,\n",
      "                         609898,  620996,  623634,  637253,  643213,  643518,\n",
      "                         644552,  645475,  646243,  659143,  659751,  666282,\n",
      "                         673019,  673390,  674650,  675345,  685521,  686779,\n",
      "                         688111,  716911,  728057,  737146,  743770,  748479,\n",
      "                         758474,  777942,  789626,  797633,  799928,  800415,\n",
      "                         815971,  825376,  826251,  833073,  836976,  849488,\n",
      "                         851503,  853925,  864542,  873283,  887069,  888797,\n",
      "                         907255,  922760,  927970,  941608,  955704,  958878,\n",
      "                         961580,  967176,  970513,  970849,  975280,  976983,\n",
      "                         977319,  986508,  999901, 1004533, 1007328, 1009177,\n",
      "                        1017126, 1027278, 1031773, 1031930, 1036677, 1043771,\n",
      "                        1047681]]),\n",
      "       values=tensor([0.0480, 0.0480, 0.0480, 0.0720, 0.0480, 0.0240, 0.0480,\n",
      "                      0.0480, 0.0240, 0.1199, 0.0240, 0.0240, 0.0959, 0.1199,\n",
      "                      0.1199, 0.2878, 0.1199, 0.0480, 0.0240, 0.0240, 0.0240,\n",
      "                      0.0240, 0.3358, 0.0240, 0.0720, 0.0240, 0.0480, 0.0480,\n",
      "                      0.0240, 0.0480, 0.0720, 0.1439, 0.0240, 0.0720, 0.0480,\n",
      "                      0.0240, 0.1199, 0.0480, 0.0480, 0.0240, 0.0720, 0.0480,\n",
      "                      0.0480, 0.1199, 0.0480, 0.0480, 0.0959, 0.0480, 0.0480,\n",
      "                      0.0480, 0.0720, 0.0480, 0.0480, 0.1679, 0.0480, 0.0480,\n",
      "                      0.0240, 0.0480, 0.0480, 0.0240, 0.0480, 0.0480, 0.0959,\n",
      "                      0.1199, 0.0480, 0.0480, 0.0240, 0.0720, 0.0240, 0.0480,\n",
      "                      0.0240, 0.0720, 0.0480, 0.0240, 0.0720, 0.0480, 0.0480,\n",
      "                      0.0480, 0.2159, 0.0480, 0.0480, 0.0240, 0.0480, 0.0240,\n",
      "                      0.0480, 0.0480, 0.0480, 0.1199, 0.0959, 0.0480, 0.0720,\n",
      "                      0.1199, 0.0480, 0.0480, 0.0480, 0.2878, 0.0240, 0.0480,\n",
      "                      0.0240, 0.0240, 0.0240, 0.0480, 0.0480, 0.0480, 0.2159,\n",
      "                      0.0480, 0.0240, 0.0480, 0.0480, 0.0959, 0.0240, 0.1199,\n",
      "                      0.0240, 0.0480, 0.0720, 0.0720, 0.0720, 0.0480, 0.2878,\n",
      "                      0.0240, 0.0480, 0.0959, 0.0480, 0.0480, 0.0480, 0.0240,\n",
      "                      0.0240, 0.0480, 0.0480, 0.0720, 0.0240, 0.0240, 0.1199,\n",
      "                      0.0720, 0.0240, 0.0480, 0.0480, 0.1199, 0.0240, 0.0240,\n",
      "                      0.0240, 0.0480, 0.0240, 0.0959, 0.1199]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=145, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    213,     368,    1592,  ..., 1046901, 1047114,\n",
      "                        1047430]]),\n",
      "       values=tensor([0.0033, 0.0033, 0.0033,  ..., 0.0497, 0.0033, 0.0066]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1415, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  25335,   34350,   44647,   45751,   51833,   72242,\n",
      "                          77469,   77995,   86754,   88611,  109992,  112122,\n",
      "                         161347,  166595,  171777,  171836,  202578,  203169,\n",
      "                         236843,  260327,  261287,  274689,  286636,  287665,\n",
      "                         288042,  291058,  293737,  306588,  342940,  361492,\n",
      "                         365508,  375128,  463390,  476364,  484243,  507438,\n",
      "                         525432,  533850,  555920,  561926,  565561,  583245,\n",
      "                         594744,  603221,  611746,  617751,  627392,  647589,\n",
      "                         662584,  667421,  668994,  676606,  688111,  725665,\n",
      "                         728086,  742242,  747377,  767413,  798860,  799529,\n",
      "                         809022,  828038,  871194,  882103,  885867,  886575,\n",
      "                         887069,  891788,  895122,  896891,  906532,  911749,\n",
      "                         933164,  936007,  977772,  978028,  982773,  990225,\n",
      "                         991640,  995365, 1014259, 1031773]]),\n",
      "       values=tensor([0.0459, 0.0918, 0.0918, 0.0459, 0.0459, 0.0918, 0.0918,\n",
      "                      0.0459, 0.0459, 0.3671, 0.0459, 0.1376, 0.0459, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.3671, 0.0459, 0.0459, 0.0918,\n",
      "                      0.2294, 0.0459, 0.2294, 0.0459, 0.0459, 0.1376, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.0459, 0.0459, 0.1376, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.0459, 0.0918, 0.0459, 0.0459,\n",
      "                      0.0459, 0.1376, 0.0459, 0.0459, 0.0459, 0.0918, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.0459, 0.5047, 0.0459, 0.0459,\n",
      "                      0.1376, 0.0459, 0.0918, 0.0459, 0.0459, 0.0459, 0.0459,\n",
      "                      0.0459, 0.0459, 0.1376, 0.0459, 0.0459, 0.0459, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.0459, 0.0918, 0.2294, 0.0459,\n",
      "                      0.0459, 0.0459, 0.0459, 0.0459, 0.0459]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=82, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [  13205,   17908,   28007,   38690,   72242,   83224,\n",
      "                          86356,   89986,  102865,  104542,  104712,  108799,\n",
      "                         124673,  138396,  160514,  193959,  211900,  216640,\n",
      "                         217008,  227698,  229204,  248539,  256425,  261287,\n",
      "                         267474,  302597,  306588,  309677,  311856,  337678,\n",
      "                         350994,  351843,  365508,  379610,  393076,  401412,\n",
      "                         411403,  463390,  465573,  483314,  487471,  489141,\n",
      "                         496720,  514087,  516086,  533850,  574938,  586285,\n",
      "                         592652,  626849,  638894,  643213,  654930,  666657,\n",
      "                         682190,  688111,  704473,  710331,  720228,  723086,\n",
      "                         740567,  752031,  754424,  761760,  766777,  779343,\n",
      "                         782597,  799928,  813696,  824702,  827446,  844366,\n",
      "                         854092,  873371,  888888,  906532,  920960,  936007,\n",
      "                         938177,  964894,  974806,  975280,  978757,  982395,\n",
      "                         988070,  988319,  990225, 1010414, 1012839, 1015432,\n",
      "                        1017321, 1036036]]),\n",
      "       values=tensor([0.0968, 0.0161, 0.0484, 0.0484, 0.0161, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0161, 0.0323, 0.0161, 0.0968,\n",
      "                      0.0161, 0.0323, 0.0968, 0.0645, 0.0968, 0.0484, 0.0323,\n",
      "                      0.2903, 0.0484, 0.0645, 0.0323, 0.0968, 0.1613, 0.0968,\n",
      "                      0.1290, 0.4516, 0.0323, 0.3226, 0.0968, 0.0161, 0.0968,\n",
      "                      0.0806, 0.0968, 0.0161, 0.0645, 0.0484, 0.0968, 0.0484,\n",
      "                      0.0484, 0.0645, 0.0484, 0.0484, 0.0968, 0.0645, 0.0161,\n",
      "                      0.0968, 0.0161, 0.0323, 0.0323, 0.0484, 0.0161, 0.0161,\n",
      "                      0.0323, 0.0968, 0.0484, 0.0484, 0.0484, 0.0484, 0.0323,\n",
      "                      0.4355, 0.0968, 0.0806, 0.0323, 0.0968, 0.0323, 0.0161,\n",
      "                      0.0645, 0.0484, 0.0161, 0.0968, 0.0323, 0.0484, 0.0161,\n",
      "                      0.0484, 0.1452, 0.0484, 0.0161, 0.1290, 0.1290, 0.0161,\n",
      "                      0.0484, 0.0968, 0.0323, 0.0161, 0.0484, 0.0484, 0.2097,\n",
      "                      0.0323]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=92, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     215,     235,  ..., 1047114, 1047321,\n",
      "                        1047825]]),\n",
      "       values=tensor([0.0034, 0.0040, 0.0017,  ..., 0.0126, 0.0017, 0.0051]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=6088, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [   3811,    4303,    4993,   13205,   14266,   18779,\n",
      "                          21052,   25038,   27956,   32108,   36611,   40982,\n",
      "                          42606,   43889,   46055,   47088,   49646,   49803,\n",
      "                          51475,   59097,   60787,   61851,   70775,   74314,\n",
      "                          78179,   79533,   88144,   93427,  102865,  102933,\n",
      "                         104596,  104637,  105038,  105440,  105957,  106613,\n",
      "                         108824,  109777,  113783,  115369,  116469,  119686,\n",
      "                         122688,  125532,  133411,  135066,  136816,  142842,\n",
      "                         143903,  144002,  144400,  157619,  161256,  161569,\n",
      "                         163761,  163794,  166804,  171836,  174436,  175874,\n",
      "                         176343,  178526,  179131,  179567,  181182,  183232,\n",
      "                         183792,  183872,  184875,  186346,  187607,  188122,\n",
      "                         189432,  190055,  190094,  199830,  200572,  201093,\n",
      "                         213546,  220895,  222898,  223173,  229160,  230177,\n",
      "                         231439,  231953,  235146,  236739,  237464,  240187,\n",
      "                         240459,  241049,  242811,  243789,  243993,  250486,\n",
      "                         251331,  256694,  257216,  258982,  259304,  262518,\n",
      "                         265144,  266125,  266449,  268105,  284472,  284611,\n",
      "                         286496,  292904,  296202,  297541,  302076,  307497,\n",
      "                         308617,  311111,  313954,  320073,  328671,  330608,\n",
      "                         331110,  333255,  335594,  335741,  339198,  341904,\n",
      "                         344648,  346500,  357653,  358565,  361852,  362672,\n",
      "                         364142,  366000,  371365,  371422,  375323,  377670,\n",
      "                         381013,  383007,  384551,  387864,  388891,  393920,\n",
      "                         394250,  401198,  403455,  416552,  430184,  432823,\n",
      "                         441191,  444681,  450252,  452608,  453801,  461167,\n",
      "                         470097,  470133,  470598,  471065,  471708,  474989,\n",
      "                         475379,  476504,  482353,  483079,  483314,  486132,\n",
      "                         487022,  498821,  499481,  499894,  502995,  508542,\n",
      "                         511525,  518516,  518878,  520599,  524685,  528546,\n",
      "                         533610,  534191,  536266,  536952,  537396,  541105,\n",
      "                         542317,  547031,  554671,  558363,  559305,  562373,\n",
      "                         564180,  564736,  565561,  575031,  575072,  575282,\n",
      "                         577409,  581314,  581475,  583210,  583245,  588015,\n",
      "                         588997,  590111,  593066,  595785,  604055,  604265,\n",
      "                         605103,  605787,  606574,  609410,  610636,  615202,\n",
      "                         617684,  619417,  620968,  623518,  623764,  624416,\n",
      "                         628764,  630741,  639342,  639482,  642337,  645214,\n",
      "                         648858,  649791,  657709,  658371,  662440,  664495,\n",
      "                         667110,  668991,  669302,  669705,  674218,  674977,\n",
      "                         675476,  678169,  681525,  685639,  692053,  697515,\n",
      "                         697605,  698650,  700850,  701891,  706539,  707218,\n",
      "                         711608,  713540,  716617,  718125,  718169,  718616,\n",
      "                         726370,  728117,  747878,  749617,  754424,  758474,\n",
      "                         761676,  763343,  765290,  770805,  771239,  777038,\n",
      "                         778311,  782597,  788706,  797474,  799512,  799928,\n",
      "                         803084,  814017,  815971,  816367,  822949,  823114,\n",
      "                         825535,  826358,  827004,  837105,  838529,  842188,\n",
      "                         845609,  850415,  852098,  857523,  858069,  865712,\n",
      "                         865902,  865972,  869920,  872791,  874639,  874762,\n",
      "                         884418,  888923,  892163,  898740,  900411,  901296,\n",
      "                         902657,  909746,  911512,  912818,  913072,  915275,\n",
      "                         916358,  928275,  931421,  931626,  937938,  938196,\n",
      "                         938692,  938905,  944800,  948638,  949527,  950771,\n",
      "                         951805,  953015,  953746,  959836,  961694,  965988,\n",
      "                         967201,  968853,  970709,  970731,  971889,  977581,\n",
      "                         993854,  994781,  995585,  996896,  997154,  998175,\n",
      "                         998412, 1004041, 1007300, 1008826, 1012839, 1015431,\n",
      "                        1017321, 1018078, 1018465, 1018921, 1019448, 1029264,\n",
      "                        1030107, 1033781, 1035669, 1035735, 1041109, 1044417,\n",
      "                        1044424, 1045494, 1045496, 1047600]]),\n",
      "       values=tensor([0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.1286, 0.0161,\n",
      "                      0.0161, 0.0804, 0.0321, 0.0161, 0.0643, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0321, 0.0161, 0.0161, 0.0161, 0.0321, 0.0161,\n",
      "                      0.0482, 0.0161, 0.0321, 0.0161, 0.3697, 0.0161, 0.0321,\n",
      "                      0.0161, 0.0321, 0.0161, 0.0161, 0.0321, 0.0161, 0.0161,\n",
      "                      0.1286, 0.0321, 0.0161, 0.0161, 0.0161, 0.0161, 0.0321,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0482, 0.0321, 0.0321, 0.0161,\n",
      "                      0.0964, 0.0161, 0.2893, 0.0161, 0.0482, 0.0482, 0.0321,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0321, 0.0643, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0482, 0.0161, 0.0161, 0.0161, 0.0482, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0964, 0.0964, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0482, 0.0161, 0.1768, 0.0161, 0.0321, 0.0161, 0.0161,\n",
      "                      0.1768, 0.0321, 0.0643, 0.0161, 0.0321, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.1447, 0.0482, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0482, 0.0161, 0.1125, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0482, 0.0161, 0.2733, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.1125, 0.0161, 0.0161, 0.0161, 0.0161, 0.1447, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0643, 0.0804, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0643, 0.0161, 0.0804, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.1447, 0.0321, 0.0321, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0482, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0161, 0.0804, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0161, 0.0643, 0.0161, 0.0643, 0.0643,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0321, 0.0161, 0.0643, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0482, 0.0161, 0.0161, 0.1286, 0.0321,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0321, 0.0161, 0.0482, 0.0321,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0482, 0.1286, 0.0161,\n",
      "                      0.0161, 0.0643, 0.0321, 0.0161, 0.1286, 0.0964, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.1286,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0321, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0161, 0.0643, 0.0321, 0.0482,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0321, 0.0482, 0.0161, 0.0321,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0321, 0.0643, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0161, 0.0161, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0964, 0.0161, 0.0161, 0.0161, 0.0321,\n",
      "                      0.0321, 0.0161, 0.0161, 0.0321, 0.0161, 0.0482, 0.0161,\n",
      "                      0.0643, 0.0482, 0.0161, 0.1125, 0.0643, 0.0321, 0.1125,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0321, 0.0321, 0.0482,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0321, 0.0161, 0.0161, 0.1929,\n",
      "                      0.0161, 0.0321, 0.0321, 0.1286, 0.0321, 0.0161, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0321, 0.0321, 0.0321, 0.0482, 0.0161,\n",
      "                      0.0161, 0.0643, 0.0161, 0.0161, 0.0161, 0.0161, 0.0643,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0321, 0.0161, 0.0643,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.1607, 0.0161,\n",
      "                      0.0161, 0.0161, 0.0161, 0.0161, 0.0161, 0.0804, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0321, 0.0161, 0.0161, 0.0161, 0.0321,\n",
      "                      0.0964, 0.0161, 0.0161, 0.0321, 0.0804, 0.0161, 0.0161,\n",
      "                      0.0321, 0.0161, 0.0161, 0.0804, 0.0161, 0.1125, 0.0161]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=364, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [    235,   16137,   29062,   31906,   42816,   49721,\n",
      "                          66546,   72242,   86356,   89986,   92178,   93669,\n",
      "                          99552,  115864,  124204,  125532,  134309,  143957,\n",
      "                         144955,  161760,  168893,  169846,  170200,  171836,\n",
      "                         176645,  185628,  192096,  193708,  202578,  212404,\n",
      "                         261287,  262997,  263395,  279249,  282085,  292531,\n",
      "                         312408,  328671,  332445,  342703,  348857,  365508,\n",
      "                         373284,  377670,  378754,  381982,  401992,  403294,\n",
      "                         403622,  435305,  435666,  463390,  465151,  475354,\n",
      "                         483314,  489141,  506102,  532357,  563949,  564954,\n",
      "                         582078,  600445,  603221,  606278,  611746,  620996,\n",
      "                         636941,  643213,  671962,  672788,  673498,  674650,\n",
      "                         688111,  716911,  733961,  741105,  743862,  750279,\n",
      "                         754424,  762427,  765352,  786953,  799059,  826504,\n",
      "                         832555,  836437,  838113,  873973,  878303,  878384,\n",
      "                         891788,  898813,  910749,  938177,  970944,  982773,\n",
      "                         990225,  998969, 1005379, 1008000, 1014380, 1019624,\n",
      "                        1020533, 1027345, 1032159, 1036539]]),\n",
      "       values=tensor([0.0546, 0.0546, 0.0546, 0.0546, 0.1639, 0.0546, 0.0273,\n",
      "                      0.0273, 0.1093, 0.1093, 0.0546, 0.0546, 0.0546, 0.1093,\n",
      "                      0.0546, 0.0546, 0.0546, 0.0546, 0.1093, 0.0546, 0.0546,\n",
      "                      0.0546, 0.0546, 0.1093, 0.0546, 0.0546, 0.0546, 0.0546,\n",
      "                      0.1093, 0.1093, 0.0273, 0.0546, 0.0546, 0.1639, 0.0546,\n",
      "                      0.0546, 0.0546, 0.0546, 0.0546, 0.0546, 0.1093, 0.0546,\n",
      "                      0.0546, 0.1093, 0.0546, 0.1093, 0.2732, 0.0546, 0.0546,\n",
      "                      0.0273, 0.1639, 0.0820, 0.0546, 0.1639, 0.0546, 0.0546,\n",
      "                      0.0546, 0.0546, 0.0546, 0.0546, 0.0546, 0.1093, 0.1639,\n",
      "                      0.0546, 0.0546, 0.1093, 0.0546, 0.0546, 0.0546, 0.0546,\n",
      "                      0.0546, 0.0546, 0.0273, 0.2185, 0.0546, 0.0273, 0.0546,\n",
      "                      0.0546, 0.1639, 0.0546, 0.0546, 0.3005, 0.0546, 0.0546,\n",
      "                      0.0546, 0.0546, 0.1093, 0.0820, 0.1093, 0.1093, 0.1093,\n",
      "                      0.0273, 0.0546, 0.0546, 0.0273, 0.0273, 0.2185, 0.3825,\n",
      "                      0.0546, 0.1093, 0.0546, 0.0546, 0.0546, 0.1093, 0.0546,\n",
      "                      0.0546]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=106, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4747,    4835,   13196,   31857,   33331,   45976,\n",
      "                          53192,   72242,   83224,   86356,   88144,   93216,\n",
      "                          93860,  102093,  136278,  143719,  148594,  160635,\n",
      "                         165008,  171836,  179579,  179799,  181422,  194259,\n",
      "                         198642,  225521,  244038,  261287,  285516,  336478,\n",
      "                         341769,  370023,  406511,  463390,  476592,  481351,\n",
      "                         487471,  491513,  491528,  505518,  520755,  526427,\n",
      "                         533850,  541285,  541775,  559591,  561926,  596932,\n",
      "                         620870,  638930,  640757,  662378,  664239,  666657,\n",
      "                         666856,  675680,  676606,  686779,  688111,  689560,\n",
      "                         689676,  710331,  711543,  772590,  780288,  780733,\n",
      "                         794548,  797633,  804212,  807350,  808661,  812003,\n",
      "                         822439,  846358,  847848,  871194,  881252,  887069,\n",
      "                         892822,  896891,  902989,  907003,  908398,  919814,\n",
      "                         924956,  941609,  943000,  958870,  961580,  972033,\n",
      "                         973101,  989394,  996371, 1034292, 1043007]]),\n",
      "       values=tensor([0.0447, 0.0894, 0.0447, 0.0447, 0.1787, 0.2234, 0.0447,\n",
      "                      0.0894, 0.0447, 0.0894, 0.0447, 0.0447, 0.0447, 0.0447,\n",
      "                      0.0447, 0.0894, 0.0447, 0.0447, 0.0447, 0.0894, 0.0447,\n",
      "                      0.0894, 0.0447, 0.0894, 0.0447, 0.0894, 0.0894, 0.1787,\n",
      "                      0.0447, 0.0447, 0.1787, 0.0447, 0.0894, 0.0447, 0.1787,\n",
      "                      0.0894, 0.0894, 0.0894, 0.0447, 0.0894, 0.0894, 0.0447,\n",
      "                      0.0894, 0.0894, 0.0894, 0.0894, 0.0447, 0.0447, 0.0447,\n",
      "                      0.0447, 0.0447, 0.1340, 0.2681, 0.0894, 0.0894, 0.0447,\n",
      "                      0.0894, 0.0447, 0.0447, 0.0447, 0.0894, 0.0894, 0.1340,\n",
      "                      0.0894, 0.0894, 0.1340, 0.0894, 0.0447, 0.0447, 0.2681,\n",
      "                      0.0447, 0.0447, 0.1340, 0.0447, 0.2234, 0.0447, 0.0894,\n",
      "                      0.0447, 0.1340, 0.0447, 0.0894, 0.0894, 0.0447, 0.1787,\n",
      "                      0.0894, 0.0447, 0.0894, 0.0447, 0.0447, 0.0447, 0.3574,\n",
      "                      0.1340, 0.0894, 0.0447, 0.0894]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=95, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    249,     538,     738,  ..., 1047833, 1048058,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0005, 0.0005, 0.0028,  ..., 0.0005, 0.0005, 0.0112]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=4880, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   2810,    4850,    5252,    5723,    8814,   13080,\n",
      "                          18753,   18767,   19890,   20320,   26111,   29028,\n",
      "                          29172,   30491,   38398,   39021,   45189,   45462,\n",
      "                          46837,   51349,   54399,   57623,   65680,   67863,\n",
      "                          69145,   76327,   78780,   78970,   86448,   97176,\n",
      "                         101864,  102325,  108799,  109799,  111125,  111637,\n",
      "                         114032,  114971,  116143,  116222,  121563,  123551,\n",
      "                         125825,  126692,  127721,  128388,  132394,  134182,\n",
      "                         136180,  136777,  139452,  143957,  148934,  149011,\n",
      "                         152447,  158906,  161430,  167907,  174436,  177692,\n",
      "                         179935,  183767,  186395,  191606,  191740,  193687,\n",
      "                         193963,  195112,  198297,  198435,  199298,  202286,\n",
      "                         202867,  205712,  206317,  210280,  214905,  217404,\n",
      "                         220462,  224921,  226832,  227661,  230971,  243399,\n",
      "                         250128,  253033,  255990,  259178,  259809,  265583,\n",
      "                         269246,  270191,  270995,  271792,  278793,  279810,\n",
      "                         284011,  289750,  292678,  297283,  299289,  302991,\n",
      "                         309913,  313000,  313732,  313810,  314106,  317724,\n",
      "                         327732,  327878,  328127,  328480,  329661,  330262,\n",
      "                         331110,  339612,  341564,  354042,  355733,  356264,\n",
      "                         360193,  362998,  366510,  374086,  376820,  377670,\n",
      "                         387758,  400810,  401611,  404480,  405734,  406476,\n",
      "                         409853,  415004,  417585,  417871,  420819,  432319,\n",
      "                         433801,  440881,  443394,  465088,  471904,  475090,\n",
      "                         484801,  487466,  489141,  490048,  493191,  494940,\n",
      "                         497223,  509074,  517945,  520778,  527496,  528208,\n",
      "                         529144,  529674,  530240,  532554,  532768,  547051,\n",
      "                         550853,  551161,  553934,  555936,  561302,  562041,\n",
      "                         562081,  563417,  572914,  574450,  574560,  576430,\n",
      "                         576999,  579733,  582607,  588850,  591174,  592039,\n",
      "                         592268,  594187,  595012,  597406,  600518,  603007,\n",
      "                         611091,  614328,  617382,  624228,  626747,  638193,\n",
      "                         640012,  641269,  645005,  645568,  654748,  656473,\n",
      "                         657709,  659497,  661920,  664706,  665031,  669228,\n",
      "                         671177,  672691,  673931,  675126,  677799,  682190,\n",
      "                         682272,  683455,  689831,  694231,  695396,  695689,\n",
      "                         697523,  698437,  709156,  709288,  710920,  715402,\n",
      "                         718786,  721244,  721767,  725203,  726395,  731238,\n",
      "                         731661,  736822,  743748,  746538,  753641,  756242,\n",
      "                         761690,  765581,  767413,  770300,  771506,  778298,\n",
      "                         781855,  783286,  787801,  788629,  789826,  804359,\n",
      "                         806510,  806856,  809590,  810231,  812513,  812588,\n",
      "                         815342,  815672,  820231,  821088,  822945,  824923,\n",
      "                         825319,  825624,  827286,  829507,  831740,  832939,\n",
      "                         838935,  840447,  840730,  847498,  848039,  852610,\n",
      "                         852627,  855376,  863149,  872392,  874762,  875917,\n",
      "                         877641,  880389,  880570,  881113,  883890,  884666,\n",
      "                         888298,  890537,  891640,  891788,  894889,  895548,\n",
      "                         901925,  903976,  905615,  907339,  913668,  915333,\n",
      "                         917944,  918870,  940322,  943231,  943867,  944937,\n",
      "                         949320,  952012,  953017,  957999,  969543,  971889,\n",
      "                         972105,  976096,  976891,  978789,  980985,  986856,\n",
      "                         987860,  988070,  988361,  990798,  992474,  994814,\n",
      "                         995302, 1010589, 1010756, 1020875, 1022128, 1023657,\n",
      "                        1029204, 1029625, 1034821, 1035078, 1035735, 1035860,\n",
      "                        1040025, 1042909, 1043182]]),\n",
      "       values=tensor([0.0109, 0.0109, 0.0109, 0.0217, 0.0109, 0.2284, 0.0435,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0217, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0435,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0217, 0.0109, 0.0109, 0.0435, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0435,\n",
      "                      0.0109, 0.0435, 0.0326, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0652, 0.0109, 0.0435, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0217, 0.0109, 0.0109, 0.0435, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.1196, 0.0109, 0.0217, 0.0109,\n",
      "                      0.0109, 0.0109, 0.1522, 0.0109, 0.0326, 0.0109, 0.0761,\n",
      "                      0.0109, 0.0435, 0.0109, 0.1305, 0.0109, 0.0109, 0.1087,\n",
      "                      0.0217, 0.1522, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0326, 0.0109, 0.0217, 0.0326, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0544, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0870, 0.0109, 0.0544, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0435, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0217, 0.0109, 0.0326, 0.0217, 0.0217, 0.1087,\n",
      "                      0.0109, 0.0979, 0.4567, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0217, 0.0217, 0.0326, 0.0544, 0.0217,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0217,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0217, 0.0217, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0435, 0.0217, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0326,\n",
      "                      0.0217, 0.0109, 0.0217, 0.0109, 0.0109, 0.0435, 0.0109,\n",
      "                      0.0109, 0.0217, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0435, 0.0109, 0.0217, 0.0544, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0217, 0.0979, 0.1196, 0.0109, 0.0109, 0.0217, 0.0435,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0217, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0217, 0.0979, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0217, 0.0109, 0.0109, 0.0109, 0.6525, 0.0109, 0.0217,\n",
      "                      0.0217, 0.0109, 0.0109, 0.0109, 0.0217, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0217, 0.0435, 0.0109, 0.0109, 0.0435,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0544, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0217, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0217, 0.0435, 0.0109, 0.0326, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0217, 0.0109, 0.0544, 0.0217, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0217, 0.0109, 0.0435, 0.0109,\n",
      "                      0.0109, 0.0109, 0.0435, 0.0652, 0.0109, 0.0109, 0.0217,\n",
      "                      0.0217, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109,\n",
      "                      0.0435, 0.0435, 0.0544, 0.0217, 0.0217, 0.0109, 0.0109,\n",
      "                      0.0435, 0.0109, 0.0109, 0.0109, 0.0109, 0.0109, 0.1305,\n",
      "                      0.0109, 0.0109, 0.0109, 0.0544, 0.0217, 0.0217, 0.0109,\n",
      "                      0.0326, 0.0109, 0.0109, 0.0109, 0.0109, 0.0870, 0.0435,\n",
      "                      0.0109, 0.0217, 0.0109, 0.0217]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=333, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    215,    1240,    1975,  ..., 1045408, 1047997,\n",
      "                        1048449]]),\n",
      "       values=tensor([0.0052, 0.0104, 0.0052,  ..., 0.0052, 0.0052, 0.0052]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1180, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,    1364,    1592,  ..., 1048261, 1048472,\n",
      "                        1048517]]),\n",
      "       values=tensor([0.0063, 0.0021, 0.0104,  ..., 0.0084, 0.0021, 0.0251]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1800, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     235,     333,  ..., 1047779, 1047825,\n",
      "                        1048137]]),\n",
      "       values=tensor([0.0021, 0.0005, 0.0011,  ..., 0.0021, 0.0011, 0.0016]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=5727, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     215,     366,  ..., 1047321, 1047600,\n",
      "                        1048118]]),\n",
      "       values=tensor([0.0024, 0.0012, 0.0035,  ..., 0.0071, 0.0012, 0.0012]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=5976, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,    4382,    4993,  ..., 1045496, 1045762,\n",
      "                        1046082]]),\n",
      "       values=tensor([0.0081, 0.0081, 0.0162, 0.0081, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0485, 0.0162, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0323, 0.0162,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0162, 0.0081, 0.0404, 0.0242,\n",
      "                      0.0081, 0.1938, 0.0081, 0.0081, 0.0081, 0.1373, 0.0081,\n",
      "                      0.0404, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0646, 0.0242, 0.0162, 0.0404,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0485, 0.0081, 0.0081, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0242,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0162, 0.0242, 0.0242, 0.0162, 0.0081, 0.0242, 0.0323,\n",
      "                      0.0081, 0.0646, 0.0081, 0.0081, 0.0242, 0.0081, 0.0646,\n",
      "                      0.0081, 0.0081, 0.0485, 0.0081, 0.0081, 0.0081, 0.1938,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0242, 0.0162,\n",
      "                      0.0162, 0.0727, 0.0162, 0.0162, 0.0081, 0.0081, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0242, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0242, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0404, 0.0081, 0.0565, 0.0081,\n",
      "                      0.0162, 0.0162, 0.0162, 0.0081, 0.0162, 0.0081, 0.0242,\n",
      "                      0.0081, 0.0162, 0.0162, 0.1777, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0404, 0.0323, 0.0081, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0242,\n",
      "                      0.0081, 0.0969, 0.0162, 0.0242, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0646, 0.0162, 0.0242, 0.0081, 0.0081, 0.0162, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0242, 0.0162, 0.0081,\n",
      "                      0.0242, 0.0081, 0.0081, 0.0323, 0.0242, 0.0404, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0162,\n",
      "                      0.1777, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0485, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0242, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0242, 0.0081, 0.0323, 0.0081, 0.0162,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0081, 0.0323, 0.0081, 0.0081,\n",
      "                      0.0808, 0.0081, 0.0081, 0.0081, 0.0242, 0.0162, 0.0162,\n",
      "                      0.0162, 0.0081, 0.0162, 0.0323, 0.0081, 0.0081, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0404, 0.0162, 0.0081, 0.0323,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0242, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0081, 0.0081, 0.0162, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0242, 0.0162, 0.0242, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0242, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0404, 0.0162, 0.0162, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0485, 0.0081, 0.0081, 0.0485, 0.0323,\n",
      "                      0.0162, 0.0081, 0.0162, 0.0081, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0323, 0.0081, 0.0162, 0.0081, 0.0081, 0.0565, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0162, 0.0162, 0.0081, 0.0081, 0.0162, 0.0162, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0081, 0.1857, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0485, 0.1211, 0.0081, 0.0081, 0.0646, 0.0081,\n",
      "                      0.1131, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0323,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0242, 0.0081, 0.0081, 0.0242,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0404, 0.0323, 0.0081,\n",
      "                      0.0323, 0.0162, 0.0081, 0.0081, 0.0081, 0.0081, 0.0162,\n",
      "                      0.0081, 0.0081, 0.0323, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0485, 0.0162, 0.0081, 0.1050, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0081, 0.0323, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0162, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0242, 0.0081, 0.0081, 0.0081, 0.0323,\n",
      "                      0.0565, 0.0081, 0.0081, 0.0081, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.2180, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0242, 0.0162,\n",
      "                      0.0727, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0404, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0323, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.2342, 0.0081, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0242, 0.0242, 0.0081, 0.0162, 0.0485,\n",
      "                      0.0081, 0.2504, 0.0081, 0.0242, 0.2100, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0404, 0.0162, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0242, 0.0081, 0.0081, 0.0727,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0242, 0.0081, 0.2746,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0242, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0081, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0162, 0.0242, 0.0727, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0242, 0.0565, 0.0242, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0242, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0323, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0404, 0.0162, 0.0081, 0.4684, 0.0404,\n",
      "                      0.0162, 0.0162, 0.0081, 0.0323, 0.0242, 0.0808, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0162, 0.0162, 0.0081,\n",
      "                      0.0162, 0.0081, 0.0081, 0.0242, 0.0162, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0162, 0.0081, 0.0404, 0.0485, 0.1292, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0162, 0.0081, 0.0323, 0.0081, 0.0081,\n",
      "                      0.0242, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0323,\n",
      "                      0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081, 0.0081,\n",
      "                      0.0081, 0.0081, 0.0969, 0.0081, 0.0081, 0.0162, 0.0081,\n",
      "                      0.0485, 0.0081, 0.0081, 0.0081, 0.0162, 0.0162, 0.0081,\n",
      "                      0.0081, 0.0646, 0.0242, 0.0081, 0.0081, 0.0162, 0.0242,\n",
      "                      0.0162, 0.0162, 0.0081, 0.0323, 0.0242, 0.0081]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=692, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4807,   11524,   50793,   60810,   64467,   71812,\n",
      "                          72242,   83224,   92178,  116557,  129255,  132693,\n",
      "                         138396,  156310,  168459,  198194,  219992,  221874,\n",
      "                         249893,  250486,  261287,  270167,  306588,  336823,\n",
      "                         347442,  376138,  391421,  401992,  401997,  402250,\n",
      "                         406627,  406752,  457988,  463390,  485787,  489267,\n",
      "                         499894,  507438,  520880,  521557,  526232,  533850,\n",
      "                         535530,  600968,  620992,  620996,  654390,  654930,\n",
      "                         666702,  667024,  673862,  676313,  688111,  694274,\n",
      "                         696949,  716165,  716905,  728086,  757430,  757867,\n",
      "                         758474,  788690,  790133,  798860,  806225,  837119,\n",
      "                         879904,  886496,  887069,  914583,  932301,  963589,\n",
      "                         971070,  978028,  981289,  993256, 1014259, 1014380,\n",
      "                        1028644, 1031773, 1033310]]),\n",
      "       values=tensor([0.0626, 0.0626, 0.0626, 0.0626, 0.0626, 0.1252, 0.1252,\n",
      "                      0.0626, 0.0626, 0.0626, 0.1252, 0.0626, 0.1252, 0.0626,\n",
      "                      0.0626, 0.1879, 0.1879, 0.0626, 0.0626, 0.0626, 0.1252,\n",
      "                      0.1252, 0.1252, 0.0626, 0.0626, 0.0626, 0.0626, 0.0626,\n",
      "                      0.1879, 0.1252, 0.0626, 0.1252, 0.3757, 0.0626, 0.0626,\n",
      "                      0.1252, 0.1879, 0.0626, 0.0626, 0.0626, 0.0626, 0.0626,\n",
      "                      0.0626, 0.0626, 0.0626, 0.1252, 0.0626, 0.0626, 0.0626,\n",
      "                      0.0626, 0.3757, 0.1252, 0.0626, 0.0626, 0.1879, 0.0626,\n",
      "                      0.0626, 0.0626, 0.1252, 0.0626, 0.0626, 0.1252, 0.0626,\n",
      "                      0.0626, 0.1252, 0.0626, 0.0626, 0.0626, 0.0626, 0.0626,\n",
      "                      0.0626, 0.1879, 0.0626, 0.0626, 0.0626, 0.0626, 0.0626,\n",
      "                      0.1879, 0.0626, 0.0626, 0.1252]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=81, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [   5237,   11793,   13179,   21084,   21306,   25146,\n",
      "                          28764,   32718,   37299,   40384,   56165,   57008,\n",
      "                          71105,   72841,   76753,   86883,   86937,   96635,\n",
      "                         105850,  109992,  116469,  120218,  120371,  125462,\n",
      "                         125514,  130366,  133411,  136816,  143140,  144165,\n",
      "                         152979,  172293,  174436,  185628,  186239,  186346,\n",
      "                         201093,  201625,  216752,  222666,  227772,  227819,\n",
      "                         230706,  240459,  248894,  250128,  253347,  253557,\n",
      "                         266125,  268510,  282689,  284611,  287022,  287665,\n",
      "                         289989,  295735,  302991,  309854,  318996,  326981,\n",
      "                         331110,  340215,  343846,  355353,  355933,  364142,\n",
      "                         373184,  373689,  375323,  379463,  381013,  383007,\n",
      "                         384551,  387864,  396650,  399300,  399596,  400949,\n",
      "                         413936,  425731,  427197,  428768,  430184,  432621,\n",
      "                         438149,  444736,  447793,  447812,  455212,  456017,\n",
      "                         470133,  471568,  475962,  483052,  487022,  487160,\n",
      "                         489141,  496039,  504942,  510528,  512296,  513519,\n",
      "                         513664,  519751,  520755,  522112,  531044,  534848,\n",
      "                         541522,  561302,  562081,  564511,  568847,  571393,\n",
      "                         579733,  581314,  593881,  608037,  613616,  615761,\n",
      "                         635982,  644478,  659405,  665941,  666775,  671995,\n",
      "                         685100,  688036,  689560,  692697,  696946,  701765,\n",
      "                         707218,  709886,  716387,  718565,  728086,  732205,\n",
      "                         737972,  745356,  748610,  749039,  751444,  754835,\n",
      "                         758388,  761339,  766076,  766687,  770057,  776603,\n",
      "                         777599,  783375,  788706,  800394,  825319,  831822,\n",
      "                         831983,  839187,  842188,  847219,  851663,  861714,\n",
      "                         865902,  868850,  870239,  873375,  874639,  876033,\n",
      "                         876035,  883158,  888797,  893558,  893771,  894963,\n",
      "                         896246,  898816,  908010,  913072,  913185,  913855,\n",
      "                         915365,  922951,  930539,  930913,  935512,  936007,\n",
      "                         937938,  948526,  951629,  954083,  961291,  961694,\n",
      "                         978282,  979174,  982395,  983251,  988071,  990542,\n",
      "                         992474,  997217,  997284, 1004542, 1004898, 1005809,\n",
      "                        1008826, 1009915, 1033052, 1034292, 1045345, 1045494]]),\n",
      "       values=tensor([0.0293, 0.0293, 0.0586, 0.2050, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0879, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.1757, 0.0586, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0879, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.1171, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.1171, 0.0586, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.1757, 0.0586,\n",
      "                      0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0586, 0.4686, 0.0293, 0.0293, 0.0586, 0.0586,\n",
      "                      0.0586, 0.0293, 0.0293, 0.0293, 0.0586, 0.0293, 0.0586,\n",
      "                      0.1757, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.0879, 0.0293, 0.0293,\n",
      "                      0.0586, 0.0293, 0.0586, 0.0293, 0.0879, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0879, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.1171, 0.0293,\n",
      "                      0.0293, 0.3514, 0.0586, 0.1171, 0.0879, 0.0293, 0.0586,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0586,\n",
      "                      0.0586, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0586, 0.0293, 0.0586, 0.0293, 0.2343, 0.0293, 0.0586,\n",
      "                      0.1464, 0.0293, 0.0293, 0.0879, 0.0293, 0.0293, 0.0586,\n",
      "                      0.0293, 0.0293, 0.0586, 0.0879, 0.0293, 0.0293, 0.0293,\n",
      "                      0.0293, 0.0293, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293,\n",
      "                      0.1171, 0.0293, 0.0293, 0.0293, 0.0293, 0.0586, 0.0293,\n",
      "                      0.0293, 0.0586, 0.1464, 0.0586, 0.0879, 0.0586, 0.0293,\n",
      "                      0.1171, 0.0586, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293,\n",
      "                      0.1464, 0.0293, 0.0586, 0.0293, 0.0293, 0.0293, 0.0293]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=210, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     181,    1616,  ..., 1047114, 1047825,\n",
      "                        1047878]]),\n",
      "       values=tensor([0.0141, 0.0070, 0.0141,  ..., 0.0246, 0.0070, 0.0035]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1557, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [     47,     123,     286,  ..., 1048465, 1048495,\n",
      "                        1048522]]),\n",
      "       values=tensor([0.0004, 0.0004, 0.0004,  ..., 0.0004, 0.0004, 0.0012]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=15222, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [   1967,    4993,   16464,   21383,   34945,   48280,\n",
      "                          56261,   69302,   71038,   73941,   77129,   85700,\n",
      "                          91828,  100742,  104242,  105100,  105194,  106059,\n",
      "                         109690,  111896,  113783,  119937,  127542,  133223,\n",
      "                         140982,  145738,  162318,  166615,  168010,  177590,\n",
      "                         177821,  179131,  195278,  197385,  201093,  215001,\n",
      "                         236138,  265583,  266125,  268239,  276113,  279527,\n",
      "                         282349,  292771,  299846,  313096,  315138,  320073,\n",
      "                         323109,  323956,  331110,  331363,  331821,  336147,\n",
      "                         340215,  366912,  375128,  381013,  384979,  387261,\n",
      "                         387408,  393920,  399596,  400949,  406752,  409846,\n",
      "                         413986,  417495,  428569,  428859,  434197,  443214,\n",
      "                         448470,  451146,  455007,  455326,  457582,  467797,\n",
      "                         477987,  479744,  489141,  496785,  501270,  501748,\n",
      "                         502600,  505984,  508079,  510103,  511842,  519751,\n",
      "                         524302,  577428,  584938,  606574,  608037,  609898,\n",
      "                         610846,  615500,  617042,  617684,  618024,  620889,\n",
      "                         630331,  633975,  634473,  635649,  635958,  641264,\n",
      "                         643395,  650813,  664290,  668500,  682190,  693779,\n",
      "                         697515,  701765,  709318,  718125,  721312,  728658,\n",
      "                         733846,  738127,  738924,  739778,  743371,  746235,\n",
      "                         751677,  754835,  756637,  780144,  781602,  789080,\n",
      "                         814357,  819643,  821870,  822883,  822949,  825123,\n",
      "                         833060,  834206,  836349,  842188,  845609,  847848,\n",
      "                         854047,  857691,  861172,  877466,  879401,  880556,\n",
      "                         887276,  892163,  893290,  901125,  902896,  908010,\n",
      "                         913074,  915102,  916173,  917258,  931172,  933416,\n",
      "                         935576,  942665,  944787,  948638,  949644,  955308,\n",
      "                         956638,  961291,  961694,  968160,  983418, 1005809,\n",
      "                        1008826, 1009915, 1020875, 1040889]]),\n",
      "       values=tensor([0.0383, 0.0383, 0.1531, 0.0383, 0.0383, 0.0765, 0.0765,\n",
      "                      0.0383, 0.0383, 0.0383, 0.1913, 0.0765, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.1148, 0.1148, 0.1531, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.1148, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.1531, 0.0765, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.1148, 0.0383, 0.0765, 0.1148, 0.0383, 0.0765, 0.0765,\n",
      "                      0.0383, 0.1531, 0.0383, 0.1148, 0.0765, 0.0765, 0.0765,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0765, 0.0383,\n",
      "                      0.1531, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.1531,\n",
      "                      0.0383, 0.1148, 0.1148, 0.0383, 0.0383, 0.1148, 0.0383,\n",
      "                      0.0383, 0.0765, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.1148, 0.0765, 0.0383,\n",
      "                      0.1148, 0.0383, 0.0765, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.1531, 0.0383, 0.0383, 0.1148, 0.0383, 0.1531, 0.0383,\n",
      "                      0.0383, 0.0765, 0.1148, 0.0383, 0.0383, 0.1148, 0.0765,\n",
      "                      0.0383, 0.1913, 0.0383, 0.0383, 0.1148, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0765, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.1531, 0.0765, 0.0383, 0.0383,\n",
      "                      0.0765, 0.1148, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.0765, 0.0383, 0.1148, 0.1531, 0.0765, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383,\n",
      "                      0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.0383, 0.1148,\n",
      "                      0.0383, 0.0383, 0.1148, 0.0765, 0.1531, 0.0383, 0.0765,\n",
      "                      0.3061, 0.0383, 0.1148]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=178, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  41680,   71032,   72242,   86356,   86765,   86937,\n",
      "                          87586,   88377,  116557,  129255,  129969,  172149,\n",
      "                         202893,  218023,  219854,  221874,  249381,  261287,\n",
      "                         284011,  302597,  306442,  309156,  311111,  350994,\n",
      "                         386175,  393650,  420375,  462155,  463390,  465243,\n",
      "                         476771,  489141,  520321,  561926,  608153,  619266,\n",
      "                         638894,  639716,  661293,  666657,  688111,  692198,\n",
      "                         754424,  788297,  797633,  813696,  838113,  845609,\n",
      "                         851503,  859593,  864542,  871194,  883205,  887177,\n",
      "                         896848,  916173,  932901,  944378,  950158,  989394,\n",
      "                        1018689, 1026760, 1037154]]),\n",
      "       values=tensor([0.0768, 0.0768, 0.0384, 0.0768, 0.0768, 0.0384, 0.1535,\n",
      "                      0.0768, 0.0768, 0.0768, 0.0384, 0.0768, 0.0384, 0.0768,\n",
      "                      0.0768, 0.0768, 0.0768, 0.1151, 0.0768, 0.1535, 0.0768,\n",
      "                      0.0768, 0.0768, 0.0768, 0.0768, 0.1535, 0.2303, 0.1535,\n",
      "                      0.0384, 0.0768, 0.3838, 0.0768, 0.0768, 0.0768, 0.0768,\n",
      "                      0.0768, 0.0768, 0.0768, 0.0768, 0.0768, 0.0384, 0.0768,\n",
      "                      0.0768, 0.0768, 0.0384, 0.0768, 0.0768, 0.0768, 0.1919,\n",
      "                      0.1535, 0.1919, 0.0768, 0.0384, 0.0768, 0.0768, 0.1535,\n",
      "                      0.0768, 0.0768, 0.0768, 0.5373, 0.0768, 0.0768, 0.1535]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=63, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    714,    1964,    1983,  ..., 1047802, 1048291,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0018, 0.0009, 0.0009,  ..., 0.0036, 0.0009, 0.0027]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2545, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    281,    1772,    1935,  ..., 1047686, 1047802,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0010, 0.0010, 0.0010,  ..., 0.0010, 0.0080, 0.0060]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2371, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [    101,     235,    2290,    4993,    7663,    9331,\n",
      "                          11672,   13205,   13583,   14962,   16807,   19433,\n",
      "                          20903,   23852,   25997,   29062,   31493,   35063,\n",
      "                          37299,   41865,   42606,   46486,   55747,   56261,\n",
      "                          59671,   60299,   71038,   77129,   80770,   86937,\n",
      "                          89209,   91101,   91112,   92408,   92674,   94449,\n",
      "                          95597,   96173,   96239,   99686,  104712,  104925,\n",
      "                         106343,  108799,  109777,  109992,  113783,  115859,\n",
      "                         122124,  122688,  122909,  130014,  130191,  132892,\n",
      "                         147277,  148212,  150957,  153157,  155650,  156639,\n",
      "                         157431,  158457,  162675,  163657,  164964,  172527,\n",
      "                         177196,  177590,  179131,  180414,  180851,  181182,\n",
      "                         181434,  183122,  186976,  189942,  190089,  190143,\n",
      "                         192096,  198436,  204537,  211867,  213919,  216012,\n",
      "                         217231,  220895,  221603,  222671,  224860,  225460,\n",
      "                         230367,  236563,  246056,  246274,  247599,  248774,\n",
      "                         253881,  258694,  258821,  260032,  262001,  262997,\n",
      "                         267980,  268847,  275502,  278322,  281722,  284011,\n",
      "                         284163,  288082,  289911,  292462,  292625,  293093,\n",
      "                         293570,  295172,  295783,  296949,  298123,  298152,\n",
      "                         303205,  308651,  311111,  314106,  314497,  320357,\n",
      "                         321776,  326444,  335594,  337169,  340277,  342567,\n",
      "                         345653,  347401,  349509,  354593,  355933,  356440,\n",
      "                         357984,  363402,  369945,  370304,  373284,  376138,\n",
      "                         379610,  386462,  393649,  398150,  399389,  405270,\n",
      "                         409098,  409865,  416397,  420474,  422077,  427197,\n",
      "                         428477,  428569,  429649,  432823,  435343,  437200,\n",
      "                         440710,  444013,  444757,  445749,  446183,  447812,\n",
      "                         455007,  466907,  471708,  475974,  481129,  481727,\n",
      "                         482292,  482293,  483404,  483567,  483631,  483903,\n",
      "                         484106,  484941,  487022,  487160,  489141,  490205,\n",
      "                         490777,  493098,  499325,  500219,  501748,  501842,\n",
      "                         503647,  506950,  508607,  511764,  513107,  516271,\n",
      "                         520880,  524031,  524905,  526453,  529362,  530108,\n",
      "                         534037,  534719,  535391,  536473,  537410,  540769,\n",
      "                         542233,  553057,  555920,  556145,  558500,  562081,\n",
      "                         563164,  565595,  572693,  573155,  574322,  578289,\n",
      "                         581096,  581314,  582541,  586667,  590848,  592652,\n",
      "                         593881,  594851,  595167,  597244,  597307,  602430,\n",
      "                         604812,  607811,  610636,  613709,  615761,  616214,\n",
      "                         618135,  618576,  621773,  623042,  623840,  627392,\n",
      "                         634572,  638499,  638615,  641955,  642400,  645342,\n",
      "                         649090,  657382,  665783,  666657,  668385,  668882,\n",
      "                         668991,  670091,  676606,  677411,  677832,  678044,\n",
      "                         680386,  680635,  680788,  682045,  682190,  684696,\n",
      "                         691622,  691802,  692291,  695130,  697515,  698811,\n",
      "                         699490,  700128,  700850,  701378,  701786,  708052,\n",
      "                         711743,  712555,  713635,  722147,  722205,  725332,\n",
      "                         725815,  731735,  732205,  736686,  738341,  740901,\n",
      "                         746956,  751424,  751728,  753213,  753338,  754424,\n",
      "                         757430,  757712,  757957,  761760,  763343,  766076,\n",
      "                         766619,  768404,  770379,  772590,  779749,  780527,\n",
      "                         782539,  784179,  790835,  794350,  796574,  798221,\n",
      "                         800415,  801472,  803692,  804044,  805469,  806148,\n",
      "                         808269,  809951,  813363,  815326,  816717,  817660,\n",
      "                         819632,  820231,  822945,  825138,  825319,  826251,\n",
      "                         828149,  830147,  832208,  832703,  836526,  839044,\n",
      "                         839265,  840968,  844679,  845609,  846718,  847848,\n",
      "                         848005,  848380,  850415,  850599,  851931,  856581,\n",
      "                         857474,  857691,  859096,  863339,  866926,  873375,\n",
      "                         874762,  875226,  880743,  880980,  883188,  886624,\n",
      "                         889411,  894446,  898138,  898740,  901925,  902233,\n",
      "                         903801,  908010,  912688,  913731,  915333,  915814,\n",
      "                         917180,  920667,  920708,  922951,  925587,  925761,\n",
      "                         925977,  928950,  931547,  933710,  934655,  936007,\n",
      "                         937864,  938077,  938201,  941808,  950739,  957152,\n",
      "                         961291,  968224,  969369,  973332,  977484,  978524,\n",
      "                         978536,  978789,  982773,  986799,  986861,  988070,\n",
      "                         990225,  994750,  995174,  995302,  995585,  996220,\n",
      "                         996371,  997238, 1005438, 1009915, 1012546, 1014380,\n",
      "                        1016490, 1021413, 1027888, 1028505, 1029986, 1037158,\n",
      "                        1038762, 1039352, 1043325, 1043771]]),\n",
      "       values=tensor([0.0165, 0.0165, 0.0165, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0579, 0.0083, 0.0083, 0.0083, 0.0083, 0.0248, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0248, 0.0165, 0.0910, 0.0331, 0.0083,\n",
      "                      0.0331, 0.0083, 0.0083, 0.1571, 0.0083, 0.0496, 0.0165,\n",
      "                      0.0083, 0.0083, 0.1489, 0.0744, 0.0083, 0.0248, 0.0083,\n",
      "                      0.0248, 0.0331, 0.0083, 0.0083, 0.0827, 0.0165, 0.0165,\n",
      "                      0.0165, 0.0662, 0.0083, 0.0083, 0.0331, 0.0083, 0.0165,\n",
      "                      0.0165, 0.0083, 0.0165, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0331, 0.0083, 0.1489, 0.0083, 0.0083, 0.0165,\n",
      "                      0.0083, 0.0165, 0.0083, 0.0248, 0.0165, 0.0083, 0.0083,\n",
      "                      0.4383, 0.0165, 0.0083, 0.0083, 0.0083, 0.0165, 0.0165,\n",
      "                      0.0083, 0.0165, 0.0992, 0.0083, 0.0083, 0.0413, 0.0165,\n",
      "                      0.0331, 0.0083, 0.0827, 0.0083, 0.0165, 0.0083, 0.0248,\n",
      "                      0.0083, 0.0165, 0.0827, 0.0413, 0.0083, 0.0496, 0.0083,\n",
      "                      0.0165, 0.0083, 0.0165, 0.0165, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0496, 0.0662, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0165, 0.0083, 0.0083, 0.0165, 0.0331, 0.0248, 0.0083,\n",
      "                      0.0496, 0.0083, 0.0083, 0.0910, 0.0827, 0.0083, 0.0165,\n",
      "                      0.0165, 0.0413, 0.0083, 0.0083, 0.0165, 0.0083, 0.0496,\n",
      "                      0.0083, 0.0248, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0248, 0.0662, 0.0165, 0.0413, 0.0744, 0.0083,\n",
      "                      0.2150, 0.0248, 0.1902, 0.0165, 0.0496, 0.0083, 0.0331,\n",
      "                      0.0083, 0.0165, 0.0165, 0.0331, 0.0083, 0.0083, 0.0413,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0165,\n",
      "                      0.0083, 0.0083, 0.0413, 0.0083, 0.1240, 0.0165, 0.0083,\n",
      "                      0.0331, 0.0083, 0.0744, 0.0083, 0.0083, 0.0331, 0.0165,\n",
      "                      0.0083, 0.0331, 0.0496, 0.0165, 0.0083, 0.0413, 0.0083,\n",
      "                      0.0331, 0.0248, 0.2150, 0.0165, 0.0248, 0.0083, 0.0165,\n",
      "                      0.0331, 0.0496, 0.0083, 0.0165, 0.0083, 0.0827, 0.0331,\n",
      "                      0.0165, 0.0083, 0.0331, 0.0083, 0.0579, 0.0083, 0.0248,\n",
      "                      0.0248, 0.0083, 0.0083, 0.0165, 0.0165, 0.0662, 0.0165,\n",
      "                      0.0083, 0.0165, 0.0083, 0.0165, 0.0083, 0.0248, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0744, 0.0248, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0662, 0.0083, 0.0331, 0.0083,\n",
      "                      0.0083, 0.0083, 0.1158, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.1240, 0.0248, 0.0165, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0165, 0.0083, 0.0248, 0.0165,\n",
      "                      0.0083, 0.0083, 0.0331, 0.0083, 0.0662, 0.0083, 0.0248,\n",
      "                      0.0083, 0.0413, 0.0165, 0.0083, 0.0165, 0.0165, 0.0083,\n",
      "                      0.0083, 0.0165, 0.0248, 0.0165, 0.0083, 0.0165, 0.0165,\n",
      "                      0.0165, 0.0083, 0.0083, 0.0083, 0.0248, 0.0165, 0.0165,\n",
      "                      0.0165, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0165, 0.0083, 0.0165, 0.0083, 0.0248, 0.0083, 0.1489,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0083, 0.0496, 0.0165, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0165, 0.0662, 0.0579, 0.0083, 0.0165,\n",
      "                      0.0248, 0.0083, 0.0083, 0.0413, 0.0248, 0.0083, 0.0165,\n",
      "                      0.0165, 0.0083, 0.0083, 0.0083, 0.0579, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0165, 0.0083, 0.0083, 0.0248, 0.0083, 0.0083,\n",
      "                      0.0413, 0.0248, 0.0083, 0.1323, 0.0496, 0.0248, 0.0083,\n",
      "                      0.0165, 0.0083, 0.2481, 0.0083, 0.0331, 0.0083, 0.0165,\n",
      "                      0.0165, 0.0165, 0.0083, 0.1737, 0.0083, 0.0165, 0.0248,\n",
      "                      0.0083, 0.0165, 0.0165, 0.0413, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0331, 0.0579, 0.0331, 0.0248,\n",
      "                      0.0083, 0.0248, 0.0165, 0.0083, 0.0083, 0.0165, 0.0248,\n",
      "                      0.0083, 0.0910, 0.0165, 0.0165, 0.0165, 0.0083, 0.0413,\n",
      "                      0.0165, 0.3721, 0.0496, 0.0579, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0165, 0.0165, 0.0083, 0.0248, 0.0083, 0.0083, 0.0083,\n",
      "                      0.0083, 0.0248, 0.0083, 0.0083, 0.0165, 0.0331, 0.0165,\n",
      "                      0.0083, 0.0083, 0.0413, 0.0579, 0.0083, 0.0083, 0.0496,\n",
      "                      0.0165, 0.0165, 0.0165, 0.0083, 0.0165, 0.0248, 0.0083,\n",
      "                      0.0083, 0.0083, 0.0083, 0.0083, 0.0165, 0.0331, 0.0083,\n",
      "                      0.0083, 0.0248, 0.1158]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=430, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [  13205,   23767,   25335,   26491,   29667,   30518,\n",
      "                          52153,   72242,   76625,   86356,   94254,  102933,\n",
      "                         108799,  109992,  116557,  119937,  120834,  122692,\n",
      "                         144590,  160635,  166844,  202578,  261287,  272426,\n",
      "                         341769,  369087,  380221,  380611,  386911,  409518,\n",
      "                         462426,  463390,  472021,  484243,  486132,  487471,\n",
      "                         489141,  494507,  520755,  533850,  537410,  544891,\n",
      "                         562081,  604812,  606278,  609898,  644471,  688111,\n",
      "                         688357,  689676,  728086,  732360,  734364,  753213,\n",
      "                         795936,  846699,  879270,  881383,  891788,  892458,\n",
      "                         915333,  919814,  936007,  943000,  961580,  970960,\n",
      "                         972095,  973566,  979016,  994890, 1002821, 1005669,\n",
      "                        1031297]]),\n",
      "       values=tensor([0.1506, 0.0502, 0.0502, 0.0502, 0.1506, 0.0502, 0.0502,\n",
      "                      0.1004, 0.1506, 0.1004, 0.0502, 0.0502, 0.1004, 0.1004,\n",
      "                      0.0502, 0.1506, 0.0502, 0.0502, 0.0502, 0.0502, 0.0502,\n",
      "                      0.0502, 0.1004, 0.0502, 0.1004, 0.0502, 0.1004, 0.0502,\n",
      "                      0.0502, 0.0502, 0.1506, 0.1004, 0.0502, 0.0502, 0.0502,\n",
      "                      0.0502, 0.0502, 0.0502, 0.0502, 0.0502, 0.0502, 0.4517,\n",
      "                      0.0502, 0.0502, 0.2008, 0.1004, 0.0502, 0.0502, 0.0502,\n",
      "                      0.5019, 0.0502, 0.1004, 0.0502, 0.0502, 0.3011, 0.0502,\n",
      "                      0.1004, 0.0502, 0.0502, 0.0502, 0.0502, 0.0502, 0.0502,\n",
      "                      0.1506, 0.0502, 0.1506, 0.0502, 0.0502, 0.1004, 0.0502,\n",
      "                      0.0502, 0.0502, 0.1004]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=73, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,     378,    1111,  ..., 1047114, 1047321,\n",
      "                        1047825]]),\n",
      "       values=tensor([0.0011, 0.0011, 0.0006,  ..., 0.0045, 0.0136, 0.0006]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=3688, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [  20523,   65776,   67946,   72242,   93216,  103279,\n",
      "                         104712,  116557,  132835,  134541,  172293,  179579,\n",
      "                         194259,  220895,  250486,  252108,  256425,  261287,\n",
      "                         306270,  313691,  319456,  328671,  335594,  354938,\n",
      "                         361492,  414030,  431615,  440866,  457113,  463229,\n",
      "                         463390,  520755,  536952,  545583,  555469,  555920,\n",
      "                         611672,  619417,  627191,  676606,  682190,  688111,\n",
      "                         689676,  690661,  693562,  696524,  701891,  702972,\n",
      "                         728086,  768513,  822945,  825495,  847848,  856162,\n",
      "                         910457,  915333,  924443,  937750,  938462,  938692,\n",
      "                         940142,  944378,  946531,  960282,  963781,  982773,\n",
      "                         987200, 1031297, 1033635]]),\n",
      "       values=tensor([0.0693, 0.0693, 0.0693, 0.1387, 0.0693, 0.0693, 0.0693,\n",
      "                      0.0693, 0.0693, 0.3467, 0.0693, 0.0693, 0.0693, 0.0693,\n",
      "                      0.0693, 0.0693, 0.0693, 0.1387, 0.0693, 0.0693, 0.2080,\n",
      "                      0.0693, 0.1387, 0.1387, 0.0693, 0.0693, 0.0693, 0.0693,\n",
      "                      0.3467, 0.1387, 0.0693, 0.0693, 0.0693, 0.0693, 0.0693,\n",
      "                      0.0693, 0.0693, 0.0693, 0.0693, 0.0693, 0.0693, 0.0693,\n",
      "                      0.2080, 0.0693, 0.0693, 0.0693, 0.0693, 0.0693, 0.0693,\n",
      "                      0.0693, 0.0693, 0.0693, 0.3467, 0.1387, 0.0693, 0.0693,\n",
      "                      0.0693, 0.3467, 0.0693, 0.0693, 0.0693, 0.1387, 0.0693,\n",
      "                      0.0693, 0.0693, 0.1387, 0.0693, 0.1387, 0.0693]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=69, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     213,     820,  ..., 1046818, 1047321,\n",
      "                        1047825]]),\n",
      "       values=tensor([0.0223, 0.0056, 0.0056,  ..., 0.0167, 0.0167, 0.0390]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1017, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [  15518,   26070,   27346,   28027,   32098,   32311,\n",
      "                          33618,   36053,   43741,   44349,   46356,   52076,\n",
      "                          58523,   66818,   67475,   72890,   80277,   83300,\n",
      "                          94263,  102865,  102944,  104637,  105652,  115276,\n",
      "                         118442,  122518,  123753,  124946,  132826,  133013,\n",
      "                         138171,  156310,  174859,  177319,  182887,  186237,\n",
      "                         189239,  190055,  191606,  195527,  201684,  202118,\n",
      "                         205402,  213669,  217152,  225255,  231953,  233752,\n",
      "                         235289,  236209,  236858,  243399,  244107,  247599,\n",
      "                         253407,  256092,  256667,  258345,  267638,  295029,\n",
      "                         301543,  302284,  311669,  312706,  316449,  318881,\n",
      "                         321188,  333033,  335771,  343540,  346411,  346500,\n",
      "                         347464,  358707,  358724,  361482,  364286,  365309,\n",
      "                         377670,  378884,  381013,  386287,  386913,  394250,\n",
      "                         396724,  401658,  410275,  424634,  436688,  436791,\n",
      "                         437552,  448082,  456592,  457593,  461608,  463390,\n",
      "                         469516,  471065,  471328,  473012,  473046,  483359,\n",
      "                         484988,  501748,  510432,  511708,  512021,  512381,\n",
      "                         517428,  520306,  521599,  532057,  541797,  553760,\n",
      "                         561849,  563389,  566124,  567943,  572693,  577275,\n",
      "                         578108,  580309,  592311,  592418,  593881,  594744,\n",
      "                         598357,  605020,  605523,  607380,  611672,  613152,\n",
      "                         620195,  623066,  623503,  623634,  635171,  642841,\n",
      "                         645342,  646297,  648431,  650668,  664712,  665783,\n",
      "                         673533,  674650,  680275,  709608,  715120,  721883,\n",
      "                         726395,  728065,  729388,  732101,  732205,  734256,\n",
      "                         736769,  745356,  751798,  753894,  758601,  780816,\n",
      "                         786504,  788572,  792729,  809326,  809668,  810594,\n",
      "                         823780,  823891,  825123,  826208,  826223,  826759,\n",
      "                         836212,  841617,  842188,  846972,  853300,  855638,\n",
      "                         860774,  860839,  861016,  862474,  868495,  875142,\n",
      "                         877669,  877687,  885568,  888770,  890583,  891788,\n",
      "                         893771,  908339,  909746,  921050,  930073,  933819,\n",
      "                         946777,  947991,  948262,  959925,  967616,  969771,\n",
      "                         970570,  980041,  982773,  984175,  988071,  993854,\n",
      "                         995302,  998087, 1000926, 1027345, 1032060, 1033794,\n",
      "                        1036359, 1038762, 1040169, 1041925, 1043325, 1043771,\n",
      "                        1045494]]),\n",
      "       values=tensor([0.0632, 0.0316, 0.1264, 0.0316, 0.0632, 0.0632, 0.0316,\n",
      "                      0.0632, 0.0632, 0.0316, 0.0316, 0.0316, 0.0316, 0.0632,\n",
      "                      0.0316, 0.0316, 0.0632, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0632, 0.0316, 0.0948,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0632, 0.0632,\n",
      "                      0.0632, 0.0316, 0.1580, 0.0316, 0.0316, 0.0632, 0.2211,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0632, 0.0316, 0.3159, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.1264, 0.0632, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.1264, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0632, 0.0632, 0.0632,\n",
      "                      0.0632, 0.0316, 0.0632, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0948, 0.0316, 0.0316, 0.0632, 0.0316, 0.1895,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0632, 0.0316, 0.0316, 0.0632,\n",
      "                      0.0316, 0.0316, 0.0632, 0.0632, 0.0632, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0316, 0.2211, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0632, 0.0316, 0.0316, 0.0316, 0.0316, 0.0948,\n",
      "                      0.0632, 0.0316, 0.0316, 0.0316, 0.0316, 0.0948, 0.0316,\n",
      "                      0.1264, 0.0316, 0.0316, 0.0632, 0.0316, 0.0948, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0948,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0632, 0.0316, 0.0316, 0.0632,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0948, 0.1264, 0.0316, 0.0316, 0.0948, 0.1264, 0.0316,\n",
      "                      0.0632, 0.0316, 0.0316, 0.1264, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0632, 0.0948, 0.0632, 0.0948,\n",
      "                      0.0632, 0.0316, 0.1580, 0.0948, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0316, 0.0316, 0.1895, 0.0316, 0.0316, 0.0316, 0.0632,\n",
      "                      0.0316, 0.0316, 0.0316, 0.0632, 0.0316, 0.0632, 0.0316,\n",
      "                      0.0632, 0.0316, 0.0316, 0.0316, 0.0316, 0.2527, 0.0316,\n",
      "                      0.0316, 0.0632, 0.0316, 0.0632, 0.0316, 0.0316, 0.0316,\n",
      "                      0.0948, 0.0632, 0.0316, 0.0316, 0.0316, 0.0316, 0.0948,\n",
      "                      0.0316, 0.2211, 0.0316, 0.0948, 0.0316, 0.1264]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=223, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    366,     820,    1592,  ..., 1045663, 1047114,\n",
      "                        1047825]]),\n",
      "       values=tensor([0.0020, 0.0040, 0.0060,  ..., 0.0020, 0.0060, 0.0040]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1333, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  13205,   22696,   23347,   28007,   36060,   44157,\n",
      "                          55531,   60394,   72242,   73732,   81199,   83224,\n",
      "                          86356,   89986,   94335,  101624,  108799,  112250,\n",
      "                         122692,  146891,  148594,  151305,  160635,  171499,\n",
      "                         181854,  182713,  183232,  183790,  201868,  203179,\n",
      "                         216752,  220895,  229760,  260327,  261287,  269076,\n",
      "                         286447,  286463,  286620,  289350,  293038,  338517,\n",
      "                         353478,  356488,  365508,  379610,  437929,  438129,\n",
      "                         453917,  463390,  470590,  482690,  497646,  510072,\n",
      "                         514293,  533814,  541285,  544194,  561926,  576996,\n",
      "                         584486,  596932,  608037,  609563,  611672,  627191,\n",
      "                         628331,  643213,  654930,  659143,  676606,  682190,\n",
      "                         688111,  731903,  737566,  754424,  789626,  795166,\n",
      "                         798860,  814001,  822945,  856162,  871194,  874112,\n",
      "                         881252,  895310,  914583,  933164,  964894,  975200,\n",
      "                         982773,  989394, 1031297, 1037220]]),\n",
      "       values=tensor([0.1553, 0.0518, 0.0518, 0.1036, 0.0518, 0.0518, 0.0518,\n",
      "                      0.0518, 0.1036, 0.0518, 0.0518, 0.0518, 0.1553, 0.0518,\n",
      "                      0.0518, 0.0518, 0.0518, 0.1036, 0.0518, 0.0518, 0.1036,\n",
      "                      0.0518, 0.0518, 0.0518, 0.3624, 0.3624, 0.0518, 0.0518,\n",
      "                      0.0518, 0.0518, 0.1553, 0.1036, 0.0518, 0.0518, 0.1036,\n",
      "                      0.0518, 0.1036, 0.0518, 0.0518, 0.0518, 0.1036, 0.0518,\n",
      "                      0.0518, 0.0518, 0.1553, 0.0518, 0.0518, 0.0518, 0.1036,\n",
      "                      0.0518, 0.0518, 0.1036, 0.0518, 0.0518, 0.0518, 0.0518,\n",
      "                      0.2589, 0.0518, 0.1036, 0.0518, 0.0518, 0.0518, 0.0518,\n",
      "                      0.0518, 0.1553, 0.3107, 0.0518, 0.0518, 0.1553, 0.0518,\n",
      "                      0.1553, 0.0518, 0.0518, 0.0518, 0.1553, 0.0518, 0.0518,\n",
      "                      0.2071, 0.0518, 0.1036, 0.0518, 0.0518, 0.1036, 0.0518,\n",
      "                      0.0518, 0.0518, 0.0518, 0.0518, 0.0518, 0.0518, 0.0518,\n",
      "                      0.1036, 0.1036, 0.0518]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=94, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [     77,     243,    2466,  ..., 1046698, 1047114,\n",
      "                        1047448]]),\n",
      "       values=tensor([0.0024, 0.0049, 0.0024,  ..., 0.0584, 0.0024, 0.0073]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1510, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [  13205,   24268,   24582,   28385,   41680,   50793,\n",
      "                          72242,   83224,   87586,   89986,  102865,  109992,\n",
      "                         116557,  138396,  144109,  164918,  181854,  193959,\n",
      "                         202893,  244038,  261287,  283449,  286447,  353478,\n",
      "                         360380,  393076,  440710,  451635,  463390,  472476,\n",
      "                         489141,  489861,  533850,  561926,  568934,  595785,\n",
      "                         599951,  606366,  612352,  674972,  688111,  691100,\n",
      "                         710331,  724105,  737566,  748708,  771562,  774524,\n",
      "                         799928,  822439,  874094,  881252,  899093,  924956,\n",
      "                         960073,  976091,  989394, 1020430, 1025299, 1037220]]),\n",
      "       values=tensor([0.1054, 0.3162, 0.1054, 0.3689, 0.1054, 0.1054, 0.0527,\n",
      "                      0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.1054,\n",
      "                      0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.1581,\n",
      "                      0.1054, 0.0527, 0.2108, 0.1054, 0.1054, 0.1054, 0.1054,\n",
      "                      0.0527, 0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.1054,\n",
      "                      0.1054, 0.1054, 0.1054, 0.1054, 0.1054, 0.0527, 0.1054,\n",
      "                      0.1054, 0.1054, 0.1054, 0.1054, 0.0527, 0.1054, 0.1054,\n",
      "                      0.3162, 0.1054, 0.1054, 0.1054, 0.2108, 0.0527, 0.1054,\n",
      "                      0.1054, 0.1054, 0.1054, 0.1054]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=60, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [    101,   41680,   42127,   59806,   61672,   72242,\n",
      "                          73454,   75066,   76982,   77286,   83224,   83422,\n",
      "                          85586,  102865,  104542,  122688,  131210,  132263,\n",
      "                         154549,  171883,  172875,  181854,  183232,  185917,\n",
      "                         188122,  196296,  197385,  201684,  215586,  228376,\n",
      "                         231953,  233597,  258511,  261287,  297337,  303805,\n",
      "                         336745,  338987,  342345,  347735,  350603,  361492,\n",
      "                         377670,  387261,  394250,  412112,  414767,  419763,\n",
      "                         421496,  423931,  428236,  430909,  455386,  463390,\n",
      "                         463751,  465452,  470292,  489141,  491142,  492612,\n",
      "                         509876,  521944,  548716,  552191,  563741,  566202,\n",
      "                         569221,  575579,  592418,  592773,  593761,  595335,\n",
      "                         599951,  607449,  608697,  629933,  632592,  640258,\n",
      "                         642462,  643518,  649791,  652222,  655740,  656850,\n",
      "                         662440,  667215,  669342,  674557,  675973,  676201,\n",
      "                         688111,  707184,  707447,  710331,  716889,  718616,\n",
      "                         727419,  748327,  750727,  754424,  769814,  778000,\n",
      "                         778315,  800415,  819611,  822945,  827473,  831456,\n",
      "                         847848,  861074,  862741,  864728,  883158,  886551,\n",
      "                         899093,  908627,  930683,  932291,  932359,  932901,\n",
      "                         971070,  973725,  976983,  987798,  997154, 1007328,\n",
      "                        1009981, 1014380, 1017367, 1034292, 1039680, 1041348,\n",
      "                        1043771]]),\n",
      "       values=tensor([0.0980, 0.1470, 0.0735, 0.0490, 0.0490, 0.0245, 0.0980,\n",
      "                      0.0980, 0.0490, 0.0245, 0.0980, 0.0490, 0.0980, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0245, 0.1225, 0.0980, 0.0490,\n",
      "                      0.2204, 0.0490, 0.2449, 0.0490, 0.1959, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.1714, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.2939, 0.0490, 0.0490, 0.0980,\n",
      "                      0.0490, 0.1470, 0.0490, 0.0490, 0.0245, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0980, 0.0490, 0.0490, 0.0490, 0.0245, 0.0490,\n",
      "                      0.0490, 0.0490, 0.1470, 0.0490, 0.1959, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0245, 0.0980, 0.0490, 0.0490, 0.0245,\n",
      "                      0.0735, 0.0490, 0.0980, 0.0490, 0.0490, 0.0490, 0.0490,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0490, 0.0980,\n",
      "                      0.0980, 0.1959, 0.0490, 0.0490, 0.0490, 0.2204, 0.0490,\n",
      "                      0.0980, 0.0490, 0.1470, 0.0245, 0.1470, 0.0980, 0.0490,\n",
      "                      0.0490, 0.0490, 0.2939, 0.0490, 0.0490, 0.0980, 0.0980,\n",
      "                      0.0490, 0.0490, 0.0490, 0.0490, 0.0245, 0.0980, 0.0980]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=133, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    366,     661,    1364,  ..., 1046533, 1047114,\n",
      "                        1047825]]),\n",
      "       values=tensor([0.0032, 0.0032, 0.0159,  ..., 0.0011, 0.0074, 0.0021]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=3167, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   1058,   48357,   57100,   61556,   65895,   72242,\n",
      "                          83224,   88166,   89986,  112585,  116557,  124600,\n",
      "                         125532,  134676,  138396,  171506,  181854,  182713,\n",
      "                         200360,  202578,  236858,  242390,  246056,  258821,\n",
      "                         261287,  302597,  306263,  306588,  308585,  311111,\n",
      "                         323109,  335963,  356488,  372510,  373756,  376612,\n",
      "                         377207,  377670,  385570,  392560,  403294,  424396,\n",
      "                         430930,  463390,  478128,  478794,  521557,  526453,\n",
      "                         532376,  533850,  562081,  564914,  569699,  570465,\n",
      "                         590778,  620996,  636575,  643213,  654758,  666657,\n",
      "                         674972,  688111,  710615,  777475,  778214,  798396,\n",
      "                         812366,  817497,  818597,  819459,  819559,  829871,\n",
      "                         843134,  850415,  852110,  883188,  891788,  894091,\n",
      "                         942665,  964894,  968425,  982773,  987200,  988071,\n",
      "                        1010414, 1014128, 1032175]]),\n",
      "       values=tensor([0.0492, 0.0984, 0.0492, 0.0492, 0.0492, 0.0492, 0.0984,\n",
      "                      0.0492, 0.0492, 0.0984, 0.0492, 0.0492, 0.0492, 0.0492,\n",
      "                      0.0492, 0.0492, 0.0492, 0.1476, 0.0492, 0.0492, 0.0492,\n",
      "                      0.1476, 0.0984, 0.0492, 0.0984, 0.1476, 0.0492, 0.0984,\n",
      "                      0.0492, 0.0492, 0.0492, 0.0492, 0.0492, 0.0492, 0.0984,\n",
      "                      0.0492, 0.0492, 0.0492, 0.0492, 0.0984, 0.0984, 0.0492,\n",
      "                      0.0984, 0.0492, 0.5413, 0.1476, 0.0492, 0.0492, 0.0492,\n",
      "                      0.0492, 0.0492, 0.0492, 0.0492, 0.0984, 0.1476, 0.0492,\n",
      "                      0.0492, 0.0492, 0.0984, 0.0984, 0.0492, 0.0492, 0.2952,\n",
      "                      0.0492, 0.0492, 0.0492, 0.1476, 0.0492, 0.0492, 0.1476,\n",
      "                      0.1476, 0.0492, 0.3444, 0.0492, 0.1476, 0.0492, 0.0492,\n",
      "                      0.0492, 0.0492, 0.0984, 0.0492, 0.0492, 0.0984, 0.0492,\n",
      "                      0.0984, 0.0984, 0.0492]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=87, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    368,     714,    1412,  ..., 1045494, 1045737,\n",
      "                        1046698]]),\n",
      "       values=tensor([0.0072, 0.0014, 0.0043,  ..., 0.0536, 0.0478, 0.0029]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1499, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   5237,   13196,   15588,   28007,   28753,   33675,\n",
      "                          35071,   40974,   44234,   46055,   60787,   60964,\n",
      "                          62211,   64103,   74004,   74483,   77031,   86302,\n",
      "                          86356,   87586,   96053,  103911,  106263,  106521,\n",
      "                         109777,  113276,  119920,  122688,  125519,  125532,\n",
      "                         138883,  142842,  144109,  148594,  151203,  157568,\n",
      "                         159131,  159658,  160635,  171836,  172475,  180414,\n",
      "                         181182,  181854,  187694,  192544,  200342,  212235,\n",
      "                         220895,  223911,  225521,  231149,  231953,  237230,\n",
      "                         241823,  247599,  248539,  261287,  263395,  264350,\n",
      "                         266846,  280856,  286463,  286496,  288354,  290895,\n",
      "                         298414,  302597,  311111,  314106,  314742,  314959,\n",
      "                         317955,  330919,  331110,  337678,  337898,  342526,\n",
      "                         355283,  360674,  365394,  367521,  390161,  402535,\n",
      "                         404216,  411380,  412730,  413986,  426204,  432528,\n",
      "                         439491,  455437,  455782,  460508,  462794,  463751,\n",
      "                         479291,  493430,  496039,  497472,  498821,  499894,\n",
      "                         505368,  505773,  507438,  518362,  522649,  523120,\n",
      "                         525432,  525761,  541454,  547294,  547340,  548553,\n",
      "                         549376,  556492,  562554,  562857,  567239,  583934,\n",
      "                         584486,  587420,  592652,  592773,  594663,  594744,\n",
      "                         595785,  605461,  606164,  610636,  612987,  617442,\n",
      "                         619113,  623066,  627392,  630014,  639716,  649255,\n",
      "                         655454,  659405,  670091,  673543,  674650,  674972,\n",
      "                         676606,  676693,  682190,  682873,  684696,  686106,\n",
      "                         687125,  695058,  701891,  704251,  706231,  716000,\n",
      "                         724105,  752050,  754424,  765371,  776577,  801260,\n",
      "                         810044,  810818,  815607,  817497,  817944,  820331,\n",
      "                         825376,  825495,  827405,  827737,  828783,  839265,\n",
      "                         844366,  847848,  853556,  853662,  856042,  860916,\n",
      "                         864371,  873258,  874094,  874762,  883440,  887069,\n",
      "                         891999,  902989,  910749,  912690,  915964,  922951,\n",
      "                         926952,  936007,  940855,  941775,  943627,  982773,\n",
      "                         983464,  988070,  990225,  995365,  997099,  998689,\n",
      "                        1009981, 1011917, 1014259, 1014380, 1018689, 1022460,\n",
      "                        1032159, 1033052, 1037792]]),\n",
      "       values=tensor([0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.1263,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.1263,\n",
      "                      0.0421, 0.0842, 0.0842, 0.0842, 0.0842, 0.0421, 0.0842,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0842,\n",
      "                      0.0842, 0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.1263, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0842, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0842, 0.0842,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.2948, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0842, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0842, 0.0421,\n",
      "                      0.4632, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0842, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0842, 0.0421, 0.0421, 0.0421, 0.0842, 0.0421,\n",
      "                      0.1263, 0.0421, 0.2105, 0.0421, 0.1263, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.1263, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0842, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.1684, 0.1684, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0421, 0.0421, 0.0842, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0842, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421, 0.0421,\n",
      "                      0.0421, 0.0421, 0.0421]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=213, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    235,    1616,    3811,  ..., 1037529, 1040296,\n",
      "                        1043325]]),\n",
      "       values=tensor([0.0114, 0.0057, 0.0057, 0.0057, 0.0454, 0.0057, 0.0284,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0057, 0.0341, 0.0284, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0114,\n",
      "                      0.0114, 0.0511, 0.0114, 0.0114, 0.0057, 0.0057, 0.0114,\n",
      "                      0.0114, 0.0284, 0.0114, 0.0057, 0.0170, 0.0114, 0.0624,\n",
      "                      0.0057, 0.0114, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0114, 0.0057, 0.0057, 0.0057, 0.0114, 0.0170,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0511, 0.0057, 0.0057, 0.0170, 0.0057, 0.0454,\n",
      "                      0.0624, 0.0057, 0.0057, 0.0284, 0.0170, 0.0057, 0.0852,\n",
      "                      0.0057, 0.0057, 0.0454, 0.0057, 0.0284, 0.0057, 0.0114,\n",
      "                      0.0170, 0.0057, 0.0114, 0.0057, 0.0057, 0.0227, 0.0057,\n",
      "                      0.0284, 0.0057, 0.0227, 0.0454, 0.0057, 0.0114, 0.0114,\n",
      "                      0.0057, 0.0227, 0.0908, 0.0114, 0.0114, 0.0114, 0.0057,\n",
      "                      0.0170, 0.0114, 0.0114, 0.0511, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0057, 0.0057, 0.0114, 0.0397, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0227, 0.0114, 0.0057, 0.0057, 0.0454,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0170, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057, 0.0227,\n",
      "                      0.0057, 0.0114, 0.0057, 0.0057, 0.0114, 0.0341, 0.0114,\n",
      "                      0.0170, 0.0170, 0.0057, 0.0852, 0.0057, 0.0170, 0.0057,\n",
      "                      0.0170, 0.0057, 0.0227, 0.0057, 0.1419, 0.0057, 0.0057,\n",
      "                      0.0341, 0.0057, 0.0114, 0.0057, 0.0341, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0965, 0.0114, 0.0170, 0.0057, 0.0227, 0.0114,\n",
      "                      0.0114, 0.4996, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0114, 0.0057, 0.0057, 0.0170, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0057, 0.0114, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0114, 0.0341, 0.0227, 0.0057, 0.0170, 0.0114, 0.0284,\n",
      "                      0.0057, 0.0284, 0.0057, 0.0170, 0.1022, 0.0114, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0057, 0.0227, 0.0114, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0624, 0.0057, 0.0114, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0397, 0.0114, 0.0227, 0.0057, 0.0057, 0.0511, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0114, 0.0568, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0227, 0.0114, 0.0795, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0511, 0.0057, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0170, 0.0170, 0.0057, 0.0057, 0.0114, 0.0114,\n",
      "                      0.0227, 0.0114, 0.0114, 0.0057, 0.0057, 0.0170, 0.0057,\n",
      "                      0.0057, 0.0341, 0.0057, 0.0057, 0.0057, 0.0057, 0.0341,\n",
      "                      0.0227, 0.0114, 0.0114, 0.0568, 0.0170, 0.0057, 0.0624,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0341, 0.0341, 0.0057,\n",
      "                      0.0057, 0.0227, 0.0341, 0.0341, 0.0114, 0.0908, 0.0397,\n",
      "                      0.0738, 0.0114, 0.0057, 0.0057, 0.0057, 0.0227, 0.0568,\n",
      "                      0.0114, 0.0057, 0.0170, 0.0170, 0.0057, 0.0284, 0.0114,\n",
      "                      0.0057, 0.0170, 0.0227, 0.0114, 0.0057, 0.0114, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0170, 0.1135, 0.1306, 0.0057, 0.0114,\n",
      "                      0.0114, 0.0170, 0.0114, 0.0057, 0.0341, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0114, 0.0397, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0454, 0.0057, 0.0284, 0.0114, 0.0057, 0.0057, 0.0114,\n",
      "                      0.0170, 0.0170, 0.0057, 0.0057, 0.0170, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0114, 0.0170, 0.0114, 0.0341,\n",
      "                      0.0114, 0.0057, 0.0114, 0.0454, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0114, 0.0114, 0.1476, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0170, 0.0057, 0.0057, 0.0227, 0.0114, 0.0114,\n",
      "                      0.0227, 0.0057, 0.0057, 0.0227, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0057, 0.0114, 0.0114, 0.0057, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0057, 0.0397, 0.0114, 0.0114, 0.0114, 0.0397, 0.0341,\n",
      "                      0.0454, 0.0170, 0.0170, 0.0057, 0.0057, 0.1192, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0227, 0.0057, 0.0057, 0.0114, 0.0227,\n",
      "                      0.0227, 0.0057, 0.0057, 0.0341, 0.0227, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0284, 0.0114, 0.0057, 0.0057, 0.0057, 0.0454,\n",
      "                      0.0170, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0681, 0.0057, 0.0341, 0.0568, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0397, 0.0057, 0.0057, 0.0170, 0.0057, 0.0227,\n",
      "                      0.0114, 0.0057, 0.0114, 0.0341, 0.0057, 0.0341, 0.0057,\n",
      "                      0.0852, 0.0341, 0.0057, 0.2612, 0.0057, 0.0227, 0.0057,\n",
      "                      0.0114, 0.0170, 0.0114, 0.0341, 0.0057, 0.0057, 0.0454,\n",
      "                      0.0170, 0.0057, 0.0057, 0.0284, 0.0057, 0.0454, 0.0057,\n",
      "                      0.0114, 0.0284, 0.0057, 0.0227, 0.0511, 0.0170, 0.0057,\n",
      "                      0.0341, 0.0057, 0.0057, 0.0114, 0.0114, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0568, 0.0114, 0.0170,\n",
      "                      0.0057, 0.0170, 0.0057, 0.0057, 0.0114, 0.0114, 0.0114,\n",
      "                      0.0341, 0.0170, 0.0114, 0.0454, 0.0341, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0170, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0057, 0.0341, 0.0114, 0.0170,\n",
      "                      0.0057, 0.0057, 0.0170, 0.0057, 0.1022, 0.0511, 0.0057,\n",
      "                      0.0057, 0.0170, 0.0057, 0.0057, 0.0170, 0.0057, 0.0114,\n",
      "                      0.0341, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0284,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0568, 0.0341, 0.0057, 0.0114,\n",
      "                      0.0227, 0.0057, 0.0057, 0.0057, 0.0114, 0.0114, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0170, 0.0057, 0.0057, 0.0284, 0.0057,\n",
      "                      0.0057, 0.0511, 0.0057, 0.0057, 0.0057, 0.0511, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0397, 0.0284, 0.0114, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0852, 0.0057, 0.0057, 0.0795,\n",
      "                      0.0114, 0.0057, 0.0114, 0.0057, 0.0114, 0.0227, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0057, 0.0397, 0.0284, 0.0227, 0.0057,\n",
      "                      0.0170, 0.0057, 0.0057, 0.3520, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0227, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0852, 0.0114, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0170, 0.0227, 0.0057, 0.0057, 0.0284, 0.0057,\n",
      "                      0.0624, 0.0057, 0.0397, 0.0057, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0114, 0.0227, 0.0908, 0.0057, 0.0284, 0.0170,\n",
      "                      0.0227, 0.0114, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0170, 0.0057,\n",
      "                      0.0227, 0.0114, 0.0284, 0.0114, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0057, 0.0057, 0.1192, 0.0114, 0.0057, 0.0284, 0.0170,\n",
      "                      0.0057, 0.0114, 0.0114, 0.0057, 0.0057, 0.0057, 0.0170,\n",
      "                      0.0057, 0.0057, 0.0908, 0.0057, 0.0057, 0.0170, 0.0057,\n",
      "                      0.0057, 0.0170, 0.0284, 0.0057, 0.0397, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0057, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0057, 0.0057, 0.0114, 0.1703,\n",
      "                      0.0227, 0.0057, 0.0057, 0.0057, 0.0057, 0.0170, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0170, 0.0057, 0.0057, 0.0057, 0.0114,\n",
      "                      0.0114, 0.1249, 0.0057, 0.0057, 0.0511, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0170, 0.0170, 0.0057, 0.0114, 0.0114, 0.0284,\n",
      "                      0.0114, 0.0170, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0114, 0.0057, 0.0057, 0.0114, 0.0057, 0.0681, 0.0397,\n",
      "                      0.0170, 0.0170, 0.0057, 0.0624, 0.0114, 0.0568, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0170, 0.0341, 0.0057, 0.0114, 0.0114,\n",
      "                      0.0057, 0.0114, 0.0284, 0.0454, 0.0057, 0.0114, 0.0057,\n",
      "                      0.0057, 0.0057, 0.1533, 0.0114, 0.0114, 0.0057, 0.0057,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0795, 0.0114, 0.0057, 0.0114,\n",
      "                      0.0057, 0.0057, 0.0057, 0.0114, 0.0114, 0.0170, 0.0057,\n",
      "                      0.0057, 0.0114, 0.0057, 0.0057, 0.0057, 0.0057, 0.0057,\n",
      "                      0.0397, 0.0057, 0.0057, 0.0057, 0.0057, 0.0170, 0.0114,\n",
      "                      0.0057, 0.0057, 0.1135, 0.0057, 0.0114]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=866, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0],\n",
      "                       [   4993,   12702,   16120,   18622,   24487,   28007,\n",
      "                          37693,   47334,   48841,   53940,   64103,   66506,\n",
      "                          75066,   86356,   90135,   93860,  102865,  103911,\n",
      "                         116557,  123434,  124896,  125545,  131210,  132835,\n",
      "                         134676,  147803,  151203,  163761,  165719,  172475,\n",
      "                         180414,  181854,  183232,  183580,  185374,  191544,\n",
      "                         195278,  200968,  207087,  226071,  229204,  236418,\n",
      "                         239057,  246056,  247599,  248539,  253407,  258821,\n",
      "                         261287,  275193,  290308,  290371,  303929,  306588,\n",
      "                         311111,  321855,  332445,  335594,  341272,  341771,\n",
      "                         345345,  349124,  351843,  360380,  361492,  366912,\n",
      "                         370644,  393076,  401412,  404216,  407400,  422011,\n",
      "                         425738,  436494,  460613,  463390,  464839,  472932,\n",
      "                         486356,  487103,  489141,  489861,  491896,  497646,\n",
      "                         502723,  507438,  526427,  541285,  549376,  562081,\n",
      "                         566937,  579207,  580349,  592295,  598695,  602716,\n",
      "                         619113,  623066,  627191,  634210,  634875,  645342,\n",
      "                         647355,  660320,  662533,  670091,  676606,  685491,\n",
      "                         686106,  695865,  723662,  739468,  776577,  777942,\n",
      "                         780070,  780816,  782597,  785295,  791191,  794144,\n",
      "                         813363,  815053,  817497,  818687,  820231,  821918,\n",
      "                         827446,  831116,  838812,  844138,  850415,  859121,\n",
      "                         871194,  874094,  874762,  894997,  896739,  902989,\n",
      "                         915102,  917258,  935807,  940994,  944676,  945014,\n",
      "                         947008,  978757,  987798,  988319,  989394, 1010677,\n",
      "                        1011474, 1011917, 1014259, 1017321, 1018689, 1026007,\n",
      "                        1033052, 1033889, 1034821, 1034967, 1038630, 1039313,\n",
      "                        1040609, 1041634, 1043325]]),\n",
      "       values=tensor([0.1025, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.0342, 0.2734, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.5127, 0.0684, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0684, 0.0684, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0684, 0.0342, 0.0342, 0.0684, 0.2051, 0.0342,\n",
      "                      0.0684, 0.0342, 0.0684, 0.0684, 0.0342, 0.0342, 0.1367,\n",
      "                      0.0684, 0.0342, 0.0342, 0.0342, 0.0684, 0.0342, 0.0684,\n",
      "                      0.0342, 0.1025, 0.0342, 0.0342, 0.1025, 0.0342, 0.0684,\n",
      "                      0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.2734, 0.0342,\n",
      "                      0.0684, 0.1025, 0.0342, 0.0684, 0.0342, 0.0342, 0.0342,\n",
      "                      0.1367, 0.0342, 0.0684, 0.0342, 0.1709, 0.0342, 0.1709,\n",
      "                      0.0684, 0.0684, 0.0342, 0.0684, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0684, 0.0684,\n",
      "                      0.0342, 0.0342, 0.0342, 0.0684, 0.0684, 0.0342, 0.0342,\n",
      "                      0.0684, 0.0684, 0.0342, 0.0684, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0684, 0.0342, 0.0342, 0.0342, 0.1025, 0.1025, 0.0342,\n",
      "                      0.0342, 0.0684, 0.0342, 0.1025, 0.0342, 0.2051, 0.0342,\n",
      "                      0.0684, 0.0342, 0.0342, 0.0342, 0.0342, 0.0684, 0.1025,\n",
      "                      0.0342, 0.0684, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.1709, 0.0342, 0.0342, 0.0684, 0.0684, 0.0684,\n",
      "                      0.0342, 0.0342, 0.0684, 0.0342, 0.0342, 0.0342, 0.0342,\n",
      "                      0.0342, 0.0342, 0.0684, 0.0342]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=165, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [  55764,   55813,   70775,   80277,  103593,  106059,\n",
      "                         111683,  113837,  121563,  124195,  126714,  178867,\n",
      "                         179131,  190796,  196296,  205454,  207582,  227698,\n",
      "                         230201,  236739,  248399,  250486,  251099,  260788,\n",
      "                         268510,  323038,  331110,  341771,  342183,  349571,\n",
      "                         354017,  355353,  366000,  385598,  390965,  407250,\n",
      "                         409325,  417871,  433722,  435649,  442341,  459642,\n",
      "                         464598,  467797,  489861,  513664,  517464,  524578,\n",
      "                         530131,  532443,  557539,  575575,  580349,  581560,\n",
      "                         586679,  587173,  592027,  607841,  608697,  613400,\n",
      "                         617684,  635649,  648388,  658632,  665941,  676526,\n",
      "                         713106,  737585,  792819,  822949,  836789,  842188,\n",
      "                         850415,  857474,  863238,  872440,  884392,  893290,\n",
      "                         899277,  903694,  944787,  948774,  961694,  982773,\n",
      "                         989351,  994998, 1001553, 1020101, 1030747, 1045494]]),\n",
      "       values=tensor([0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.1538, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.2308, 0.1538, 0.0769, 0.0769, 0.1538, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.0769, 0.1538, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.1538, 0.1538, 0.3846, 0.1538,\n",
      "                      0.0769, 0.0769, 0.0769, 0.2308, 0.1538, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.1538, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.1538, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.1538, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769, 0.0769,\n",
      "                      0.0769, 0.1538, 0.0769, 0.1538, 0.0769, 0.0769]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=90, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[     0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0],\n",
      "                       [  4993,  30089,  55531,  59824,  72242,  81097,  83224,\n",
      "                         89164,  91729,  93216, 101466, 122688, 138396, 138804,\n",
      "                        144109, 147939, 171836, 179579, 180584, 181854, 197014,\n",
      "                        202578, 212317, 226071, 261287, 286447, 288082, 291916,\n",
      "                        335594, 341769, 375128, 402535, 402576, 410594, 421640,\n",
      "                        431894, 433749, 435080, 436838, 437253, 438129, 444746,\n",
      "                        463390, 478084, 489375, 503211, 530555, 532357, 561926,\n",
      "                        565561, 579038, 593865, 607058, 620992, 654930, 659143,\n",
      "                        660320, 676606, 688111, 689312, 689560, 689676, 690661,\n",
      "                        710331, 740660, 743770, 758354, 795166, 797633, 804474,\n",
      "                        818597, 837119, 849587, 856162, 859584, 887069, 891788,\n",
      "                        920708, 926952, 933164, 936007, 964585, 989394, 990225,\n",
      "                        995585]]),\n",
      "       values=tensor([0.2696, 0.0539, 0.0539, 0.0539, 0.1078, 0.1078, 0.0539,\n",
      "                      0.2696, 0.0539, 0.0539, 0.0539, 0.0539, 0.1078, 0.0539,\n",
      "                      0.1617, 0.0539, 0.0539, 0.0539, 0.1078, 0.1078, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.1078, 0.0539, 0.0539, 0.2157,\n",
      "                      0.0539, 0.1078, 0.0539, 0.2157, 0.0539, 0.0539, 0.0539,\n",
      "                      0.1078, 0.3235, 0.0539, 0.0539, 0.2157, 0.0539, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.1617, 0.0539, 0.0539, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.0539, 0.0539, 0.1078, 0.3235,\n",
      "                      0.0539, 0.1617, 0.0539, 0.0539, 0.0539, 0.0539, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.0539, 0.0539, 0.0539, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.2696, 0.0539, 0.0539, 0.0539,\n",
      "                      0.0539, 0.0539, 0.0539, 0.2157, 0.0539, 0.1078, 0.0539,\n",
      "                      0.1078]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=85, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[     0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0,      0,      0,      0,      0,\n",
      "                             0,      0,      0],\n",
      "                       [  4993,  13205,  37299,  51833,  60951,  72242,  82454,\n",
      "                         83224,  86356,  87998, 102812, 102865, 104396, 107756,\n",
      "                        113400, 127604, 129255, 152864, 163377, 166631, 177821,\n",
      "                        180414, 181854, 232366, 235234, 256425, 258821, 261287,\n",
      "                        286447, 329595, 338720, 342183, 348563, 360380, 373756,\n",
      "                        374205, 387864, 393938, 433749, 448946, 461813, 463390,\n",
      "                        470779, 476592, 478794, 497646, 501295, 527730, 528363,\n",
      "                        532357, 533850, 555920, 559591, 595785, 596932, 643423,\n",
      "                        649227, 649972, 666657, 666813, 669217, 688111, 710331,\n",
      "                        737566, 754424, 762584, 777475, 780733, 788264, 813020,\n",
      "                        815971, 817497, 825495, 847848, 850337, 871194, 897660,\n",
      "                        908044, 918281, 926862, 960073, 965993, 989394, 990799,\n",
      "                        992917, 993256, 998923]]),\n",
      "       values=tensor([0.0487, 0.0975, 0.0975, 0.0487, 0.0487, 0.0487, 0.0975,\n",
      "                      0.0487, 0.0975, 0.0975, 0.0487, 0.0487, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0487, 0.0487, 0.2437, 0.0487, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0975, 0.1949, 0.0487, 0.0487, 0.0975, 0.0975,\n",
      "                      0.0487, 0.0487, 0.0487, 0.0487, 0.0487, 0.0487, 0.0487,\n",
      "                      0.0975, 0.0487, 0.0487, 0.0487, 0.0487, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0975, 0.0487, 0.0487, 0.3899, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0487, 0.0487, 0.0975, 0.0487, 0.0487, 0.0487,\n",
      "                      0.3899, 0.0487, 0.0975, 0.0487, 0.0487, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0975, 0.0487, 0.0975, 0.3412, 0.0487, 0.0975,\n",
      "                      0.0487, 0.0487, 0.0487, 0.2924, 0.1949, 0.0487, 0.0487,\n",
      "                      0.0487, 0.1462, 0.0487, 0.0487, 0.0975, 0.0487, 0.0487,\n",
      "                      0.0487, 0.0487, 0.1949]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=87, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [   5237,    6908,    8203,    9649,   18718,   19260,\n",
      "                          23767,   41258,   42127,   55531,   59357,   71105,\n",
      "                          72242,   75066,   77949,   81322,   86937,   88697,\n",
      "                          91323,  104343,  106263,  106521,  106683,  114491,\n",
      "                         136278,  138883,  145998,  153000,  164918,  167774,\n",
      "                         171836,  172475,  174569,  181854,  192933,  202893,\n",
      "                         214161,  235683,  235759,  235779,  244038,  261287,\n",
      "                         267860,  272706,  284125,  295663,  310886,  312820,\n",
      "                         315942,  320001,  323110,  327219,  341182,  342316,\n",
      "                         342345,  346090,  348936,  352161,  361048,  365508,\n",
      "                         367444,  372667,  377412,  379135,  379628,  379994,\n",
      "                         380611,  382164,  393055,  393721,  396709,  399232,\n",
      "                         402858,  409518,  416722,  421904,  426826,  432550,\n",
      "                         432666,  434106,  438503,  440835,  443347,  449144,\n",
      "                         451795,  463390,  484354,  492068,  495184,  495511,\n",
      "                         496873,  504262,  510472,  513664,  515413,  521557,\n",
      "                         525567,  532647,  544891,  547692,  547874,  549676,\n",
      "                         566268,  569221,  574953,  578340,  583408,  596318,\n",
      "                         609877,  609898,  629098,  636331,  644065,  650246,\n",
      "                         667552,  669342,  682318,  688111,  693611,  719779,\n",
      "                         719876,  722113,  723879,  743307,  749730,  754424,\n",
      "                         778315,  779596,  780418,  786746,  789688,  795936,\n",
      "                         816717,  822828,  822945,  825495,  826251,  827473,\n",
      "                         838902,  839170,  843846,  850722,  854887,  864371,\n",
      "                         865391,  890292,  891999,  894364,  898450,  899093,\n",
      "                         902851,  905954,  906218,  919606,  923147,  933203,\n",
      "                         946452,  972095,  975864,  982849,  983945,  989394,\n",
      "                         989474, 1000438, 1019705, 1030915, 1037089, 1039864]]),\n",
      "       values=tensor([0.1775, 0.0394, 0.0394, 0.0394, 0.0197, 0.0394, 0.0197,\n",
      "                      0.0789, 0.1380, 0.1183, 0.0394, 0.1380, 0.0394, 0.0197,\n",
      "                      0.0197, 0.0592, 0.0394, 0.0986, 0.0592, 0.0394, 0.0394,\n",
      "                      0.0394, 0.2366, 0.0394, 0.0394, 0.0789, 0.0197, 0.0197,\n",
      "                      0.0394, 0.0394, 0.1183, 0.1380, 0.0197, 0.0986, 0.0197,\n",
      "                      0.0986, 0.0197, 0.0394, 0.0394, 0.0394, 0.0789, 0.0197,\n",
      "                      0.1380, 0.0197, 0.0197, 0.0789, 0.0197, 0.1577, 0.0394,\n",
      "                      0.1380, 0.0986, 0.0592, 0.0394, 0.0592, 0.1380, 0.0197,\n",
      "                      0.1972, 0.0394, 0.0197, 0.0197, 0.0197, 0.0197, 0.0394,\n",
      "                      0.0197, 0.0197, 0.0197, 0.0197, 0.0592, 0.0197, 0.1380,\n",
      "                      0.0394, 0.0197, 0.0197, 0.0197, 0.0197, 0.0197, 0.0394,\n",
      "                      0.0394, 0.0394, 0.1577, 0.0394, 0.0789, 0.0197, 0.0394,\n",
      "                      0.0592, 0.0197, 0.0197, 0.1380, 0.0197, 0.0394, 0.0197,\n",
      "                      0.0394, 0.0197, 0.0394, 0.1380, 0.0394, 0.1380, 0.0197,\n",
      "                      0.1183, 0.0197, 0.3746, 0.0394, 0.0394, 0.0592, 0.0197,\n",
      "                      0.0986, 0.0986, 0.0197, 0.1577, 0.0986, 0.0394, 0.0197,\n",
      "                      0.0789, 0.1577, 0.0197, 0.0394, 0.0394, 0.0986, 0.1380,\n",
      "                      0.0197, 0.0197, 0.0197, 0.0789, 0.0197, 0.0394, 0.0394,\n",
      "                      0.0789, 0.0197, 0.0394, 0.1380, 0.0394, 0.0789, 0.0394,\n",
      "                      0.0197, 0.0197, 0.0394, 0.0394, 0.1972, 0.0394, 0.0197,\n",
      "                      0.0986, 0.0394, 0.0592, 0.0394, 0.1183, 0.0197, 0.0789,\n",
      "                      0.0986, 0.0197, 0.0592, 0.0394, 0.0197, 0.0197, 0.0197,\n",
      "                      0.0394, 0.0197, 0.0197, 0.0592, 0.0394, 0.0592, 0.0197,\n",
      "                      0.0394, 0.0197, 0.0394, 0.0197, 0.0986, 0.0197, 0.0197]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=168, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     218,     235,  ..., 1045579, 1046172,\n",
      "                        1047114]]),\n",
      "       values=tensor([0.0012, 0.0023, 0.0069,  ..., 0.0012, 0.0023, 0.0023]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2617, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   1377,   11388,   13196,   13751,   33621,   51833,\n",
      "                          72242,   86937,  102865,  124896,  126586,  129001,\n",
      "                         148144,  169846,  181182,  202578,  212235,  218862,\n",
      "                         219854,  230859,  236037,  250150,  258511,  261287,\n",
      "                         274712,  312950,  377670,  382122,  391421,  403294,\n",
      "                         427239,  434197,  437724,  463020,  463390,  499894,\n",
      "                         555474,  555920,  584494,  611746,  619417,  620992,\n",
      "                         659801,  688111,  693127,  697605,  700745,  706757,\n",
      "                         709886,  711402,  761760,  767631,  770253,  782539,\n",
      "                         786953,  795166,  797503,  800066,  815971,  822200,\n",
      "                         822945,  825495,  843518,  847848,  873258,  884840,\n",
      "                         887069,  891788,  891999,  915747,  922198,  922873,\n",
      "                         931644,  940142,  977772,  979567,  982773,  989394,\n",
      "                         999983, 1011095, 1011404, 1014118, 1019906, 1031473,\n",
      "                        1032175, 1038788]]),\n",
      "       values=tensor([0.0422, 0.1266, 0.1266, 0.0422, 0.0422, 0.0422, 0.0211,\n",
      "                      0.1055, 0.0422, 0.1266, 0.0422, 0.2954, 0.1477, 0.1266,\n",
      "                      0.0844, 0.1266, 0.0844, 0.0844, 0.1266, 0.0422, 0.1688,\n",
      "                      0.0422, 0.0422, 0.1055, 0.0422, 0.0422, 0.0422, 0.2954,\n",
      "                      0.1266, 0.0422, 0.0422, 0.1266, 0.1477, 0.0422, 0.0211,\n",
      "                      0.0422, 0.0422, 0.1266, 0.2321, 0.1266, 0.0422, 0.0422,\n",
      "                      0.1266, 0.0211, 0.0422, 0.0844, 0.0422, 0.0422, 0.1266,\n",
      "                      0.0211, 0.0422, 0.1688, 0.0844, 0.0422, 0.1266, 0.0211,\n",
      "                      0.0844, 0.0422, 0.0422, 0.2954, 0.0844, 0.0211, 0.0422,\n",
      "                      0.0844, 0.1266, 0.1688, 0.0844, 0.1266, 0.0422, 0.0422,\n",
      "                      0.0844, 0.0422, 0.1266, 0.0422, 0.1266, 0.0844, 0.1266,\n",
      "                      0.1055, 0.1266, 0.0211, 0.0422, 0.2110, 0.1266, 0.0422,\n",
      "                      0.1266, 0.0211]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=86, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4993,   13205,   20076,   31332,   37299,   47088,\n",
      "                          59924,   62855,   71032,   71812,   72242,   83224,\n",
      "                          86937,  104542,  109992,  127284,  129001,  144550,\n",
      "                         150336,  161835,  164954,  172527,  174620,  180999,\n",
      "                         182713,  199905,  202578,  203278,  204146,  220895,\n",
      "                         258821,  259508,  261287,  270167,  272698,  274689,\n",
      "                         282699,  306588,  307966,  314106,  319920,  333199,\n",
      "                         334997,  342792,  353478,  363402,  378799,  385698,\n",
      "                         387864,  397863,  406376,  413924,  442341,  463390,\n",
      "                         471026,  483567,  484243,  489141,  521966,  526453,\n",
      "                         534719,  536347,  548579,  558500,  561926,  562084,\n",
      "                         569221,  578971,  583205,  584494,  584505,  634282,\n",
      "                         634572,  652970,  654930,  657745,  665862,  676693,\n",
      "                         679984,  688111,  712954,  718049,  729358,  729497,\n",
      "                         748708,  754424,  758917,  770253,  776247,  776312,\n",
      "                         779639,  781946,  785391,  794035,  798399,  808403,\n",
      "                         817497,  817668,  818597,  820462,  826208,  827473,\n",
      "                         845609,  851931,  873296,  880766,  883158,  884004,\n",
      "                         891788,  892163,  896902,  915747,  915818,  918038,\n",
      "                         936007,  962588,  970730,  977484,  982773,  988070,\n",
      "                         989394,  990225,  999983, 1009103, 1014380, 1020174,\n",
      "                        1031773]]),\n",
      "       values=tensor([0.0267, 0.0534, 0.0267, 0.0801, 0.0267, 0.0801, 0.0534,\n",
      "                      0.3204, 0.1335, 0.1068, 0.0134, 0.0801, 0.0267, 0.0801,\n",
      "                      0.0534, 0.0801, 0.0401, 0.1335, 0.0534, 0.0534, 0.0267,\n",
      "                      0.0267, 0.0267, 0.0267, 0.0267, 0.0267, 0.0801, 0.0267,\n",
      "                      0.0534, 0.0267, 0.1335, 0.0267, 0.2003, 0.0267, 0.0534,\n",
      "                      0.0267, 0.0267, 0.0534, 0.0534, 0.0267, 0.0801, 0.1068,\n",
      "                      0.0267, 0.0534, 0.0267, 0.0267, 0.0267, 0.0267, 0.0267,\n",
      "                      0.0134, 0.0801, 0.0534, 0.0801, 0.0134, 0.0534, 0.0267,\n",
      "                      0.0267, 0.0267, 0.0267, 0.0267, 0.0534, 0.1068, 0.0534,\n",
      "                      0.0267, 0.0267, 0.0534, 0.1068, 0.0267, 0.0267, 0.0401,\n",
      "                      0.0267, 0.0267, 0.0267, 0.0134, 0.1068, 0.0267, 0.0267,\n",
      "                      0.0267, 0.1068, 0.0134, 0.0267, 0.0534, 0.0801, 0.0801,\n",
      "                      0.0267, 0.0267, 0.1068, 0.0801, 0.2670, 0.1602, 0.0267,\n",
      "                      0.0534, 0.0267, 0.0801, 0.0267, 0.1469, 0.2670, 0.0534,\n",
      "                      0.1335, 0.2136, 0.0267, 0.0534, 0.0534, 0.0267, 0.0534,\n",
      "                      0.0267, 0.0267, 0.0267, 0.0801, 0.0267, 0.0534, 0.1335,\n",
      "                      0.0267, 0.0534, 0.0801, 0.0267, 0.0267, 0.0267, 0.0801,\n",
      "                      0.0267, 0.0401, 0.0534, 0.3204, 0.0801, 0.3471, 0.0801,\n",
      "                      0.0267]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=127, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [  15580,   36735,   43427,   50382,   72242,   77469,\n",
      "                          83224,   86356,   94335,  102865,  109992,  131210,\n",
      "                         148594,  186237,  202893,  216113,  220378,  230483,\n",
      "                         231149,  256425,  260963,  261287,  263520,  274945,\n",
      "                         280603,  293514,  313488,  313519,  320073,  327219,\n",
      "                         341769,  361492,  365508,  380983,  425164,  457113,\n",
      "                         463229,  463390,  477587,  520755,  525567,  533850,\n",
      "                         534112,  548716,  555920,  579512,  586305,  595785,\n",
      "                         601654,  611672,  616612,  617270,  627191,  628331,\n",
      "                         649727,  667368,  688111,  693611,  699827,  701891,\n",
      "                         704251,  713518,  723879,  724105,  728086,  737700,\n",
      "                         748220,  753915,  754424,  781946,  794035,  822945,\n",
      "                         846320,  848555,  849865,  892458,  894883,  915202,\n",
      "                         933164,  947078,  982773,  989394,  996342, 1006296,\n",
      "                        1029582, 1031773, 1032156, 1036325, 1048521]]),\n",
      "       values=tensor([0.0604, 0.0604, 0.0604, 0.0604, 0.1208, 0.3625, 0.0604,\n",
      "                      0.1812, 0.0604, 0.0604, 0.0604, 0.3021, 0.0604, 0.0604,\n",
      "                      0.0604, 0.0604, 0.0604, 0.1208, 0.0604, 0.0604, 0.0604,\n",
      "                      0.1208, 0.1812, 0.0604, 0.1208, 0.0604, 0.0604, 0.0604,\n",
      "                      0.2416, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.1208,\n",
      "                      0.1812, 0.0604, 0.1208, 0.0604, 0.0604, 0.0604, 0.0604,\n",
      "                      0.0604, 0.0604, 0.0604, 0.1208, 0.1812, 0.0604, 0.0604,\n",
      "                      0.1812, 0.0604, 0.0604, 0.2416, 0.0604, 0.0604, 0.0604,\n",
      "                      0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.1208,\n",
      "                      0.0604, 0.0604, 0.0604, 0.1208, 0.0604, 0.0604, 0.1812,\n",
      "                      0.0604, 0.0604, 0.2416, 0.0604, 0.0604, 0.0604, 0.0604,\n",
      "                      0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.0604, 0.1208,\n",
      "                      0.0604, 0.1208, 0.0604, 0.0604, 0.0604]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=89, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [  10065,   39277,   56261,   73829,  101993,  102865,\n",
      "                         157036,  170027,  174436,  179575,  188475,  188487,\n",
      "                         190055,  209686,  227399,  233416,  233752,  259510,\n",
      "                         269397,  282200,  291485,  292009,  301563,  313824,\n",
      "                         324259,  327423,  334059,  335353,  335765,  363346,\n",
      "                         363994,  378612,  379610,  381013,  405379,  431455,\n",
      "                         437929,  439318,  440897,  463390,  464609,  480054,\n",
      "                         484705,  484855,  488953,  492032,  499317,  512107,\n",
      "                         528106,  536298,  550551,  580611,  585734,  594372,\n",
      "                         597838,  601803,  613400,  643755,  651079,  659405,\n",
      "                         686889,  695187,  700323,  709608,  723909,  737830,\n",
      "                         745909,  749617,  750658,  762794,  771147,  780144,\n",
      "                         800528,  813913,  819644,  842188,  843832,  855638,\n",
      "                         856917,  863261,  868495,  872711,  873371,  873785,\n",
      "                         875142,  876882,  880027,  888770,  896052,  898816,\n",
      "                         900273,  901130,  906513,  910041,  946196,  957999,\n",
      "                         959367,  970570,  997944, 1007292, 1018621, 1033568,\n",
      "                        1035610, 1041084, 1043427, 1044500, 1044700, 1045546,\n",
      "                        1046435]]),\n",
      "       values=tensor([0.0739, 0.0739, 0.1478, 0.0739, 0.0739, 0.1478, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.1478, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.2957, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.2957, 0.2218, 0.0739, 0.0739,\n",
      "                      0.1478, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.2957, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.1478, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739, 0.1478, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.1478, 0.0739, 0.0739, 0.0739, 0.0739,\n",
      "                      0.0739, 0.0739, 0.0739, 0.0739]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=109, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0],\n",
      "                       [  13205,   13583,   48373,   72242,   76580,   83224,\n",
      "                         138636,  160635,  169601,  181182,  183232,  202578,\n",
      "                         220895,  222089,  242510,  250486,  261287,  274000,\n",
      "                         296127,  302597,  306588,  326480,  330919,  332873,\n",
      "                         386175,  386462,  392774,  398532,  401992,  402535,\n",
      "                         428477,  451849,  463229,  463390,  484243,  487471,\n",
      "                         488225,  494507,  518126,  520880,  533850,  544367,\n",
      "                         548553,  561926,  592002,  608037,  611746,  639768,\n",
      "                         654930,  656850,  662483,  662515,  668994,  675595,\n",
      "                         680788,  688111,  693045,  709037,  710331,  736822,\n",
      "                         748708,  754048,  789641,  838812,  842342,  859593,\n",
      "                         873258,  891788,  898816,  940322,  978210,  982041,\n",
      "                         982773,  987200, 1031297, 1047785]]),\n",
      "       values=tensor([0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.4206,\n",
      "                      0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.0601,\n",
      "                      0.0601, 0.0601, 0.1202, 0.1803, 0.0601, 0.0601, 0.1202,\n",
      "                      0.0601, 0.0601, 0.0601, 0.0601, 0.1202, 0.0601, 0.0601,\n",
      "                      0.1803, 0.1803, 0.0601, 0.1202, 0.0601, 0.0601, 0.0601,\n",
      "                      0.0601, 0.0601, 0.0601, 0.1202, 0.0601, 0.0601, 0.0601,\n",
      "                      0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.1803, 0.0601,\n",
      "                      0.0601, 0.4206, 0.0601, 0.0601, 0.0601, 0.0601, 0.0601,\n",
      "                      0.2403, 0.0601, 0.0601, 0.0601, 0.1202, 0.0601, 0.0601,\n",
      "                      0.1803, 0.1803, 0.3004, 0.0601, 0.0601, 0.0601, 0.0601,\n",
      "                      0.0601, 0.0601, 0.0601, 0.0601, 0.0601, 0.0601]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=76, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   3104,    4307,    7426,  ..., 1041925, 1043325,\n",
      "                        1045494]]),\n",
      "       values=tensor([0.0291, 0.0388, 0.0097, 0.0097, 0.0194, 0.0097, 0.0679,\n",
      "                      0.0194, 0.0291, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0194, 0.0097, 0.0194, 0.0194, 0.0194, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0582, 0.0194, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0582, 0.0097, 0.0194, 0.0388, 0.0194, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0291,\n",
      "                      0.0097, 0.0097, 0.0485, 0.0485, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0485, 0.0388, 0.0097, 0.0097, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0291, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0291, 0.0097, 0.0097, 0.0291, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0485, 0.0097, 0.0291, 0.0097, 0.0291, 0.0194, 0.0194,\n",
      "                      0.0582, 0.0388, 0.0097, 0.0097, 0.0194, 0.0097, 0.0194,\n",
      "                      0.0194, 0.3785, 0.0097, 0.0097, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0388, 0.0097, 0.0194,\n",
      "                      0.0194, 0.0097, 0.0097, 0.0097, 0.0291, 0.0097, 0.0485,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0291, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0679, 0.0097, 0.0097, 0.0097, 0.0291, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0194, 0.0194, 0.0097,\n",
      "                      0.0582, 0.0097, 0.0485, 0.0097, 0.0097, 0.0097, 0.0485,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0291, 0.0776, 0.0388, 0.0097, 0.0097, 0.0388, 0.0194,\n",
      "                      0.0194, 0.0097, 0.0485, 0.0194, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0291, 0.0194, 0.0097, 0.0291, 0.0291, 0.0097, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0097, 0.0194, 0.0097, 0.0194, 0.0194,\n",
      "                      0.0097, 0.0291, 0.0097, 0.0388, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0291, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0194, 0.0291, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0485, 0.0097, 0.0097, 0.0485, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0291, 0.0097, 0.0194, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0388, 0.0194, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0485, 0.0582,\n",
      "                      0.0194, 0.0388, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0388, 0.0194, 0.0194, 0.0097, 0.0097, 0.1165, 0.0097,\n",
      "                      0.0194, 0.0194, 0.0194, 0.0194, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0291, 0.0097, 0.0194, 0.0485, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0194, 0.0097, 0.0485,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0194, 0.0291, 0.0194, 0.0194, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0291, 0.0485, 0.0097, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0194, 0.0194, 0.0097, 0.0097, 0.0194, 0.0776,\n",
      "                      0.0097, 0.0097, 0.0291, 0.0097, 0.0097, 0.0291, 0.0194,\n",
      "                      0.0194, 0.0097, 0.0097, 0.0388, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0485, 0.0097, 0.0194, 0.0291, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0097, 0.1262, 0.0097, 0.0097, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0388, 0.0097, 0.0582,\n",
      "                      0.0194, 0.0097, 0.0291, 0.0097, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.2426, 0.0291, 0.0097, 0.0097,\n",
      "                      0.0194, 0.0194, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0582, 0.0291, 0.0097, 0.0679, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0388, 0.0097, 0.0194, 0.0582,\n",
      "                      0.0097, 0.0194, 0.0194, 0.1359, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0291, 0.0097, 0.0097, 0.3591,\n",
      "                      0.0097, 0.0291, 0.0291, 0.0097, 0.0194, 0.0097, 0.0097,\n",
      "                      0.0291, 0.0194, 0.0679, 0.0097, 0.0097, 0.0194, 0.0485,\n",
      "                      0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0388, 0.0194, 0.0097, 0.0388, 0.0097, 0.0097, 0.0582,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0194, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0194, 0.0194, 0.0097, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0291, 0.0097, 0.0291, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0485, 0.0194, 0.0291, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0776, 0.0194, 0.0194, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0194, 0.0582, 0.0194, 0.0194, 0.0097, 0.1165, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0388, 0.0194, 0.0097, 0.0194, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0291, 0.0097,\n",
      "                      0.0582, 0.0097, 0.0388, 0.0097, 0.0291, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0291, 0.0388, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0194, 0.0194, 0.0194, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0194, 0.0388,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0873, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0194, 0.0485, 0.0291, 0.0679, 0.0097,\n",
      "                      0.0388, 0.0291, 0.0097, 0.3397, 0.0097, 0.0097, 0.4076,\n",
      "                      0.0291, 0.0097, 0.0097, 0.0097, 0.0097, 0.0388, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0388, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0873, 0.0097, 0.0582, 0.0582, 0.0485,\n",
      "                      0.0097, 0.0388, 0.0097, 0.0097, 0.0194, 0.0194, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0291, 0.0097, 0.0194, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097, 0.0194,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0679, 0.0097, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0582, 0.0194, 0.0097, 0.0097, 0.0097, 0.0388, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0097, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0194, 0.0194, 0.0485,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0194, 0.0194, 0.0194, 0.0097,\n",
      "                      0.0194, 0.0097, 0.0097, 0.0097, 0.0485, 0.0097, 0.0097,\n",
      "                      0.0097, 0.0097, 0.0097, 0.0097, 0.0291, 0.0097, 0.0485,\n",
      "                      0.0194, 0.0097, 0.0097, 0.0194, 0.0097, 0.0291]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=636, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4993,    8356,   15518,   24582,   39427,   48942,\n",
      "                          49586,   58558,   58931,   59924,   67475,   69302,\n",
      "                          77779,   78171,   81097,   83224,   83393,   84151,\n",
      "                          88144,   89986,   91729,  102865,  105129,  106521,\n",
      "                         107669,  108799,  109992,  115430,  124600,  125052,\n",
      "                         127846,  135242,  136177,  141520,  144165,  150843,\n",
      "                         155421,  155868,  159658,  160635,  161185,  166486,\n",
      "                         169246,  170127,  172475,  176660,  177642,  177797,\n",
      "                         180414,  181182,  181854,  188122,  188394,  189343,\n",
      "                         190342,  190881,  199905,  204128,  205833,  218949,\n",
      "                         220895,  224777,  227819,  230201,  235234,  236037,\n",
      "                         237435,  247599,  249022,  250486,  253407,  256425,\n",
      "                         258821,  259845,  260963,  261287,  262997,  266125,\n",
      "                         268142,  283449,  291997,  295712,  295735,  303913,\n",
      "                         308978,  313691,  321855,  327219,  333199,  334349,\n",
      "                         335594,  345545,  351430,  353478,  360380,  373756,\n",
      "                         386287,  390990,  391060,  426530,  438228,  451476,\n",
      "                         451635,  451849,  471181,  487471,  489141,  490908,\n",
      "                         493620,  496039,  497646,  503973,  507438,  512083,\n",
      "                         520755,  521953,  522112,  526427,  534719,  556492,\n",
      "                         559349,  561025,  575268,  575830,  576935,  589226,\n",
      "                         594744,  597244,  598053,  605461,  607113,  612065,\n",
      "                         612602,  613616,  613877,  615244,  618039,  619113,\n",
      "                         623066,  627191,  634537,  643213,  653589,  655454,\n",
      "                         659143,  669302,  674650,  676313,  677382,  686106,\n",
      "                         689676,  691102,  694755,  711627,  720228,  732788,\n",
      "                         736686,  739468,  747758,  752103,  754424,  754887,\n",
      "                         756637,  776577,  778214,  778342,  782597,  785391,\n",
      "                         788003,  793928,  804474,  806225,  820455,  822945,\n",
      "                         832913,  833705,  838812,  856179,  859121,  871194,\n",
      "                         874762,  877485,  881252,  883158,  887069,  890515,\n",
      "                         897215,  898414,  903554,  905548,  909616,  919814,\n",
      "                         920708,  922873,  935585,  938692,  952807,  955684,\n",
      "                         956061,  958267,  961580,  962419,  968224,  969771,\n",
      "                         970944,  971070,  978789,  982773,  987610,  989595,\n",
      "                         990225,  995585,  997154,  998923, 1002821, 1005750,\n",
      "                        1011917, 1014259, 1018689, 1030915, 1031297, 1031773,\n",
      "                        1033052, 1034292, 1039352, 1040609, 1041634, 1043325,\n",
      "                        1048521]]),\n",
      "       values=tensor([0.0338, 0.0338, 0.0338, 0.1014, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0676, 0.0338,\n",
      "                      0.0676, 0.0338, 0.0338, 0.0338, 0.0676, 0.0338, 0.0676,\n",
      "                      0.0676, 0.0676, 0.0338, 0.0338, 0.0676, 0.0338, 0.0338,\n",
      "                      0.0676, 0.0338, 0.0676, 0.0338, 0.0676, 0.0676, 0.0338,\n",
      "                      0.0338, 0.1014, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.2028, 0.0338, 0.0676,\n",
      "                      0.0676, 0.3719, 0.0676, 0.0338, 0.0338, 0.0338, 0.0676,\n",
      "                      0.0676, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0676,\n",
      "                      0.1014, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.2028, 0.1014, 0.0338,\n",
      "                      0.0338, 0.0338, 0.1352, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.1690, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338, 0.1352,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0676, 0.0676, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0676, 0.0338, 0.0338, 0.1014, 0.0676,\n",
      "                      0.0676, 0.0338, 0.0338, 0.0676, 0.0338, 0.0676, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.1352, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.1014, 0.1014, 0.0338, 0.0676, 0.0338, 0.0676,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.2704, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0676,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0676, 0.0338, 0.1014, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0676, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0676, 0.0338, 0.0338, 0.0338, 0.0676, 0.0338, 0.2366,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.1352, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0676, 0.0338, 0.0338, 0.0338,\n",
      "                      0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338, 0.0338,\n",
      "                      0.1014, 0.0338, 0.0338, 0.0338, 0.0338, 0.0676, 0.0338,\n",
      "                      0.0676, 0.1352, 0.0338, 0.0676, 0.0676, 0.0338, 0.0338,\n",
      "                      0.0338, 0.2028, 0.1014, 0.0338, 0.1014]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=229, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4993,   13196,   44157,   61899,   72242,  102865,\n",
      "                         132977,  144109,  177100,  182354,  182713,  216752,\n",
      "                         258821,  261287,  286447,  313862,  332873,  351116,\n",
      "                         365508,  378068,  409865,  463390,  496814,  526453,\n",
      "                         533262,  538443,  562857,  576996,  628331,  638894,\n",
      "                         657095,  668632,  669723,  688111,  724105,  724861,\n",
      "                         727148,  788622,  795166,  817497,  822439,  874751,\n",
      "                         883158,  932301,  935777,  964894,  990542, 1012513,\n",
      "                        1031297]]),\n",
      "       values=tensor([0.0913, 0.1826, 0.0913, 0.0913, 0.1826, 0.1826, 0.0913,\n",
      "                      0.1826, 0.1826, 0.0913, 0.2739, 0.2739, 0.2739, 0.1826,\n",
      "                      0.0913, 0.0913, 0.0913, 0.0913, 0.2739, 0.0913, 0.0913,\n",
      "                      0.0913, 0.0913, 0.0913, 0.0913, 0.0913, 0.0913, 0.0913,\n",
      "                      0.0913, 0.1826, 0.0913, 0.0913, 0.0913, 0.0913, 0.1826,\n",
      "                      0.1826, 0.0913, 0.0913, 0.1826, 0.1826, 0.0913, 0.1826,\n",
      "                      0.0913, 0.0913, 0.0913, 0.0913, 0.0913, 0.1826, 0.0913]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=49, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     235,     378,  ..., 1047289, 1047600,\n",
      "                        1047729]]),\n",
      "       values=tensor([0.0073, 0.0073, 0.0073,  ..., 0.0036, 0.0109, 0.0036]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=3388, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   4993,   13205,   29667,   41680,   43482,   53162,\n",
      "                          72242,   83224,   86356,   89701,   97312,  105850,\n",
      "                         116557,  172149,  182713,  188762,  202578,  202893,\n",
      "                         220644,  231149,  249846,  260327,  261287,  279312,\n",
      "                         283613,  308585,  344134,  361492,  365508,  377670,\n",
      "                         378098,  379610,  404216,  427197,  433749,  436400,\n",
      "                         444746,  462599,  463390,  483314,  484243,  506551,\n",
      "                         517901,  521557,  522112,  533889,  559591,  565561,\n",
      "                         567862,  576957,  581096,  583245,  586305,  594744,\n",
      "                         601303,  613877,  631377,  659143,  676313,  676606,\n",
      "                         678320,  682190,  688111,  690661,  692291,  705894,\n",
      "                         729497,  730014,  730608,  742242,  750530,  754424,\n",
      "                         768534,  795166,  795345,  798860,  804729,  814001,\n",
      "                         817910,  818597,  822945,  847355,  847848,  847877,\n",
      "                         868920,  882054,  887069,  889716,  891788,  892626,\n",
      "                         906532,  915333,  933164,  961291,  964894,  965518,\n",
      "                         982395,  995585,  998969, 1005669, 1022977]]),\n",
      "       values=tensor([0.1406, 0.0469, 0.0469, 0.0938, 0.0469, 0.1875, 0.0938,\n",
      "                      0.0469, 0.0469, 0.1875, 0.1875, 0.0469, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.2344,\n",
      "                      0.0469, 0.1406, 0.0469, 0.4219, 0.0469, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.1406,\n",
      "                      0.0938, 0.0938, 0.0469, 0.0469, 0.0938, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.1406,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.1406,\n",
      "                      0.0469, 0.0938, 0.0469, 0.0938, 0.0469, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.1406, 0.1875, 0.0938, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.1875,\n",
      "                      0.1406, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.4219,\n",
      "                      0.1875, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469, 0.0469,\n",
      "                      0.0469, 0.0469, 0.0469]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=101, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   1058,    7214,   41680,   63531,   72242,   83224,\n",
      "                         104542,  109777,  119937,  122692,  146005,  151176,\n",
      "                         160635,  174569,  177692,  186834,  202578,  202893,\n",
      "                         212235,  233764,  234230,  243792,  258122,  261287,\n",
      "                         277563,  282085,  284125,  284163,  299261,  306588,\n",
      "                         311111,  328671,  337898,  339010,  348936,  352161,\n",
      "                         356488,  366283,  380611,  386254,  388461,  391111,\n",
      "                         409518,  428477,  432823,  433291,  440705,  444013,\n",
      "                         451795,  463390,  466563,  484243,  487471,  506479,\n",
      "                         533850,  534719,  537410,  544194,  555920,  559591,\n",
      "                         561590,  562081,  569221,  571785,  575313,  576122,\n",
      "                         581096,  584494,  584505,  628592,  628764,  666657,\n",
      "                         666813,  688111,  693611,  725164,  727903,  743770,\n",
      "                         745183,  750530,  775923,  781946,  782597,  794035,\n",
      "                         794188,  807850,  813363,  817497,  822945,  890292,\n",
      "                         891788,  897447,  906218,  915333,  915747,  943627,\n",
      "                         989394, 1007033, 1010414, 1018689, 1045494]]),\n",
      "       values=tensor([0.0537, 0.0537, 0.1610, 0.0268, 0.0537, 0.0537, 0.0537,\n",
      "                      0.0537, 0.0537, 0.0805, 0.0537, 0.1073, 0.1342, 0.0268,\n",
      "                      0.0537, 0.0268, 0.0537, 0.0268, 0.0537, 0.0537, 0.0537,\n",
      "                      0.0537, 0.0537, 0.0805, 0.0537, 0.0268, 0.0268, 0.0537,\n",
      "                      0.0268, 0.0537, 0.0537, 0.0537, 0.0268, 0.0805, 0.0268,\n",
      "                      0.1610, 0.1610, 0.0268, 0.0268, 0.0537, 0.0537, 0.0537,\n",
      "                      0.0268, 0.0537, 0.0537, 0.1073, 0.0537, 0.0268, 0.0268,\n",
      "                      0.0268, 0.1073, 0.0537, 0.0537, 0.0537, 0.0537, 0.0537,\n",
      "                      0.0268, 0.0537, 0.0268, 0.0537, 0.0268, 0.0268, 0.1073,\n",
      "                      0.0268, 0.0537, 0.0268, 0.0537, 0.1610, 0.0268, 0.0268,\n",
      "                      0.0537, 0.0537, 0.1878, 0.0268, 0.0537, 0.0537, 0.1073,\n",
      "                      0.0268, 0.1073, 0.0268, 0.1878, 0.2415, 0.0537, 0.1610,\n",
      "                      0.0537, 0.0537, 0.0537, 0.2147, 0.0537, 0.0805, 0.0537,\n",
      "                      0.0268, 0.0268, 0.0537, 0.6440, 0.0537, 0.0537, 0.0537,\n",
      "                      0.0537, 0.0537, 0.0268]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=101, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    375,     600,     682,  ..., 1046570, 1047802,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0012, 0.0012, 0.0012,  ..., 0.0085, 0.0061, 0.0036]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2146, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0],\n",
      "                       [   4747,    4993,   47273,   50793,   59409,   65164,\n",
      "                          70684,   72242,  100149,  102933,  104542,  109992,\n",
      "                         124977,  160635,  179131,  202085,  202578,  261287,\n",
      "                         270167,  292009,  323109,  348936,  352161,  353639,\n",
      "                         401798,  434197,  436400,  439491,  444746,  463390,\n",
      "                         466563,  472442,  481531,  483567,  484243,  489375,\n",
      "                         494082,  497646,  520755,  532647,  533537,  534719,\n",
      "                         535621,  544194,  555920,  574942,  609898,  616355,\n",
      "                         616612,  619417,  625786,  626912,  631674,  645475,\n",
      "                         654930,  661260,  674972,  678934,  688111,  689676,\n",
      "                         695058,  695066,  696949,  700786,  710331,  738341,\n",
      "                         752103,  754424,  756012,  761760,  779343,  779494,\n",
      "                         782397,  785599,  795104,  796613,  818597,  840228,\n",
      "                         847848,  849984,  856162,  871737,  881413,  881533,\n",
      "                         884004,  886575,  889476,  890292,  891788,  893574,\n",
      "                         906973,  914583,  915333,  916173,  932301,  936007,\n",
      "                         963138,  973725,  982773,  988071, 1005669, 1019906,\n",
      "                        1035669, 1037403]]),\n",
      "       values=tensor([0.0484, 0.0968, 0.0484, 0.0968, 0.0484, 0.1936, 0.0968,\n",
      "                      0.0968, 0.0484, 0.0968, 0.0484, 0.0484, 0.0484, 0.0484,\n",
      "                      0.0968, 0.0484, 0.0484, 0.0968, 0.0484, 0.0484, 0.0484,\n",
      "                      0.0484, 0.3871, 0.0484, 0.0968, 0.0484, 0.0968, 0.3388,\n",
      "                      0.0968, 0.0484, 0.0484, 0.0968, 0.1936, 0.0968, 0.0968,\n",
      "                      0.0484, 0.0484, 0.0968, 0.0484, 0.1936, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0484, 0.0484, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0968, 0.1936, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0484, 0.2420, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0484, 0.0484, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0968, 0.1452, 0.0484, 0.0484,\n",
      "                      0.0484, 0.1452, 0.0484, 0.0484, 0.1936, 0.1936, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.1452, 0.0484, 0.0484, 0.0484,\n",
      "                      0.0484, 0.0484, 0.0484, 0.0484, 0.1936, 0.0484, 0.0484,\n",
      "                      0.0968, 0.0484, 0.1452, 0.0484, 0.0484, 0.0484]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=104, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,    3453,    5866,  ..., 1035434, 1040169,\n",
      "                        1047065]]),\n",
      "       values=tensor([0.0193, 0.0193, 0.0387, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0774, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0580, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.1741,\n",
      "                      0.0580, 0.0774, 0.0193, 0.0193, 0.0580, 0.0387, 0.0387,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0387,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0967, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.1354, 0.0580, 0.0193, 0.0193, 0.0193, 0.0580,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0580, 0.0193,\n",
      "                      0.0193, 0.1161, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0967, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0387, 0.0580, 0.2128, 0.1354,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0580, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.1354, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.2128, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0193, 0.0387, 0.0193, 0.0387,\n",
      "                      0.0193, 0.0580, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0967, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0387,\n",
      "                      0.1354, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0580, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0967,\n",
      "                      0.0387, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0580,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0774, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0580, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0580, 0.0193, 0.0387, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0967, 0.0193, 0.0580, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0387, 0.0580, 0.0193, 0.0580, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0774, 0.0580, 0.0193, 0.0580,\n",
      "                      0.0580, 0.0387, 0.0193, 0.0387, 0.0193, 0.0774, 0.0193,\n",
      "                      0.0193, 0.0580, 0.0193, 0.0580, 0.0387, 0.0193, 0.0387,\n",
      "                      0.1354, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0580, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0774, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0387, 0.0387, 0.0193, 0.0774, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0967, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.1161, 0.0193, 0.0193, 0.0193, 0.0387, 0.0387, 0.1161,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.3289, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0580, 0.0193,\n",
      "                      0.0580, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0387,\n",
      "                      0.0387, 0.0580, 0.0580, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0580, 0.0387,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0193, 0.0387, 0.0580, 0.0580,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0387, 0.2515, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0580, 0.0193, 0.0774, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0387, 0.0387, 0.0193, 0.0774, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0387, 0.0193, 0.0387, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0580, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387, 0.0580, 0.0387, 0.0193, 0.0193,\n",
      "                      0.0967, 0.0193, 0.0774, 0.0193, 0.0193, 0.0193, 0.0387,\n",
      "                      0.0387, 0.0193, 0.0193, 0.0193, 0.0193, 0.0580, 0.0193,\n",
      "                      0.0967, 0.0387, 0.0387, 0.0193, 0.0193, 0.0387, 0.0387,\n",
      "                      0.0387, 0.0193, 0.0580, 0.0580, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0387, 0.0387, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193, 0.0193,\n",
      "                      0.0193, 0.0193, 0.0387]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=598, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    627,    1592,    2180,  ..., 1048189, 1048527,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0008, 0.0008, 0.0017,  ..., 0.0008, 0.0008, 0.0050]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=2753, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [   3301,    5988,   13196,   61899,   72242,   79711,\n",
      "                          80713,   83224,  116432,  116557,  157431,  182713,\n",
      "                         196498,  207087,  216752,  258821,  261287,  270167,\n",
      "                         286447,  289667,  302597,  351043,  357467,  370023,\n",
      "                         376612,  424396,  437253,  444013,  462141,  463390,\n",
      "                         468173,  497646,  518569,  533850,  561926,  562081,\n",
      "                         566937,  568216,  589453,  642013,  643213,  659143,\n",
      "                         666657,  666813,  682190,  688111,  706231,  720228,\n",
      "                         748708,  754424,  791050,  798860,  799928,  845609,\n",
      "                         850415,  902989,  919814,  926952,  936007,  941156,\n",
      "                         964894,  971070,  989394,  990225, 1037220]]),\n",
      "       values=tensor([0.0471, 0.1414, 0.0471, 0.0471, 0.0236, 0.1414, 0.0471,\n",
      "                      0.0471, 0.0471, 0.0471, 0.4713, 0.1178, 0.5184, 0.1885,\n",
      "                      0.0471, 0.0471, 0.0707, 0.0471, 0.0471, 0.0471, 0.0471,\n",
      "                      0.0471, 0.2356, 0.0471, 0.0471, 0.1414, 0.0236, 0.0471,\n",
      "                      0.0471, 0.0236, 0.0471, 0.0471, 0.0943, 0.0471, 0.0943,\n",
      "                      0.0471, 0.0943, 0.3299, 0.0471, 0.0471, 0.0236, 0.0943,\n",
      "                      0.0471, 0.0943, 0.0471, 0.0236, 0.1649, 0.0471, 0.0943,\n",
      "                      0.0471, 0.0236, 0.0471, 0.0471, 0.0471, 0.0471, 0.0943,\n",
      "                      0.0943, 0.1885, 0.0471, 0.0471, 0.0471, 0.0471, 0.0471,\n",
      "                      0.0471, 0.0943]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=65, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0],\n",
      "                       [   4807,    4993,   13583,   30089,   59824,   71032,\n",
      "                          72242,   83224,   86356,   89986,  107669,  114848,\n",
      "                         119937,  122688,  123158,  143719,  148594,  157431,\n",
      "                         160635,  169408,  172527,  180853,  225521,  236858,\n",
      "                         245341,  250732,  261287,  266125,  270360,  284011,\n",
      "                         293038,  302597,  311856,  319456,  331110,  332445,\n",
      "                         332873,  356488,  370023,  382069,  388529,  390990,\n",
      "                         400949,  463390,  469369,  476229,  484243,  497646,\n",
      "                         502141,  516086,  516612,  532357,  549047,  555920,\n",
      "                         561926,  574763,  592652,  602115,  604812,  617480,\n",
      "                         623618,  639716,  643213,  654930,  658726,  688111,\n",
      "                         699827,  706231,  716911,  718167,  724024,  754424,\n",
      "                         775018,  778214,  804474,  813363,  839866,  845609,\n",
      "                         847848,  848380,  851931,  883158,  886575,  894091,\n",
      "                         915333,  922760,  933164,  936007,  941351,  957015,\n",
      "                         961291,  982773,  982849,  990225, 1004041, 1007989,\n",
      "                        1017321, 1024115, 1032578, 1034292, 1039352, 1043312]]),\n",
      "       values=tensor([0.0530, 0.2120, 0.0530, 0.0530, 0.0530, 0.2120, 0.0530,\n",
      "                      0.0530, 0.3180, 0.0530, 0.0530, 0.0530, 0.1060, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.2120, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.1590, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.1060, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.1060, 0.0530,\n",
      "                      0.2120, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.1060, 0.0530, 0.1590, 0.2650, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.2650, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.2120, 0.0530, 0.0530, 0.0530,\n",
      "                      0.1060, 0.1060, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.2650, 0.0530, 0.0530,\n",
      "                      0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530, 0.0530,\n",
      "                      0.0530, 0.1060, 0.0530, 0.3180]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=102, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,     820,    1607,  ..., 1045546, 1045762,\n",
      "                        1048572]]),\n",
      "       values=tensor([0.0086, 0.0086, 0.0029,  ..., 0.0143, 0.0172, 0.0029]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1918, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   4747,   24582,   24739,   25335,   60394,   64333,\n",
      "                          67946,   72242,   81097,   86356,   87586,  102865,\n",
      "                         148594,  160635,  180414,  181422,  181854,  195278,\n",
      "                         202578,  225521,  236037,  250486,  261287,  289350,\n",
      "                         426251,  432391,  440866,  457113,  461847,  463390,\n",
      "                         481736,  496371,  525432,  526300,  533850,  549676,\n",
      "                         560488,  586305,  611672,  627191,  653359,  654930,\n",
      "                         660320,  671989,  682190,  688111,  693781,  721886,\n",
      "                         745183,  752103,  753537,  772575,  779608,  786953,\n",
      "                         808807,  847848,  857747,  866174,  869982,  891788,\n",
      "                         900980,  906973,  909668,  910749,  925946,  979016,\n",
      "                         982773,  995585, 1014259, 1031297, 1033635, 1037924,\n",
      "                        1041591]]),\n",
      "       values=tensor([0.0821, 0.0821, 0.0821, 0.0821, 0.0821, 0.0821, 0.0821,\n",
      "                      0.0411, 0.0821, 0.0821, 0.0821, 0.1643, 0.1643, 0.0821,\n",
      "                      0.0821, 0.0821, 0.1643, 0.0821, 0.0821, 0.1643, 0.0821,\n",
      "                      0.0821, 0.1232, 0.1643, 0.0821, 0.0821, 0.0821, 0.0821,\n",
      "                      0.0821, 0.0411, 0.1643, 0.0821, 0.0411, 0.1643, 0.0821,\n",
      "                      0.1643, 0.0821, 0.0821, 0.1232, 0.1232, 0.0821, 0.0821,\n",
      "                      0.0821, 0.0821, 0.0821, 0.0411, 0.0821, 0.0821, 0.1643,\n",
      "                      0.0821, 0.0821, 0.0821, 0.2464, 0.0821, 0.0411, 0.1643,\n",
      "                      0.0821, 0.0821, 0.0821, 0.0821, 0.0821, 0.4928, 0.0821,\n",
      "                      0.0821, 0.0821, 0.0821, 0.0411, 0.0821, 0.0821, 0.0821,\n",
      "                      0.1643, 0.0821, 0.0821]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=73, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [   2176,    6758,   11672,  ..., 1045707, 1046231,\n",
      "                        1047215]]),\n",
      "       values=tensor([0.0151, 0.0075, 0.0075, 0.0075, 0.0452, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0075, 0.0377, 0.0075,\n",
      "                      0.0151, 0.0075, 0.0151, 0.0829, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0151, 0.0226, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0151, 0.0151, 0.0075, 0.0301,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0377, 0.0829, 0.0226,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0301, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0301,\n",
      "                      0.0075, 0.0226, 0.0075, 0.0678, 0.0527, 0.0075, 0.0151,\n",
      "                      0.0151, 0.0075, 0.0151, 0.0151, 0.0226, 0.0151, 0.0301,\n",
      "                      0.0075, 0.0151, 0.0151, 0.0678, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0301, 0.0151, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0301, 0.0151, 0.0301, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0226, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0301, 0.0226, 0.0151, 0.0075, 0.0301, 0.0075, 0.0226,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0377, 0.0151, 0.0678, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0151, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.4218, 0.0075, 0.0075,\n",
      "                      0.0075, 0.3992, 0.1506, 0.0075, 0.0075, 0.0075, 0.0226,\n",
      "                      0.0151, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0151, 0.0151,\n",
      "                      0.1657, 0.0301, 0.0979, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0151, 0.0452, 0.0301, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0226, 0.0075, 0.0075, 0.0452,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0151, 0.0075, 0.0226, 0.0226,\n",
      "                      0.0151, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0301, 0.0075, 0.1582, 0.0151, 0.0075, 0.0075, 0.0904,\n",
      "                      0.0075, 0.0452, 0.0301, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0603, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0226, 0.0075, 0.0075, 0.0377,\n",
      "                      0.0075, 0.0075, 0.0904, 0.0075, 0.0075, 0.0151, 0.0452,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0527, 0.0075, 0.0151, 0.0452, 0.0301,\n",
      "                      0.0075, 0.0075, 0.0452, 0.0226, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0226, 0.0075, 0.0075, 0.0075, 0.0151, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0151, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0151, 0.0151, 0.0527, 0.0075, 0.0226, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0226, 0.0226, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0226, 0.0904, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0151, 0.0075, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0377, 0.0151, 0.0151, 0.1356, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0151, 0.0075, 0.1130, 0.0377,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0226, 0.0075, 0.0075,\n",
      "                      0.0226, 0.0301, 0.0075, 0.2561, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0301, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0301, 0.0452, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0151, 0.0075, 0.0075, 0.0151, 0.0226, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0301, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0301, 0.0151, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0301, 0.0075, 0.0075, 0.0151, 0.0226, 0.0075,\n",
      "                      0.0151, 0.0301, 0.0075, 0.0151, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0226, 0.0377, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0753, 0.0151, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0151, 0.0075, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0075, 0.0075, 0.0075, 0.0226,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0226, 0.0301, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0226, 0.0075, 0.0377, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0452, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0301, 0.0075, 0.0151,\n",
      "                      0.0151, 0.0151, 0.0075, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0226, 0.0075, 0.0151, 0.0151, 0.0075, 0.0151,\n",
      "                      0.0753, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0151, 0.0151, 0.0753, 0.0075,\n",
      "                      0.0226, 0.0151, 0.0075, 0.0527, 0.0527, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0377, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0678, 0.0075, 0.0151, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0226, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0151, 0.0301, 0.0301,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0075, 0.2260, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0301, 0.0301,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0151, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0151, 0.0075, 0.0301, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0226, 0.0075, 0.0075, 0.0075, 0.0603, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0226, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0301, 0.0151, 0.0151, 0.0075, 0.0075, 0.1582, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0452, 0.0151, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0075, 0.0301, 0.0075,\n",
      "                      0.0452, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.1130, 0.0226, 0.0151, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.1883, 0.0075, 0.0075, 0.0151, 0.0151, 0.0075,\n",
      "                      0.1506, 0.0151, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0151, 0.0075, 0.0075, 0.0075, 0.0226, 0.0226, 0.0829,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0151, 0.0075, 0.0151, 0.0075, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0075, 0.0678, 0.0075, 0.0075,\n",
      "                      0.0075, 0.1431, 0.0301, 0.0226, 0.0075, 0.0075, 0.0979,\n",
      "                      0.0075, 0.0075, 0.0151, 0.0075, 0.0151, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0226, 0.0301, 0.0151, 0.0075, 0.0452,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0377, 0.0452,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0301, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0226, 0.0151, 0.0151, 0.0075, 0.0151, 0.0075,\n",
      "                      0.0075, 0.0452, 0.0075, 0.0075, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0301, 0.0151, 0.0075, 0.0151, 0.0151, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0226,\n",
      "                      0.0075, 0.0527, 0.0226, 0.0075, 0.0075, 0.0603, 0.0075,\n",
      "                      0.0075, 0.0301, 0.0075, 0.0075, 0.0075, 0.0075, 0.0075,\n",
      "                      0.0075, 0.0151, 0.0075, 0.0075, 0.0075, 0.0075, 0.0452,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0075, 0.0075, 0.0151, 0.0151,\n",
      "                      0.0075, 0.0075, 0.0075, 0.0226, 0.0075, 0.0377, 0.0075,\n",
      "                      0.1205, 0.0075, 0.0075, 0.0527, 0.0075, 0.0301, 0.0075,\n",
      "                      0.0226]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=869, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0],\n",
      "                       [   3301,    4310,   13196,   26508,   42127,   67946,\n",
      "                          89986,  104712,  107826,  115864,  118415,  119937,\n",
      "                         122124,  123158,  124600,  128057,  132398,  147803,\n",
      "                         157024,  160635,  161390,  169192,  171836,  172475,\n",
      "                         180414,  188343,  193886,  199544,  200886,  201625,\n",
      "                         206102,  232946,  237464,  243089,  247599,  255788,\n",
      "                         256094,  258821,  261287,  284011,  288082,  299606,\n",
      "                         300999,  306588,  308651,  311111,  314106,  317322,\n",
      "                         320574,  331110,  335594,  353478,  371551,  376113,\n",
      "                         379135,  379610,  380221,  387864,  393650,  400255,\n",
      "                         408339,  426398,  427197,  433291,  435735,  451849,\n",
      "                         467814,  467952,  483567,  489861,  494507,  498442,\n",
      "                         499894,  506574,  507438,  520880,  525567,  533636,\n",
      "                         534719,  544367,  549676,  561849,  561926,  562081,\n",
      "                         562480,  568934,  575268,  577110,  581136,  584494,\n",
      "                         592773,  604812,  605564,  613452,  619113,  620678,\n",
      "                         620992,  622368,  623066,  631674,  632166,  637377,\n",
      "                         649727,  665539,  686106,  687256,  698875,  700850,\n",
      "                         702425,  716186,  720228,  727257,  743372,  750530,\n",
      "                         751728,  754424,  771966,  776247,  777298,  781946,\n",
      "                         793704,  797109,  799928,  809082,  813363,  817497,\n",
      "                         825319,  826936,  834162,  839867,  845248,  845609,\n",
      "                         855600,  859096,  859507,  869920,  873283,  895310,\n",
      "                         904073,  908010,  917258,  924956,  932304,  936007,\n",
      "                         940147,  949320,  950245,  961291,  967296,  970699,\n",
      "                         971070,  995585,  997154, 1001867, 1009241, 1010414,\n",
      "                        1011917, 1014259, 1023108, 1026219, 1031623, 1033052,\n",
      "                        1033229]]),\n",
      "       values=tensor([0.0440, 0.0220, 0.0440, 0.0220, 0.0220, 0.0220, 0.0220,\n",
      "                      0.0440, 0.0220, 0.1320, 0.0440, 0.2640, 0.0660, 0.0220,\n",
      "                      0.1100, 0.0220, 0.4400, 0.0440, 0.0660, 0.0440, 0.0220,\n",
      "                      0.0440, 0.0440, 0.0440, 0.0440, 0.0440, 0.0220, 0.0440,\n",
      "                      0.0220, 0.0880, 0.0880, 0.0220, 0.1320, 0.0440, 0.0440,\n",
      "                      0.0660, 0.0440, 0.0220, 0.0880, 0.0220, 0.0220, 0.2200,\n",
      "                      0.0220, 0.0880, 0.0220, 0.0440, 0.0660, 0.0220, 0.0440,\n",
      "                      0.1760, 0.0440, 0.0440, 0.0440, 0.0440, 0.0220, 0.0440,\n",
      "                      0.0440, 0.0220, 0.0220, 0.3080, 0.0440, 0.0880, 0.0440,\n",
      "                      0.0440, 0.0220, 0.0660, 0.0220, 0.0220, 0.0220, 0.0220,\n",
      "                      0.0440, 0.0880, 0.0220, 0.0440, 0.0440, 0.0660, 0.0220,\n",
      "                      0.0440, 0.0440, 0.1320, 0.0220, 0.0220, 0.0880, 0.0880,\n",
      "                      0.0220, 0.0220, 0.0660, 0.0220, 0.0220, 0.0440, 0.0220,\n",
      "                      0.0440, 0.0440, 0.0220, 0.0440, 0.1100, 0.1320, 0.0220,\n",
      "                      0.0440, 0.0440, 0.0220, 0.0220, 0.0660, 0.1320, 0.0440,\n",
      "                      0.0440, 0.0440, 0.0220, 0.1320, 0.0220, 0.0660, 0.0440,\n",
      "                      0.0220, 0.0220, 0.0220, 0.1540, 0.0220, 0.0660, 0.0440,\n",
      "                      0.3520, 0.0220, 0.0440, 0.0440, 0.1100, 0.0440, 0.0220,\n",
      "                      0.0220, 0.0880, 0.0220, 0.0660, 0.0220, 0.0220, 0.0440,\n",
      "                      0.0440, 0.0220, 0.0220, 0.0220, 0.0440, 0.0440, 0.0440,\n",
      "                      0.0440, 0.0440, 0.0220, 0.0220, 0.0220, 0.0660, 0.0440,\n",
      "                      0.0440, 0.0220, 0.0440, 0.0220, 0.0440, 0.0220, 0.0440,\n",
      "                      0.0220, 0.0660, 0.0440, 0.0440, 0.0440, 0.0220, 0.0220,\n",
      "                      0.0220, 0.0220]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=163, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0,       0,\n",
      "                              0,       0,       0,       0,       0],\n",
      "                       [  15518,   24582,   28007,   41680,   44157,   53940,\n",
      "                          72242,   80198,   86356,   86713,  102812,  103911,\n",
      "                         112250,  116557,  130398,  134676,  148594,  184116,\n",
      "                         188762,  202578,  213546,  237435,  261287,  269076,\n",
      "                         270167,  279527,  286620,  291997,  295172,  303805,\n",
      "                         323038,  338517,  355933,  365508,  397007,  401412,\n",
      "                         405286,  428247,  428477,  435305,  440710,  455007,\n",
      "                         463390,  470489,  473046,  489426,  518126,  526453,\n",
      "                         544194,  544889,  552646,  569221,  571393,  576556,\n",
      "                         576996,  581314,  586305,  597244,  634572,  645475,\n",
      "                         654930,  666657,  675135,  679790,  684326,  688111,\n",
      "                         710325,  710331,  711627,  718167,  754424,  758474,\n",
      "                         813696,  820231,  823364,  838529,  847848,  857691,\n",
      "                         859840,  880980,  887069,  891788,  895310,  906973,\n",
      "                         951805,  958413,  970849,  977772, 1037885]]),\n",
      "       values=tensor([0.0544, 0.0544, 0.0544, 0.0544, 0.0544, 0.0544, 0.1088,\n",
      "                      0.0544, 0.1088, 0.0544, 0.0544, 0.1088, 0.0544, 0.0544,\n",
      "                      0.0544, 0.0544, 0.1632, 0.0544, 0.0544, 0.0544, 0.1088,\n",
      "                      0.0544, 0.2176, 0.0544, 0.1088, 0.3264, 0.0544, 0.0544,\n",
      "                      0.0544, 0.0544, 0.1632, 0.0544, 0.0544, 0.0544, 0.0544,\n",
      "                      0.0544, 0.0544, 0.1632, 0.1088, 0.0544, 0.0544, 0.2176,\n",
      "                      0.0544, 0.0544, 0.0544, 0.0544, 0.0544, 0.1088, 0.2720,\n",
      "                      0.0544, 0.0544, 0.3807, 0.0544, 0.0544, 0.0544, 0.0544,\n",
      "                      0.0544, 0.2176, 0.0544, 0.0544, 0.0544, 0.0544, 0.0544,\n",
      "                      0.2176, 0.1632, 0.0544, 0.0544, 0.1088, 0.0544, 0.1088,\n",
      "                      0.0544, 0.0544, 0.1088, 0.0544, 0.0544, 0.1088, 0.0544,\n",
      "                      0.1088, 0.1088, 0.1632, 0.0544, 0.0544, 0.0544, 0.0544,\n",
      "                      0.0544, 0.0544, 0.0544, 0.0544, 0.1088]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=89, layout=torch.sparse_coo), tensor([0., 1.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    213,     465,     481,  ..., 1046796, 1047714,\n",
      "                        1048449]]),\n",
      "       values=tensor([0.0959, 0.0014, 0.0014,  ..., 0.0014, 0.0014, 0.0014]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=4623, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    375,     659,    1399,  ..., 1048291, 1048348,\n",
      "                        1048553]]),\n",
      "       values=tensor([0.0002, 0.0002, 0.0004,  ..., 0.0016, 0.0002, 0.0016]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=5670, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0')), (tensor(indices=tensor([[      0,       0,       0,  ...,       0,       0,\n",
      "                              0],\n",
      "                       [    101,    4993,    5216,  ..., 1046875, 1047114,\n",
      "                        1048029]]),\n",
      "       values=tensor([0.0093, 0.0093, 0.0046,  ..., 0.0046, 0.0046, 0.0046]),\n",
      "       device='cuda:0', size=(1, 1048576), nnz=1052, layout=torch.sparse_coo), tensor([1., 0.], device='cuda:0'))]\n"
     ]
    },
    {
     "ename": "RuntimeError",
     "evalue": "reshape is not implemented for sparse tensors",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mRuntimeError\u001b[0m                              Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[31], line 15\u001b[0m\n\u001b[1;32m     13\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m autocast(device_type\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcuda\u001b[39m\u001b[38;5;124m'\u001b[39m,dtype\u001b[38;5;241m=\u001b[39mtorch\u001b[38;5;241m.\u001b[39mfloat16):\n\u001b[1;32m     14\u001b[0m     optimizer\u001b[38;5;241m.\u001b[39mzero_grad()\n\u001b[0;32m---> 15\u001b[0m     y_pred \u001b[38;5;241m=\u001b[39m \u001b[43mmodel\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     16\u001b[0m     loss \u001b[38;5;241m=\u001b[39m loss_fn(y_pred[:,\u001b[38;5;241m0\u001b[39m], y)\n\u001b[1;32m     18\u001b[0m scaler\u001b[38;5;241m.\u001b[39mscale(loss)\u001b[38;5;241m.\u001b[39mbackward()\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/GreyLit/GreyLiteratureClassifier/src/PyTorch/LogisticRegressionClassifier.py:30\u001b[0m, in \u001b[0;36mLogisticRegressionClassifier.forward\u001b[0;34m(self, vector)\u001b[0m\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, vector):\n\u001b[0;32m---> 30\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mlinear2(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39msigmoid(\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[43mvector\u001b[49m\u001b[43m)\u001b[49m)))\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1532\u001b[0m, in \u001b[0;36mModule._wrapped_call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1530\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_compiled_call_impl(\u001b[38;5;241m*\u001b[39margs, \u001b[38;5;241m*\u001b[39m\u001b[38;5;241m*\u001b[39mkwargs)  \u001b[38;5;66;03m# type: ignore[misc]\u001b[39;00m\n\u001b[1;32m   1531\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m-> 1532\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_call_impl\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/nn/modules/module.py:1541\u001b[0m, in \u001b[0;36mModule._call_impl\u001b[0;34m(self, *args, **kwargs)\u001b[0m\n\u001b[1;32m   1536\u001b[0m \u001b[38;5;66;03m# If we don't have any hooks, we want to skip the rest of the logic in\u001b[39;00m\n\u001b[1;32m   1537\u001b[0m \u001b[38;5;66;03m# this function, and just call forward.\u001b[39;00m\n\u001b[1;32m   1538\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m (\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_forward_pre_hooks\n\u001b[1;32m   1539\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_backward_pre_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_backward_hooks\n\u001b[1;32m   1540\u001b[0m         \u001b[38;5;129;01mor\u001b[39;00m _global_forward_hooks \u001b[38;5;129;01mor\u001b[39;00m _global_forward_pre_hooks):\n\u001b[0;32m-> 1541\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mforward_call\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1543\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[1;32m   1544\u001b[0m     result \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mNone\u001b[39;00m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/nn/modules/linear.py:116\u001b[0m, in \u001b[0;36mLinear.forward\u001b[0;34m(self, input)\u001b[0m\n\u001b[1;32m    115\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mforward\u001b[39m(\u001b[38;5;28mself\u001b[39m, \u001b[38;5;28minput\u001b[39m: Tensor) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Tensor:\n\u001b[0;32m--> 116\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mF\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mlinear\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43minput\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mweight\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mbias\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/torch/utils/_device.py:78\u001b[0m, in \u001b[0;36mDeviceContext.__torch_function__\u001b[0;34m(self, func, types, args, kwargs)\u001b[0m\n\u001b[1;32m     76\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m func \u001b[38;5;129;01min\u001b[39;00m _device_constructors() \u001b[38;5;129;01mand\u001b[39;00m kwargs\u001b[38;5;241m.\u001b[39mget(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m) \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[1;32m     77\u001b[0m     kwargs[\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdevice\u001b[39m\u001b[38;5;124m'\u001b[39m] \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice\n\u001b[0;32m---> 78\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43mfunc\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43margs\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mkwargs\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mRuntimeError\u001b[0m: reshape is not implemented for sparse tensors"
     ]
    }
   ],
   "source": [
    "from IPython.display import clear_output\n",
    "from torch.amp import GradScaler, autocast\n",
    "\n",
    "EPOCHS = 25\n",
    "scaler = GradScaler()\n",
    "\n",
    "\n",
    "with trange(EPOCHS) as epochs:\n",
    "    for epoch in epochs:\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\",leave=False,position=0):\n",
    "        # for x, y in train_loader:\n",
    "\n",
    "            with autocast(device_type='cuda',dtype=torch.float16):\n",
    "                optimizer.zero_grad()\n",
    "                y_pred = model(x)\n",
    "                loss = loss_fn(y_pred[:,0], y)\n",
    "\n",
    "            scaler.scale(loss).backward()\n",
    "            scaler.step(optimizer)\n",
    "            scaler.update()\n",
    "\n",
    "        epochs.set_postfix(loss=loss.item())\n",
    "\n",
    "clear_output()\n",
    "print('Completed Training.')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['A',\n",
       " 'H',\n",
       " 'T',\n",
       " '__abs__',\n",
       " '__add__',\n",
       " '__array_priority__',\n",
       " '__bool__',\n",
       " '__class__',\n",
       " '__delattr__',\n",
       " '__dict__',\n",
       " '__dir__',\n",
       " '__div__',\n",
       " '__doc__',\n",
       " '__eq__',\n",
       " '__format__',\n",
       " '__ge__',\n",
       " '__getattribute__',\n",
       " '__getitem__',\n",
       " '__gt__',\n",
       " '__hash__',\n",
       " '__iadd__',\n",
       " '__idiv__',\n",
       " '__imul__',\n",
       " '__init__',\n",
       " '__init_subclass__',\n",
       " '__isub__',\n",
       " '__iter__',\n",
       " '__itruediv__',\n",
       " '__le__',\n",
       " '__len__',\n",
       " '__lt__',\n",
       " '__matmul__',\n",
       " '__module__',\n",
       " '__mul__',\n",
       " '__ne__',\n",
       " '__neg__',\n",
       " '__new__',\n",
       " '__nonzero__',\n",
       " '__pow__',\n",
       " '__radd__',\n",
       " '__rdiv__',\n",
       " '__reduce__',\n",
       " '__reduce_ex__',\n",
       " '__repr__',\n",
       " '__rmatmul__',\n",
       " '__rmul__',\n",
       " '__rsub__',\n",
       " '__rtruediv__',\n",
       " '__setattr__',\n",
       " '__setitem__',\n",
       " '__sizeof__',\n",
       " '__str__',\n",
       " '__sub__',\n",
       " '__subclasshook__',\n",
       " '__truediv__',\n",
       " '__weakref__',\n",
       " '_add',\n",
       " '_add_sparse',\n",
       " '_arg_min_or_max',\n",
       " '_arg_min_or_max_axis',\n",
       " '_arg_minor_reduce',\n",
       " '_argmax_argmin_code',\n",
       " '_asindices',\n",
       " '_comparison',\n",
       " '_compressed_sparse_matrix__get_has_canonical_format',\n",
       " '_compressed_sparse_matrix__get_sorted',\n",
       " '_compressed_sparse_matrix__set_has_canonical_format',\n",
       " '_compressed_sparse_matrix__set_sorted',\n",
       " '_convert_dense',\n",
       " '_descr',\n",
       " '_get_arrayXarray',\n",
       " '_get_arrayXint',\n",
       " '_get_arrayXslice',\n",
       " '_get_columnXarray',\n",
       " '_get_intXarray',\n",
       " '_get_intXint',\n",
       " '_get_intXslice',\n",
       " '_get_sliceXarray',\n",
       " '_get_sliceXint',\n",
       " '_get_sliceXslice',\n",
       " '_has_canonical_format_kern',\n",
       " '_has_sorted_indices_kern',\n",
       " '_insert_many',\n",
       " '_is_scalar',\n",
       " '_major_index_fancy',\n",
       " '_major_slice',\n",
       " '_max_arg_reduction_mod',\n",
       " '_max_min_reduction_code',\n",
       " '_max_nonzero_reduction_kern',\n",
       " '_max_reduction_kern',\n",
       " '_maximum_minimum',\n",
       " '_min_arg_reduction_mod',\n",
       " '_min_nonzero_reduction_kern',\n",
       " '_min_or_max',\n",
       " '_min_or_max_axis',\n",
       " '_min_reduction_kern',\n",
       " '_minor_index_fancy',\n",
       " '_minor_reduce',\n",
       " '_minor_slice',\n",
       " '_parse_indices',\n",
       " '_perform_insert',\n",
       " '_prepare_indices',\n",
       " '_set_arrayXarray',\n",
       " '_set_arrayXarray_sparse',\n",
       " '_set_intXint',\n",
       " '_set_many',\n",
       " '_shape',\n",
       " '_swap',\n",
       " '_tocsx',\n",
       " '_with_data',\n",
       " '_zero_many',\n",
       " 'arcsin',\n",
       " 'arcsinh',\n",
       " 'arctan',\n",
       " 'arctanh',\n",
       " 'argmax',\n",
       " 'argmin',\n",
       " 'asformat',\n",
       " 'asfptype',\n",
       " 'astype',\n",
       " 'ceil',\n",
       " 'conj',\n",
       " 'conjugate',\n",
       " 'copy',\n",
       " 'count_nonzero',\n",
       " 'data',\n",
       " 'deg2rad',\n",
       " 'device',\n",
       " 'diagonal',\n",
       " 'dot',\n",
       " 'dtype',\n",
       " 'eliminate_zeros',\n",
       " 'expm1',\n",
       " 'floor',\n",
       " 'format',\n",
       " 'get',\n",
       " 'getH',\n",
       " 'get_shape',\n",
       " 'getcol',\n",
       " 'getformat',\n",
       " 'getmaxprint',\n",
       " 'getnnz',\n",
       " 'getrow',\n",
       " 'has_canonical_format',\n",
       " 'has_sorted_indices',\n",
       " 'indices',\n",
       " 'indptr',\n",
       " 'log1p',\n",
       " 'max',\n",
       " 'maximum',\n",
       " 'mean',\n",
       " 'min',\n",
       " 'minimum',\n",
       " 'multiply',\n",
       " 'ndim',\n",
       " 'nnz',\n",
       " 'power',\n",
       " 'rad2deg',\n",
       " 'reshape',\n",
       " 'rint',\n",
       " 'set_shape',\n",
       " 'setdiag',\n",
       " 'shape',\n",
       " 'sign',\n",
       " 'sin',\n",
       " 'sinh',\n",
       " 'size',\n",
       " 'sort_indices',\n",
       " 'sorted_indices',\n",
       " 'sqrt',\n",
       " 'sum',\n",
       " 'sum_duplicates',\n",
       " 'tan',\n",
       " 'tanh',\n",
       " 'toarray',\n",
       " 'tobsr',\n",
       " 'tocoo',\n",
       " 'tocsc',\n",
       " 'tocsr',\n",
       " 'todense',\n",
       " 'todia',\n",
       " 'todok',\n",
       " 'tolil',\n",
       " 'transpose',\n",
       " 'trunc']"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dir(xVector)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "ename": "MemoryError",
     "evalue": "std::bad_alloc: out_of_memory: CUDA error at: /tmp/pip-build-env-plvhtgkx/overlay/lib/python3.10/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mMemoryError\u001b[0m                               Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[17], line 4\u001b[0m\n\u001b[1;32m      1\u001b[0m \u001b[38;5;28;01mwith\u001b[39;00m torch\u001b[38;5;241m.\u001b[39mno_grad():\n\u001b[1;32m      2\u001b[0m     model\u001b[38;5;241m.\u001b[39meval()\n\u001b[0;32m----> 4\u001b[0m     xTestTensor, yTestTensor \u001b[38;5;241m=\u001b[39m \u001b[43mtest_dataset\u001b[49m\u001b[43m[\u001b[49m\u001b[43m:\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m      5\u001b[0m     yTestTensor \u001b[38;5;241m=\u001b[39m yTestTensor[:,\u001b[38;5;241m1\u001b[39m]\n\u001b[1;32m      7\u001b[0m     yPred \u001b[38;5;241m=\u001b[39m model(xTestTensor)\n",
      "File \u001b[0;32m/workspace/GreyLit/GreyLiteratureClassifier/src/PyTorch/LazyTextDataset.py:60\u001b[0m, in \u001b[0;36mLazyTextDataset.__getitem__\u001b[0;34m(self, index)\u001b[0m\n\u001b[1;32m     54\u001b[0m xVector \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mxVectors[index]\n\u001b[1;32m     57\u001b[0m \u001b[38;5;66;03m# xTensor = torch.tensor(xVector.todense(), dtype=self.dtype, device=self.device)\u001b[39;00m\n\u001b[1;32m     58\u001b[0m \u001b[38;5;66;03m# yTensor = torch.tensor(yLabels, dtype=self.dtype, device=self.device)\u001b[39;00m\n\u001b[0;32m---> 60\u001b[0m xTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(\u001b[43mxVector\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtodense\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     61\u001b[0m yTensor \u001b[38;5;241m=\u001b[39m torch\u001b[38;5;241m.\u001b[39mas_tensor(yLabels, dtype\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdtype, device\u001b[38;5;241m=\u001b[39m\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mdevice)\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m xTensor, yTensor\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/cupyx/scipy/sparse/_base.py:554\u001b[0m, in \u001b[0;36mspmatrix.todense\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    552\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mtodense\u001b[39m(\u001b[38;5;28mself\u001b[39m, order\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, out\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m):\n\u001b[1;32m    553\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Return a dense matrix representation of this matrix.\"\"\"\u001b[39;00m\n\u001b[0;32m--> 554\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtoarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mout\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mout\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/cupyx/scipy/sparse/_csr.py:419\u001b[0m, in \u001b[0;36mcsr_matrix.toarray\u001b[0;34m(self, order, out)\u001b[0m\n\u001b[1;32m    415\u001b[0m x\u001b[38;5;241m.\u001b[39msum_duplicates()\n\u001b[1;32m    416\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m (cusparse\u001b[38;5;241m.\u001b[39mcheck_availability(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124msparseToDense\u001b[39m\u001b[38;5;124m'\u001b[39m)\n\u001b[1;32m    417\u001b[0m         \u001b[38;5;129;01mand\u001b[39;00m (\u001b[38;5;129;01mnot\u001b[39;00m runtime\u001b[38;5;241m.\u001b[39mis_hip \u001b[38;5;129;01mor\u001b[39;00m (x\u001b[38;5;241m.\u001b[39mnnz \u001b[38;5;241m>\u001b[39m \u001b[38;5;241m0\u001b[39m))):\n\u001b[1;32m    418\u001b[0m     \u001b[38;5;66;03m# On HIP, nnz=0 is problematic as of ROCm 4.2.0\u001b[39;00m\n\u001b[0;32m--> 419\u001b[0m     y \u001b[38;5;241m=\u001b[39m \u001b[43mcusparse\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msparseToDense\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    420\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m order \u001b[38;5;241m==\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mF\u001b[39m\u001b[38;5;124m'\u001b[39m:\n\u001b[1;32m    421\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m y\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/cupyx/cusparse.py:1821\u001b[0m, in \u001b[0;36msparseToDense\u001b[0;34m(x, out)\u001b[0m\n\u001b[1;32m   1819\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m dtype\u001b[38;5;241m.\u001b[39mchar \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mfdFD\u001b[39m\u001b[38;5;124m'\u001b[39m\n\u001b[1;32m   1820\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m out \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m:\n\u001b[0;32m-> 1821\u001b[0m     out \u001b[38;5;241m=\u001b[39m \u001b[43m_cupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mzeros\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[38;5;124;43mF\u001b[39;49m\u001b[38;5;124;43m'\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1822\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m   1823\u001b[0m     \u001b[38;5;28;01massert\u001b[39;00m out\u001b[38;5;241m.\u001b[39mflags\u001b[38;5;241m.\u001b[39mf_contiguous\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/cupy/_creation/basic.py:248\u001b[0m, in \u001b[0;36mzeros\u001b[0;34m(shape, dtype, order)\u001b[0m\n\u001b[1;32m    229\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21mzeros\u001b[39m(\n\u001b[1;32m    230\u001b[0m         shape: _ShapeLike,\n\u001b[1;32m    231\u001b[0m         dtype: DTypeLike \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mfloat\u001b[39m,\n\u001b[1;32m    232\u001b[0m         order: _OrderCF \u001b[38;5;241m=\u001b[39m \u001b[38;5;124m'\u001b[39m\u001b[38;5;124mC\u001b[39m\u001b[38;5;124m'\u001b[39m,\n\u001b[1;32m    233\u001b[0m ) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m NDArray[Any]:\n\u001b[1;32m    234\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Returns a new array of given shape and dtype, filled with zeros.\u001b[39;00m\n\u001b[1;32m    235\u001b[0m \n\u001b[1;32m    236\u001b[0m \u001b[38;5;124;03m    Args:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    246\u001b[0m \n\u001b[1;32m    247\u001b[0m \u001b[38;5;124;03m    \"\"\"\u001b[39;00m\n\u001b[0;32m--> 248\u001b[0m     a \u001b[38;5;241m=\u001b[39m \u001b[43mcupy\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mndarray\u001b[49m\u001b[43m(\u001b[49m\u001b[43mshape\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mdtype\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43morder\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43morder\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    249\u001b[0m     a\u001b[38;5;241m.\u001b[39mdata\u001b[38;5;241m.\u001b[39mmemset_async(\u001b[38;5;241m0\u001b[39m, a\u001b[38;5;241m.\u001b[39mnbytes)\n\u001b[1;32m    250\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m a\n",
      "File \u001b[0;32mcupy/_core/core.pyx:135\u001b[0m, in \u001b[0;36mcupy._core.core.ndarray.__new__\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/_core/core.pyx:223\u001b[0m, in \u001b[0;36mcupy._core.core._ndarray_base._init\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32mcupy/cuda/memory.pyx:738\u001b[0m, in \u001b[0;36mcupy.cuda.memory.alloc\u001b[0;34m()\u001b[0m\n",
      "File \u001b[0;32m/workspace/GreyLit/venv/lib/python3.10/site-packages/rmm/allocators/cupy.py:37\u001b[0m, in \u001b[0;36mrmm_cupy_allocator\u001b[0;34m(nbytes)\u001b[0m\n\u001b[1;32m     34\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mModuleNotFoundError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mNo module named \u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mcupy\u001b[39m\u001b[38;5;124m'\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     36\u001b[0m stream \u001b[38;5;241m=\u001b[39m Stream(obj\u001b[38;5;241m=\u001b[39mcupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mget_current_stream())\n\u001b[0;32m---> 37\u001b[0m buf \u001b[38;5;241m=\u001b[39m \u001b[43mlibrmm\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdevice_buffer\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mDeviceBuffer\u001b[49m\u001b[43m(\u001b[49m\u001b[43msize\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mnbytes\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mstream\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mstream\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     38\u001b[0m dev_id \u001b[38;5;241m=\u001b[39m \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m1\u001b[39m \u001b[38;5;28;01mif\u001b[39;00m buf\u001b[38;5;241m.\u001b[39mptr \u001b[38;5;28;01melse\u001b[39;00m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mdevice\u001b[38;5;241m.\u001b[39mget_device_id()\n\u001b[1;32m     39\u001b[0m mem \u001b[38;5;241m=\u001b[39m cupy\u001b[38;5;241m.\u001b[39mcuda\u001b[38;5;241m.\u001b[39mUnownedMemory(\n\u001b[1;32m     40\u001b[0m     ptr\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39mptr, size\u001b[38;5;241m=\u001b[39mbuf\u001b[38;5;241m.\u001b[39msize, owner\u001b[38;5;241m=\u001b[39mbuf, device_id\u001b[38;5;241m=\u001b[39mdev_id\n\u001b[1;32m     41\u001b[0m )\n",
      "File \u001b[0;32mdevice_buffer.pyx:96\u001b[0m, in \u001b[0;36mrmm._lib.device_buffer.DeviceBuffer.__cinit__\u001b[0;34m()\u001b[0m\n",
      "\u001b[0;31mMemoryError\u001b[0m: std::bad_alloc: out_of_memory: CUDA error at: /tmp/pip-build-env-plvhtgkx/overlay/lib/python3.10/site-packages/librmm/include/rmm/mr/device/cuda_memory_resource.hpp:60: cudaErrorMemoryAllocation out of memory"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "\n",
    "    xTestTensor, yTestTensor = test_dataset[:]\n",
    "    yTestTensor = yTestTensor[:,1]\n",
    "\n",
    "    yPred = model(xTestTensor)\n",
    "\n",
    "    yPred = torch.argmax(yPred,dim=1).cpu().numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00      2092\n",
      "           1       1.00      1.00      1.00      1765\n",
      "\n",
      "    accuracy                           1.00      3857\n",
      "   macro avg       1.00      1.00      1.00      3857\n",
      "weighted avg       1.00      1.00      1.00      3857\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(yTestVector, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.9992221934145709\n",
      "F1: 0.9991503823279524\n",
      "Precision: 0.9988674971687429\n",
      "Recall: 0.9994334277620397\n"
     ]
    },
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAj0AAAGwCAYAAABCV9SaAAAAOXRFWHRTb2Z0d2FyZQBNYXRwbG90bGliIHZlcnNpb24zLjkuMSwgaHR0cHM6Ly9tYXRwbG90bGliLm9yZy/TGe4hAAAACXBIWXMAAA9hAAAPYQGoP6dpAABOc0lEQVR4nO3de1xU1doH8N+ADPcZRIVhFEE0URRQyRexvKWB1zLNK4ql6bEDlShmnryAlpimptXJ0pTshbBe73jF+408SuEFjZRUNAEtFQTiNrPfPzhsnWCUYQaBze/7+exP7L3X2vMMDfKwnrX2lgmCIICIiIhI4sxqOwAiIiKip4FJDxERETUITHqIiIioQWDSQ0RERA0Ckx4iIiJqEJj0EBERUYPApIeIiIgahEa1HQA9mVarxa1bt2Bvbw+ZTFbb4RARkYEEQcCDBw+gVqthZlZz4w2FhYUoLi42+jpyuRxWVlYmiKhuYdJTD9y6dQuurq61HQYRERnpxo0baNGiRY1cu7CwEK3c7JB1W2P0tVQqFa5evSq5xIdJTz1gb28PALj+kzsUdqxIkjS90ta7tkMgqjGlKMFx7BL/Pa8JxcXFyLqtwfVkdyjsq/+7IveBFm5+11BcXMykh56+8pKWws7MqA8yUV3WSGZR2yEQ1Zz/PvDpaUxRsLOXwc6++q+jhXSnUTDpISIikhCNoIXGiKdqagSt6YKpY5j0EBERSYgWArSoftZjTN+6jrUSIiIiahA40kNERCQhWmhhTIHKuN51G5MeIiIiCdEIAjRC9UtUxvSt61jeIiIiogaBIz1EREQSwonM+jHpISIikhAtBGiY9FSK5S0iIiJqEJj0EBERSUh5ecuYzRDR0dHo2rUr7O3t4eTkhKFDhyItLU2nTWFhIUJDQ9GkSRPY2dlh+PDhyM7O1mmTkZGBQYMGwcbGBk5OTpg5cyZKS0t12hw+fBhdunSBpaUl2rRpg5iYGINiZdJDREQkIeWrt4zZDHHkyBGEhobixx9/RGJiIkpKShAYGIj8/HyxTXh4OHbs2IEffvgBR44cwa1btzBs2LCHMWs0GDRoEIqLi3Hy5El88803iImJwbx588Q2V69exaBBg9CnTx+kpKRg2rRpeOONN7B3794qxyoTBAmvTZOI3NxcKJVK3PvVg8/eIskKUneq7RCIakypUILD2IacnBwoFIoaeY3y3xW/XnKGvRG/Kx480KJt++xqx3rnzh04OTnhyJEj6NmzJ3JyctCsWTPExcXh1VdfBQD88ssvaN++PZKSktCtWzfs3r0bgwcPxq1bt+Ds7AwAWL16NWbNmoU7d+5ALpdj1qxZ2LlzJy5cuCC+1ujRo3H//n3s2bOnSrHxNygREZGEaE2wAWVJ1KNbUVFRlV4/JycHAODo6AgASE5ORklJCfr16ye2adeuHVq2bImkpCQAQFJSEry9vcWEBwCCgoKQm5uL1NRUsc2j1yhvU36NqmDSQ0REJCGa/67eMmYDAFdXVyiVSnGLjo5+4mtrtVpMmzYNzz33HDp27AgAyMrKglwuh4ODg05bZ2dnZGVliW0eTXjKz5efe1yb3Nxc/PXXX1X63nDJOhERkYRoBBj5lPWy/964cUOnvGVpafnEvqGhobhw4QKOHz9e/QBqEEd6iIiIqAKFQqGzPSnpCQsLQ0JCAg4dOoQWLVqIx1UqFYqLi3H//n2d9tnZ2VCpVGKbv6/mKt9/UhuFQgFra+sqvScmPURERBJiqjk9VSUIAsLCwrBlyxYcPHgQrVq10jnv5+cHCwsLHDhwQDyWlpaGjIwMBAQEAAACAgJw/vx53L59W2yTmJgIhUIBLy8vsc2j1yhvU36NqmB5i4iISEK0kEEDmVH9DREaGoq4uDhs27YN9vb24hwcpVIJa2trKJVKTJo0CdOnT4ejoyMUCgXeeustBAQEoFu3bgCAwMBAeHl5Yfz48ViyZAmysrIwZ84chIaGiiNMU6dOxWeffYZ3330XEydOxMGDB/H9999j586dVY6VIz1ERERUbV988QVycnLQu3dvuLi4iNvGjRvFNitWrMDgwYMxfPhw9OzZEyqVCps3bxbPm5ubIyEhAebm5ggICMC4ceMQEhKCBQsWiG1atWqFnTt3IjExEb6+vli2bBnWrl2LoKCgKsfK+/TUA7xPDzUEvE8PSdnTvE/PmVRn2BnxuyLvgRbPdqj+fXrqMpa3iIiIJERjZHnLmL51HYcNiIiIqEHgSA8REZGEcKRHPyY9REREEqIVZNAKRqzeMqJvXcfyFhERETUIHOkhIiKSEJa39GPSQ0REJCEamEFjRCFHY8JY6homPURERBIiGDmnR+CcHiIiIqL6jSM9REREEsI5Pfox6SEiIpIQjWAGjWDEnB4JP5yK5S0iIiJqEDjSQ0REJCFayKA1YkxDC+kO9TDpISIikhDO6dGP5S0iIiJqEDjSQ0REJCHGT2RmeYuIiIjqgbI5PUY8cJTlLSIiIqL6jSM9REREEqI18tlbXL1FRERE9QLn9OjHpIeIiEhCtDDjfXr04JweIiIiahA40kNERCQhGkEGjWDEzQmN6FvXMekhIiKSEI2RE5k1LG8RERER1W8c6SEiIpIQrWAGrRGrt7RcvUVERET1Actb+rG8RURERA0CR3qIiIgkRAvjVmBpTRdKncOkh4iISEKMvzmhdItA0n1nRERERI/gSA8REZGEGP/sLemOhzDpISIikhAtZNDCmDk9vCMzERER1QMc6dFPuu+MiIiIatzRo0cxZMgQqNVqyGQybN26Vee8TCardFu6dKnYxt3dvcL5xYsX61zn3Llz6NGjB6ysrODq6oolS5YYHCtHeoiIiCTE+JsTGtY3Pz8fvr6+mDhxIoYNG1bhfGZmps7+7t27MWnSJAwfPlzn+IIFCzB58mRx397eXvw6NzcXgYGB6NevH1avXo3z589j4sSJcHBwwJQpU6ocK5MeIiIiCdEKMmiNuU/Pf/vm5ubqHLe0tISlpWWF9gMGDMCAAQP0Xk+lUunsb9u2DX369IGHh4fOcXt7+wpty8XGxqK4uBjr1q2DXC5Hhw4dkJKSguXLlxuU9LC8RURERBW4urpCqVSKW3R0tNHXzM7Oxs6dOzFp0qQK5xYvXowmTZqgc+fOWLp0KUpLS8VzSUlJ6NmzJ+RyuXgsKCgIaWlpuHfvXpVfnyM9REREEqI1srxVfnPCGzduQKFQiMcrG+Ux1DfffAN7e/sKZbC3334bXbp0gaOjI06ePInZs2cjMzMTy5cvBwBkZWWhVatWOn2cnZ3Fc40bN67S6zPpISIikhDjn7Je1lehUOgkPaawbt06BAcHw8rKSuf49OnTxa99fHwgl8vxj3/8A9HR0SZJtsqxvEVEREQ17tixY0hLS8Mbb7zxxLb+/v4oLS3FtWvXAJTNC8rOztZpU76vbx5QZZj0EBERSYgGMqO3mvD111/Dz88Pvr6+T2ybkpICMzMzODk5AQACAgJw9OhRlJSUiG0SExPh6elZ5dIWwKSHiIhIUsrLW8ZshsjLy0NKSgpSUlIAAFevXkVKSgoyMjLENrm5ufjhhx8qHeVJSkrCJ598grNnz+K3335DbGwswsPDMW7cODGhGTt2LORyOSZNmoTU1FRs3LgRK1eu1CmLVQXn9BAREVG1nTlzBn369BH3yxORCRMmICYmBgAQHx8PQRAwZsyYCv0tLS0RHx+PyMhIFBUVoVWrVggPD9dJaJRKJfbt24fQ0FD4+fmhadOmmDdvnkHL1QFAJgiCUI33SE9Rbm4ulEol7v3qAYU9B+dImoLUnWo7BKIaUyqU4DC2IScnx+STg8uV/66Yd6ofrOwsqn2dwrwSLPDfX6Ox1haO9BAREUmIqVZvSRGTHiIiIgnhA0f1k+47IyIiInoER3qIiIgkRIAMWiOWnQs1tGS9LmDSQ0REJCEsb+kn3XdGRERE9AiO9BAREUmIVpBBK1S/RGVM37qOSQ8REZGEaIx8yroxfes66b4zIiIiokdwpIeIiEhCWN7Sj0kPERGRhGhhBq0RhRxj+tZ10n1nRERERI/gSA8REZGEaAQZNEaUqIzpW9cx6SEiIpIQzunRj0kPERGRhAhGPmVd4B2ZiYiIiOo3jvQQERFJiAYyaIx4aKgxfes6Jj1EREQSohWMm5ejFUwYTB3D8hYRERE1CBzpIUmK/9QJJ3Y54MYVS8ittPB6tgCT3r8F1zZFYpviQhm+ilLj8PbGKCmSwa/3A7wVfRONm5WKbX4+Zodvlrjg2i9WsLLRot+Iu3j9vUyYP/KT89tFK3z2rxb49awNlI6leHniHxgZevtpvl2iKhkVlo3nBubAtU0RigvNcPGMDb7+0AU3061qOzQyIa2RE5mN6VvX1eo76927N6ZNm2by68bExMDBwcHk16X641ySHYa89gc+SbiM6Ph0aEqBf41pjcKChx/51ZHN8WOiEnO+vIaPN1/B3WwLLJjkLp5PT7XC3PEeeLZPLj7fl4Z/rb6GH/cp8fWHarFN/gMz/GtMazi3KMZne37F5Lm38L/LVNj1v02e5tslqhKfgHzsiGmKaYOfwezRHjBvJGDRd7/B0lpT26GRCWkhM3qTqlod6dm8eTMsLCxqM4SnKiYmBtOmTcP9+/drOxTJWxT3m87+jE8yMMrbG5fPWcO7Wz7yc82w9ztHvPf5dXR6Pg8AMH15Bib3ao9LyTZo71eAI9sbo1X7Qoybng0AaN6qGG/MuYUPp7pj3Iws2NhpcXBzY5SUyDB9+Q1YyAW4exYiPdUam75shoHj/nzq75vocd4P9tDZXzatJb6/kIpnfP7ChVN2tRQV0dNTqyM9jo6OsLe3r/RccXFxhWMajQZarbamwyIJys81BwDYO5T9RXv5nA1KS8zQuUee2KblM0Vwal6MS8m2AICSYhksLHU/b3IrLYoLzXD5nA0A4FKyLbz982Ehfzjzz6/3A9xMt8KD++Y1+p6IjGWrKPt54GdVWsrvyGzMJlV1przl7u6OhQsXIiQkBAqFAlOmTBHLVNu3b4eXlxcsLS2RkZGBoqIiREREoHnz5rC1tYW/vz8OHz782Nfatm0bunTpAisrK3h4eCAqKgqlpWVzN8aOHYtRo0bptC8pKUHTpk2xYcMGAMCePXvw/PPPw8HBAU2aNMHgwYORnp4utr927RpkMhk2b96MPn36wMbGBr6+vkhKSgIAHD58GK+//jpycnIgk8kgk8kQGRlpmm8kPZZWC6ye3xwduubBvV0hAODu7UawkGthp9Qd1ndoVoK7t8sGQJ/t9QCXztji0BYHaDTAH5kWiF2hKuufXdbm3u1GaNysROca5fv37nDKHNVdMpmAqVG/48J/bHA9zbq2wyETKp/TY8wmVXXqnX388cfw9fXFzz//jLlz5wIACgoK8NFHH2Ht2rVITU2Fk5MTwsLCkJSUhPj4eJw7dw4jRoxA//79cfny5Uqve+zYMYSEhOCdd97BxYsX8eWXXyImJgYffvghACA4OBg7duxAXt7Dv/r37t2LgoICvPLKKwCA/Px8TJ8+HWfOnMGBAwdgZmaGV155pcLI0/vvv4+IiAikpKSgbdu2GDNmDEpLS9G9e3d88sknUCgUyMzMRGZmJiIiIiqNt6ioCLm5uTobVd9n/2qB679YY/YX1w3q59f7Ad6Yewur3nPFYHdfTHy+Hf7nhbL/F7I69ZNDZLiwRb/DrV0hot90q+1QiJ6aOvWn6AsvvIAZM2aI+8eOHUNJSQn+/e9/w9fXFwCQkZGB9evXIyMjA2p12YTSiIgI7NmzB+vXr8eiRYsqXDcqKgrvvfceJkyYAADw8PDAwoUL8e6772L+/PkICgqCra0ttmzZgvHjxwMA4uLi8NJLL4nlt+HDh+tcc926dWjWrBkuXryIjh07iscjIiIwaNAg8XU7dOiAK1euoF27dlAqlZDJZFCpVI/9PkRHRyMqKsqg7x1V7rN/NcepRAWWbbmCZuqHIzKOTqUoKTZDXo65zmjP/TsWcHR6uHpr+D/uYNiUO7ib3Qh2Sg2yb8qxLloNF7eyVWCNnUpx747uvLTy/UdXgRHVJaEf3oT/i7mY8Upr/JEpr+1wyMS0MPLZWxKeyFyn/l599tlnKxyTy+Xw8fER98+fPw+NRoO2bdvCzs5O3I4cOaJTbnrU2bNnsWDBAp32kydPRmZmJgoKCtCoUSOMHDkSsbGxAMpGdbZt24bg4GDxGpcvX8aYMWPg4eEBhUIBd3d3AGVJ2KMejdXFxQUAcPu2YcuXZ8+ejZycHHG7ceOGQf0JEISyhOfkHiWW/HAFqpa6c8Se8SlAIwstfj7+cPLmjSuWuP27HO398nXaymRAE1UpLK0FHNrSGM3UxWjj/RcAoL1fPs6fskXpIxWun47ao0XrQnH+EFHdISD0w5vo3j8H745ojewblrUdENUAwciVW4KEk546NdJja2tb4Zi1tTVksof/A/Ly8mBubo7k5GSYm+tOvrOzq3z1QV5eHqKiojBs2LAK56ysyu5PERwcjF69euH27dtITEyEtbU1+vfvL7YbMmQI3NzcsGbNGqjVami1WnTs2LHChOtHV6OVx23o5GtLS0tYWvIfI2N89q8WOLSlMSLX/wZrO604T8fWXgNLawG2Ci2CxtzFV5HNYe+gga29Bp+/3wLt/fLR3q9AvM4P/26GZ/s8gMwMOLFLie8/d8L7q6+j/KP3wiv3ELtcheUzWmJk6G1c+8UKW9c2xdSoW7XxtokeK2zR7+jzyj1Evt4Kf+WZifPP8h+Yo7iwTv0NTEbgU9b1q1NJT1V07twZGo0Gt2/fRo8eParUp0uXLkhLS0ObNm30tunevTtcXV2xceNG7N69GyNGjBATmD///BNpaWlYs2aN+JrHjx83OHa5XA6Nhn/9Pw0J3zQFAMwc/ozO8RkrMhA46i4AYGrk7zCTCVg42R0lRTI82/sBwqJv6rQ/fUiB71apUFIsg4fXX4hcfxVdX3ggnrdVaLHou3R89q8WCOvfFkrHUgSHZ3O5OtVJQ14r+1x+vFl3VPzjaa5I/N6xNkIieqrqXdLTtm1bBAcHIyQkBMuWLUPnzp1x584dHDhwAD4+PuJ8mkfNmzcPgwcPRsuWLfHqq6/CzMwMZ8+exYULF/DBBx+I7caOHYvVq1fj119/xaFDh8TjjRs3RpMmTfDVV1/BxcUFGRkZeO+99wyO3d3dHXl5eThw4AB8fX1hY2MDGxub6n0j6LH23kp5Yhu5lYCw6N8RFv273jZLfqi8ZPooD69CLN96xZDwiGpFkNq3tkOgp4B3ZNavXr6z9evXIyQkBDNmzICnpyeGDh2K06dPo2XLlpW2DwoKQkJCAvbt24euXbuiW7duWLFiBdzcdFctBAcH4+LFi2jevDmee+458biZmRni4+ORnJyMjh07Ijw8HEuXLjU47u7du2Pq1KkYNWoUmjVrhiVLlhh8DSIioscpL28Zs0mVTBAECT9PVRpyc3OhVCpx71cPKOzrZZ5K9ERB6k61HQJRjSkVSnAY25CTkwOFQlEjr1H+u+LlfRNhYVv9VXkl+cXYFriuRmOtLfWuvEVERET6Gfv8LC5ZJyIionrhaZe3jh49iiFDhkCtVkMmk2Hr1q0651977TXxSQTl26OrowHg7t27CA4OhkKhgIODAyZNmqRzw2AAOHfuHHr06AErKyu4urpWa4oIkx4iIiKqtvz8fPj6+uLzzz/X26Z///7i0wgyMzPx3Xff6ZwPDg5GamoqEhMTkZCQgKNHj2LKlCni+dzcXAQGBsLNzQ3JyclYunQpIiMj8dVXXxkUK8tbREREEvK079MzYMAADBgw4LFtLC0t9T6N4NKlS9izZw9Onz4t3qT4008/xcCBA/Hxxx9DrVYjNjYWxcXFWLduHeRyOTp06ICUlBQsX75cJzl6Eo70EBERSYipylt/fwZkUVFRtWM6fPgwnJyc4OnpiTfffBN//vnwXmZJSUlwcHDQeSpDv379YGZmhlOnToltevbsCbn84QTtoKAgpKWl4d69e1WOg0kPERERVeDq6gqlUilu0dHR1bpO//79sWHDBhw4cAAfffQRjhw5ggEDBog3683KyoKTk5NOn0aNGsHR0RFZWVliG2dnZ5025fvlbaqC5S0iIiIJMVV568aNGzpL1qv7eKTRo0eLX3t7e8PHxwetW7fG4cOH0bdv32rHWR0c6SEiIpIQATDygaNlFAqFzmaqZ0J6eHigadOmuHKl7E72KpWqwoO5S0tLcffuXXEekEqlQnZ2tk6b8n19c4Uqw6SHiIhIQur6HZlv3ryJP//8Ey4uLgCAgIAA3L9/H8nJyWKbgwcPQqvVwt/fX2xz9OhRlJSUiG0SExPh6emJxo0bV/m1mfQQERFRteXl5SElJQUpKSkAgKtXryIlJQUZGRnIy8vDzJkz8eOPP+LatWs4cOAAXn75ZbRp0wZBQUEAgPbt26N///6YPHky/vOf/+DEiRMICwvD6NGjoVarAZQ9G1Mul2PSpElITU3Fxo0bsXLlSkyfPt2gWDmnh4iISEKe9pL1M2fOoE+fPuJ+eSIyYcIEfPHFFzh37hy++eYb3L9/H2q1GoGBgVi4cKFOuSw2NhZhYWHo27cvzMzMMHz4cKxatUo8r1QqsW/fPoSGhsLPzw9NmzbFvHnzDFquDjDpISIikpSnnfT07t0bj3uM5969e594DUdHR8TFxT22jY+PD44dO2ZQbH/H8hYRERE1CBzpISIikpCnPdJTnzDpISIikhBBkEEwInExpm9dx/IWERERNQgc6SEiIpKQ8psMGtNfqpj0EBERSQjn9OjH8hYRERE1CBzpISIikhBOZNaPSQ8REZGEsLylH5MeIiIiCeFIj36c00NEREQNAkd6iIiIJEQwsrwl5ZEeJj1EREQSIgB4zPM/q9RfqljeIiIiogaBIz1EREQSooUMMt6RuVJMeoiIiCSEq7f0Y3mLiIiIGgSO9BAREUmIVpBBxpsTVopJDxERkYQIgpGrtyS8fIvlLSIiImoQONJDREQkIZzIrB+THiIiIglh0qMfkx4iIiIJ4URm/Tinh4iIiBoEjvQQERFJCFdv6cekh4iISELKkh5j5vSYMJg6huUtIiIiahA40kNERCQhXL2lH5MeIiIiCRH+uxnTX6pY3iIiIqIGgSM9REREEsLyln5MeoiIiKSE9S29mPQQERFJiZEjPZDwSA/n9BAREVG1HT16FEOGDIFarYZMJsPWrVvFcyUlJZg1axa8vb1ha2sLtVqNkJAQ3Lp1S+ca7u7ukMlkOtvixYt12pw7dw49evSAlZUVXF1dsWTJEoNjZdJDREQkIeV3ZDZmM0R+fj58fX3x+eefVzhXUFCAn376CXPnzsVPP/2EzZs3Iy0tDS+99FKFtgsWLEBmZqa4vfXWW+K53NxcBAYGws3NDcnJyVi6dCkiIyPx1VdfGRQry1tEREQS8rQnMg8YMAADBgyo9JxSqURiYqLOsc8++wz/8z//g4yMDLRs2VI8bm9vD5VKVel1YmNjUVxcjHXr1kEul6NDhw5ISUnB8uXLMWXKlCrHypEeIiIiqiA3N1dnKyoqMsl1c3JyIJPJ4ODgoHN88eLFaNKkCTp37oylS5eitLRUPJeUlISePXtCLpeLx4KCgpCWloZ79+5V+bU50kNERCQlgsy4ycj/7evq6qpzeP78+YiMjDQiMKCwsBCzZs3CmDFjoFAoxONvv/02unTpAkdHR5w8eRKzZ89GZmYmli9fDgDIyspCq1atdK7l7OwsnmvcuHGVXp9JDxERkYSY6inrN27c0ElMLC0tjYqrpKQEI0eOhCAI+OKLL3TOTZ8+Xfzax8cHcrkc//jHPxAdHW306z6K5S0iIiKqQKFQ6GzGJB/lCc/169eRmJiok0xVxt/fH6Wlpbh27RoAQKVSITs7W6dN+b6+eUCVYdJDREQkJYIJNhMqT3guX76M/fv3o0mTJk/sk5KSAjMzMzg5OQEAAgICcPToUZSUlIhtEhMT4enpWeXSFsDyFhERkaQ87dVbeXl5uHLlirh/9epVpKSkwNHRES4uLnj11Vfx008/ISEhARqNBllZWQAAR0dHyOVyJCUl4dSpU+jTpw/s7e2RlJSE8PBwjBs3Tkxoxo4di6ioKEyaNAmzZs3ChQsXsHLlSqxYscKgWKuU9Gzfvr3KF6xs7T0RERFJ05kzZ9CnTx9xv3x+zoQJExAZGSnmEJ06ddLpd+jQIfTu3RuWlpaIj49HZGQkioqK0KpVK4SHh+vM81Eqldi3bx9CQ0Ph5+eHpk2bYt68eQYtVweqmPQMHTq0SheTyWTQaDQGBUBEREQm9hSfn9W7d28Ij5k5/bhzANClSxf8+OOPT3wdHx8fHDt2zOD4HlWlpEer1Rr1IkRERPR08Cnr+hk1kbmwsNBUcRAREZEp1LGJzHWJwUmPRqPBwoUL0bx5c9jZ2eG3334DAMydOxdff/21yQMkIiIiMgWDk54PP/wQMTExWLJkic7toDt27Ii1a9eaNDgiIiIylMwEmzQZnPRs2LABX331FYKDg2Fubi4e9/X1xS+//GLS4IiIiMhALG/pZXDS8/vvv6NNmzYVjmu1Wp2bBhERERHVJQYnPV5eXpUuGfu///s/dO7c2SRBERERUTVxpEcvg+/IPG/ePEyYMAG///47tFotNm/ejLS0NGzYsAEJCQk1ESMRERFVlYmesi5FBo/0vPzyy9ixYwf2798PW1tbzJs3D5cuXcKOHTvw4osv1kSMREREREar1rO3evTogcTERFPHQkREREYShLLNmP5SVe0Hjp45cwaXLl0CUDbPx8/Pz2RBERERUTUZOy+HSc9DN2/exJgxY3DixAk4ODgAAO7fv4/u3bsjPj4eLVq0MHWMREREREYzeE7PG2+8gZKSEly6dAl3797F3bt3cenSJWi1Wrzxxhs1ESMRERFVVflEZmM2iTJ4pOfIkSM4efIkPD09xWOenp749NNP0aNHD5MGR0RERIaRCWWbMf2lyuCkx9XVtdKbEGo0GqjVapMERURERNXEOT16GVzeWrp0Kd566y2cOXNGPHbmzBm88847+Pjjj00aHBEREZGpVGmkp3HjxpDJHtb48vPz4e/vj0aNyrqXlpaiUaNGmDhxIoYOHVojgRIREVEV8OaEelUp6fnkk09qOAwiIiIyCZa39KpS0jNhwoSajoOIiIioRlX75oQAUFhYiOLiYp1jCoXCqICIiIjICBzp0cvgicz5+fkICwuDk5MTbG1t0bhxY52NiIiIahGfsq6XwUnPu+++i4MHD+KLL76ApaUl1q5di6ioKKjVamzYsKEmYiQiIiIymsHlrR07dmDDhg3o3bs3Xn/9dfTo0QNt2rSBm5sbYmNjERwcXBNxEhERUVVw9ZZeBo/03L17Fx4eHgDK5u/cvXsXAPD888/j6NGjpo2OiIiIDFJ+R2ZjNqkyOOnx8PDA1atXAQDt2rXD999/D6BsBKj8AaREREREdY3BSc/rr7+Os2fPAgDee+89fP7557CyskJ4eDhmzpxp8gCJiIjIAJzIrJfBc3rCw8PFr/v164dffvkFycnJaNOmDXx8fEwaHBEREZGpGHWfHgBwc3ODm5ubKWIhIiIiI8lg5FPWTRZJ3VOlpGfVqlVVvuDbb79d7WCIiIiIakqVkp4VK1ZU6WIymYxJTw16pa03GsksajsMohox/NLt2g6BqMb8lVeKw88+pRfjknW9qpT0lK/WIiIiojqOj6HQy+DVW0RERET1kdETmYmIiKgO4UiPXhzpISIikpCnfUfmo0ePYsiQIVCr1ZDJZNi6davOeUEQMG/ePLi4uMDa2hr9+vXD5cuXddrcvXsXwcHBUCgUcHBwwKRJk5CXl6fT5ty5c+jRowesrKzg6uqKJUuWGPy9YdJDRERE1Zafnw9fX198/vnnlZ5fsmQJVq1ahdWrV+PUqVOwtbVFUFAQCgsLxTbBwcFITU1FYmIiEhIScPToUUyZMkU8n5ubi8DAQLi5uSE5ORlLly5FZGQkvvrqK4NiZXmLiIhISp5yeWvAgAEYMGBA5ZcSBHzyySeYM2cOXn75ZQDAhg0b4OzsjK1bt2L06NG4dOkS9uzZg9OnT+PZZ8uWuH366acYOHAgPv74Y6jVasTGxqK4uBjr1q2DXC5Hhw4dkJKSguXLl+skR09SrZGeY8eOYdy4cQgICMDvv/8OAPj2229x/Pjx6lyOiIiITMVEj6HIzc3V2YqKigwO5erVq8jKykK/fv3EY0qlEv7+/khKSgIAJCUlwcHBQUx4gLInPpiZmeHUqVNim549e0Iul4ttgoKCkJaWhnv37lU5HoOTnk2bNiEoKAjW1tb4+eefxW9CTk4OFi1aZOjliIiIqA5ydXWFUqkUt+joaIOvkZWVBQBwdnbWOe7s7Cyey8rKgpOTk875Ro0awdHRUadNZdd49DWqwuDy1gcffIDVq1cjJCQE8fHx4vHnnnsOH3zwgaGXIyIiIhOqzmTkv/cHgBs3bkChUIjHLS0tjYys9hk80pOWloaePXtWOK5UKnH//n1TxERERETVVX5HZmM2AAqFQmerTtKjUqkAANnZ2TrHs7OzxXMqlQq3b+vekb20tBR3797VaVPZNR59jaowOOlRqVS4cuVKhePHjx+Hh4eHoZcjIiIiUzLRnB5TaNWqFVQqFQ4cOCAey83NxalTpxAQEAAACAgIwP3795GcnCy2OXjwILRaLfz9/cU2R48eRUlJidgmMTERnp6eaNy4cZXjMTjpmTx5Mt555x2cOnUKMpkMt27dQmxsLCIiIvDmm28aejkiIiKqx/Ly8pCSkoKUlBQAZZOXU1JSkJGRAZlMhmnTpuGDDz7A9u3bcf78eYSEhECtVmPo0KEAgPbt26N///6YPHky/vOf/+DEiRMICwvD6NGjoVarAQBjx46FXC7HpEmTkJqaio0bN2LlypWYPn26QbEaPKfnvffeg1arRd++fVFQUICePXvC0tISEREReOuttwy9HBEREZmQqeb0VNWZM2fQp08fcb88EZkwYQJiYmLw7rvvIj8/H1OmTMH9+/fx/PPPY8+ePbCyshL7xMbGIiwsDH379oWZmRmGDx+OVatWieeVSiX27duH0NBQ+Pn5oWnTppg3b55By9XL3psgVOtbU1xcjCtXriAvLw9eXl6ws7OrzmWoCnJzc6FUKtEbL/Mp6yRZfMo6SdlfeaWY/uxJ5OTk6EwONqXy3xUe8xbB7JGEwlDawkL8tuBfNRprban2zQnlcjm8vLxMGQsRERFRjTE46enTpw9kMpne8wcPHjQqICIiIjKCkeUtKT9w1OCkp1OnTjr7JSUlSElJwYULFzBhwgRTxUVERETVwaes62Vw0rNixYpKj0dGRlZ4IioRERFRXWGyp6yPGzcO69atM9XliIiIqDrq0H166hqTPWU9KSlJZ/kZERERPX1Pe8l6fWJw0jNs2DCdfUEQkJmZiTNnzmDu3LkmC4yIiIjIlAxOepRKpc6+mZkZPD09sWDBAgQGBposMCIiIiJTMijp0Wg0eP311+Ht7W3Qsy6IiIjoKeHqLb0Mmshsbm6OwMBAPk2diIiojiqf02PMJlUGr97q2LEjfvvtt5qIhYiIiKjGGJz0fPDBB4iIiEBCQgIyMzORm5ursxEREVEt43L1SlV5Ts+CBQswY8YMDBw4EADw0ksv6TyOQhAEyGQyaDQa00dJREREVcM5PXpVOemJiorC1KlTcejQoZqMh4iIiKhGVDnpEYSy1K9Xr141FgwREREZhzcn1M+gJeuPe7o6ERER1QEsb+llUNLTtm3bJyY+d+/eNSogIiIioppgUNITFRVV4Y7MREREVHewvKWfQUnP6NGj4eTkVFOxEBERkbFY3tKryvfp4XweIiIiqs8MXr1FREREdRhHevSqctKj1WprMg4iIiIyAc7p0c+gOT1ERERUx3GkRy+Dn71FREREVB9xpIeIiEhKONKjF5MeIiIiCeGcHv1Y3iIiIqIGgSM9REREUsLyll5MeoiIiCSE5S39WN4iIiKiBoEjPURERFLC8pZeTHqIiIikhEmPXixvERERUYPAkR4iIiIJkf13M6a/VHGkh4iISEoEE2wGcHd3h0wmq7CFhoYCAHr37l3h3NSpU3WukZGRgUGDBsHGxgZOTk6YOXMmSktLq/sd0IsjPURERBLytJesnz59GhqNRty/cOECXnzxRYwYMUI8NnnyZCxYsEDct7GxEb/WaDQYNGgQVCoVTp48iczMTISEhMDCwgKLFi2q/hupBJMeIiIiqiA3N1dn39LSEpaWlhXaNWvWTGd/8eLFaN26NXr16iUes7GxgUqlqvR19u3bh4sXL2L//v1wdnZGp06dsHDhQsyaNQuRkZGQy+UmeDdlWN4iIiKSEhOVt1xdXaFUKsUtOjr6iS9dXFyM//3f/8XEiRMhkz2cHRQbG4umTZuiY8eOmD17NgoKCsRzSUlJ8Pb2hrOzs3gsKCgIubm5SE1Nrf73oRIc6SEiIpIaEyw7v3HjBhQKhbhf2SjP323duhX379/Ha6+9Jh4bO3Ys3NzcoFarce7cOcyaNQtpaWnYvHkzACArK0sn4QEg7mdlZRn/Rh7BpIeIiIgqUCgUOklPVXz99dcYMGAA1Gq1eGzKlCni197e3nBxcUHfvn2Rnp6O1q1bmyzeqmB5i4iISELKJzIbs1XH9evXsX//frzxxhuPbefv7w8AuHLlCgBApVIhOztbp035vr55QNXFpIeIiEhKnvKS9XLr16+Hk5MTBg0a9Nh2KSkpAAAXFxcAQEBAAM6fP4/bt2+LbRITE6FQKODl5VW9YPRgeYuIiIiMotVqsX79ekyYMAGNGj1MLdLT0xEXF4eBAweiSZMmOHfuHMLDw9GzZ0/4+PgAAAIDA+Hl5YXx48djyZIlyMrKwpw5cxAaGlqleUSGYNJDREQkIU/7Pj0AsH//fmRkZGDixIk6x+VyOfbv349PPvkE+fn5cHV1xfDhwzFnzhyxjbm5ORISEvDmm28iICAAtra2mDBhgs59fUyFSQ8REZGU1MIDRwMDAyEIFTu6urriyJEjT+zv5uaGXbt2Gf7CBuKcHiIiImoQONJDREQkIbVR3qovmPQQERFJSS2Ut+oLJj1ERERSwqRHL87pISIiogaBIz1EREQSwjk9+jHpISIikhKWt/RieYuIiIgaBI70EBERSYhMECCr5EaBhvSXKiY9REREUsLyll4sbxEREVGDwJEeIiIiCeHqLf2Y9BAREUkJy1t6sbxFREREDQJHeoiIiCSE5S39mPQQERFJCctbejHpISIikhCO9OjHOT1ERETUIHCkh4iISEpY3tKLSQ8REZHESLlEZQyWt4iIiKhB4EgPERGRlAhC2WZMf4li0kNERCQhXL2lH8tbRERE1CBwpIeIiEhKuHpLLyY9REREEiLTlm3G9JcqlreIiIioQeBID9EjOvrnYcQ/7+AZ7wI0UZUicqI7kvYoazssogrunLbAr+tscD+1EQrvmKPbp/fRvF+xeH5Te6dK+3WMyIPnpAJxP/OwHJe+sEVOWiOYWwpo2rUE3T/LqdCv6J4MB15xxF/Z5hhy6g7kCgnXQOo7lrf0kmTS07t3b3Tq1AmffPJJbYdC9YyVjRa/pVph73eOmL/uWm2HQ6SX5i8ZHDxL4T7sL/z4tkOF84OO/qGzn3VMjuQ59mgeWCge+32fJZLn2aPjtDw08y+BoAFyL1f+ayF5rgKKtqX4K9vcpO+DTI+rt/STZNJTVzEZq/vOHFLgzCFFbYdB9ESqnsVQ9SzWe96qme7EjFsHLdHMvwR2rmXHtaXA2UV28I7IQ6tXHyZCijaaCtdK/84aJbkytP9nPrKPWZroHVCN4X169Kp3c3qKi/X/kBMRUUWFf8iQdUQO9+F/icfuX2yEv7LNITMD9g9rjJ09muD4FCVyftUdycm9Yo5f/m2DrotzIat3vzGIdNX5j3Dv3r0RFhaGadOmoWnTpggKCsKFCxcwYMAA2NnZwdnZGePHj8cff/yh9xpFRUWIiIhA8+bNYWtrC39/fxw+fBgAkJubC2tra+zevVunz5YtW2Bvb4+CgrLa96xZs9C2bVvY2NjAw8MDc+fORUlJidg+MjISnTp1wrfffgt3d3colUqMHj0aDx48AAC89tprOHLkCFauXAmZTAaZTIZr167pjTc3N1dnIyKqrutbrdHIVkDzF4vEY/k3ypKbS5/Zov3UAnRfnQO5QsDRCY1RfF8GANAUA/+JUMB7Zh5s1BJe0iMx5eUtYzapqvNJDwB88803kMvlOHHiBBYvXowXXngBnTt3xpkzZ7Bnzx5kZ2dj5MiRevuHhYUhKSkJ8fHxOHfuHEaMGIH+/fvj8uXLUCgUGDx4MOLi4nT6xMbGYujQobCxsQEA2NvbIyYmBhcvXsTKlSuxZs0arFixQqdPeno6tm7dioSEBCQkJODIkSNYvHgxAGDlypUICAjA5MmTkZmZiczMTLi6ulYab3R0NJRKpbjpa0dEVBXXNluh5eBCmD9SmSqvYHhOzUfzwCI07lAKv0W5gAy4ubes4YXldrD30KDlS0WVXJXqLMEEm0TVi6TnmWeewZIlS+Dp6YnExER07twZixYtQrt27dC5c2esW7cOhw4dwq+//lqhb0ZGBtavX48ffvgBPXr0QOvWrREREYHnn38e69evBwAEBwdj69at4qhObm4udu7cieDgYPE6c+bMQffu3eHu7o4hQ4YgIiIC33//vc5rabVaxMTEoGPHjujRowfGjx+PAwcOAACUSiXkcjlsbGygUqmgUqlgbl75hMDZs2cjJydH3G7cuGGS7yMRNTx/nLFA3tVGcH9k3g7wcM6PovXDOTzmcsDWVYOCzLJ/m+6cssDNvZbY3LEZNndshqOvOwAAEro3xcVPbZ/OG6A6LzIyUqxglG/t2rUTzxcWFiI0NBRNmjSBnZ0dhg8fjuzsbJ1rZGRkYNCgQbCxsYGTkxNmzpyJ0tJSk8daLyYy+/n5iV+fPXsWhw4dgp2dXYV26enpaNu2rc6x8+fPQ6PRVDheVFSEJk2aAAAGDhwICwsLbN++HaNHj8amTZugUCjQr18/sf3GjRuxatUqpKenIy8vD6WlpVAodCe8uru7w97eXtx3cXHB7du3DX6/lpaWsLTkZEEiMt61TVZw6FACh3a6v0AadyiFmVzAg6vmaOpXVqrXlgAFv5vDRl2WCHVbmQvNI7nSvQsWSH5fgV7f3oNty4oTnqluqI3VWx06dMD+/fvF/UaNHqYX4eHh2LlzJ3744QcolUqEhYVh2LBhOHHiBABAo9Fg0KBBUKlUOHnyJDIzMxESEgILCwssWrSo+m+kEvUi6bG1ffgXRV5eHoYMGYKPPvqoQjsXF5cKx/Ly8mBubo7k5OQKIyvliZNcLserr76KuLg4jB49GnFxcRg1apT4Py0pKQnBwcGIiopCUFAQlEol4uPjsWzZMp3rWVhY6OzLZDJotayD1ydWNhqoWz2cLK9yLYZHh7/w4L457vwur8XIiHSV5suQl/Hw37SCm+a4f6kR5EqtOP+mJE+Gm3ut4PPugwr9LewEeIz6C5c+s4WNixY2ag1+/bqsnN8iqKycZfe3xKb4fllxwL61hvfpqctMtHrr7/NJH/cHeaNGjaBSqSocz8nJwddff424uDi88MILAID169ejffv2+PHHH9GtWzfs27cPFy9exP79++Hs7IxOnTph4cKFmDVrFiIjIyGXm+7f3nqR9DyqS5cu2LRpE9zd3XUySX06d+4MjUaD27dvo0ePHnrbBQcH48UXX0RqaioOHjyIDz74QDx38uRJuLm54f333xePXb9+3eDY5XI5NBr+dVSXtfX9C0s3pYv7U6NuAQD2bWyMZeEtayssogrupTbC0QmNxf1zH5WNMrsN/QvPRpclOTd2WQIC4Dqo8jk53jPzIGsEnJ6lgKYQcPQpRc/19yBXMqEhVJhPOn/+fERGRlba9vLly1Cr1bCyskJAQACio6PRsmVLJCcno6SkRKdy0q5dO7Rs2RJJSUno1q0bkpKS4O3tDWdnZ7FNUFAQ3nzzTaSmpqJz584me0/1LukJDQ3FmjVrMGbMGLz77rtwdHTElStXEB8fj7Vr11YYzWnbti2Cg4MREhKCZcuWoXPnzrhz5w4OHDgAHx8fDBo0CADQs2dPqFQqBAcHo1WrVvD39xev8cwzzyAjIwPx8fHo2rUrdu7ciS1bthgcu7u7O06dOoVr167Bzs4Ojo6OMDOrF9OqGoxzSXYIUvvWdhhET9Tsf0ow/NLjy+ceIwvhMbJQ73kzC8Dn3Tz4vJtnstek2meq8taNGzd0pnHoG+Xx9/dHTEwMPD09kZmZiaioKPTo0QMXLlxAVlYW5HI5HBwcdPo4OzsjKysLAJCVlaWT8JSfLz9nSvXuN65arcaJEyeg0WgQGBgIb29vTJs2DQ4ODnoTiPXr1yMkJAQzZsyAp6cnhg4ditOnT6Nly4d/uctkMowZMwZnz57VmcAMAC+99BLCw8MRFhaGTp064eTJk5g7d67BsUdERMDc3BxeXl5o1qwZMjIyDL4GERHRY5lo9ZZCodDZ9CU9AwYMwIgRI+Dj44OgoCDs2rUL9+/fr7DYpy6QCYKEb70oEbm5uVAqleiNl9FIZvHkDkT1EEcQSMr+yivF9GdPIicnp8IiGFMp/10R0H8BGllYVfs6pSWFSNozz6hYu3btin79+uHFF19E3759ce/ePZ3RHjc3N0ybNg3h4eGYN28etm/fjpSUFPH81atX4eHhgZ9++smk5a16N9JDRERE+tX2zQnz8vKQnp4OFxcX+Pn5wcLCQrx9CwCkpaUhIyMDAQEBAICAgACcP39eZ7VzYmIiFAoFvLy8jAvmb+rdnB4iIiJ6DK1QthnT3wAREREYMmQI3NzccOvWLcyfPx/m5uYYM2YMlEolJk2ahOnTp8PR0REKhQJvvfUWAgIC0K1bNwBAYGAgvLy8MH78eCxZsgRZWVmYM2cOQkNDTX77FiY9REREUmLsXZUN7Hvz5k2MGTMGf/75J5o1a4bnn38eP/74I5o1awYAWLFiBczMzDB8+HAUFRUhKCgI//73v8X+5ubmSEhIwJtvvomAgADY2tpiwoQJWLBggRFvonJMeoiIiKja4uPjH3veysoKn3/+OT7//HO9bdzc3LBr1y5Th1YBkx4iIiIJkcHIJesmi6TuYdJDREQkJSa6I7MUcfUWERERNQgc6SEiIpKQ2njgaH3BpIeIiEhKnvLqrfqE5S0iIiJqEDjSQ0REJCEyQYDMiMnIxvSt65j0EBERSYn2v5sx/SWK5S0iIiJqEDjSQ0REJCEsb+nHpIeIiEhKuHpLLyY9REREUsI7MuvFOT1ERETUIHCkh4iISEJ4R2b9mPQQERFJCctberG8RURERA0CR3qIiIgkRKYt24zpL1VMeoiIiKSE5S29WN4iIiKiBoEjPURERFLCmxPqxaSHiIhIQvgYCv1Y3iIiIqIGgSM9REREUsKJzHox6SEiIpISAYAxy86lm/Mw6SEiIpISzunRj3N6iIiIqEHgSA8REZGUCDByTo/JIqlzmPQQERFJCScy68XyFhERETUIHOkhIiKSEi0AmZH9JYpJDxERkYRw9ZZ+LG8RERFRg8CRHiIiIinhRGa9ONJDREQkJeVJjzGbAaKjo9G1a1fY29vDyckJQ4cORVpamk6b3r17QyaT6WxTp07VaZORkYFBgwbBxsYGTk5OmDlzJkpLS43+djyKIz1ERERUbUeOHEFoaCi6du2K0tJS/Otf/0JgYCAuXrwIW1tbsd3kyZOxYMECcd/Gxkb8WqPRYNCgQVCpVDh58iQyMzMREhICCwsLLFq0yGSxMukhIiKSkqdc3tqzZ4/OfkxMDJycnJCcnIyePXuKx21sbKBSqSq9xr59+3Dx4kXs378fzs7O6NSpExYuXIhZs2YhMjIScrnc8PdRCZa3iIiIpERrgg1Abm6uzlZUVFSll8/JyQEAODo66hyPjY1F06ZN0bFjR8yePRsFBQXiuaSkJHh7e8PZ2Vk8FhQUhNzcXKSmphr4DdCPIz1EREQSYqol666urjrH58+fj8jIyMf21Wq1mDZtGp577jl07NhRPD527Fi4ublBrVbj3LlzmDVrFtLS0rB582YAQFZWlk7CA0Dcz8rKqvZ7+TsmPURERFTBjRs3oFAoxH1LS8sn9gkNDcWFCxdw/PhxneNTpkwRv/b29oaLiwv69u2L9PR0tG7d2nRBPwHLW0RERFJiotVbCoVCZ3tS0hMWFoaEhAQcOnQILVq0eGxbf39/AMCVK1cAACqVCtnZ2Tptyvf1zQOqDiY9REREUqIVjN8MIAgCwsLCsGXLFhw8eBCtWrV6Yp+UlBQAgIuLCwAgICAA58+fx+3bt8U2iYmJUCgU8PLyMiiex2F5i4iIiKotNDQUcXFx2LZtG+zt7cU5OEqlEtbW1khPT0dcXBwGDhyIJk2a4Ny5cwgPD0fPnj3h4+MDAAgMDISXlxfGjx+PJUuWICsrC3PmzEFoaGiVympVxZEeIiIiKXnKNyf84osvkJOTg969e8PFxUXcNm7cCACQy+XYv38/AgMD0a5dO8yYMQPDhw/Hjh07xGuYm5sjISEB5ubmCAgIwLhx4xASEqJzXx9T4EgPERGRpBh5nx4YXt56HFdXVxw5cuSJ13Fzc8OuXbsMem1DcaSHiIiIGgSO9BAREUkJHziqF5MeIiIiKdEKMLREVbG/NLG8RURERA0CR3qIiIikRNCWbcb0lygmPURERFLCOT16MekhIiKSEs7p0YtzeoiIiKhB4EgPERGRlLC8pReTHiIiIikRYGTSY7JI6hyWt4iIiKhB4EgPERGRlLC8pReTHiIiIinRagEYca8drXTv08PyFhERETUIHOkhIiKSEpa39GLSQ0REJCVMevRieYuIiIgaBI70EBERSQkfQ6EXkx4iIiIJEQQtBCOelG5M37qOSQ8REZGUCIJxozWc00NERERUv3Gkh4iISEoEI+f0SHikh0kPERGRlGi1gMyIeTkSntPD8hYRERE1CBzpISIikhKWt/Ri0kNERCQhglYLwYjylpSXrLO8RURERA0CR3qIiIikhOUtvZj0EBERSYlWAGRMeirD8hYRERE1CBzpISIikhJBAGDMfXqkO9LDpIeIiEhCBK0AwYjylsCkh4iIiOoFQQvjRnq4ZJ2IiIioXuNIDxERkYSwvKUfkx4iIiIpYXlLLyY99UB51l2KEqPuN0VUl/2VV1rbIRDVmML/fr6fxiiKsb8rSlFiumDqGCY99cCDBw8AAMexq5YjIao5h5+t7QiIat6DBw+gVCpr5NpyuRwqlQrHs4z/XaFSqSCXy00QVd0iE6RcvJMIrVaLW7duwd7eHjKZrLbDaRByc3Ph6uqKGzduQKFQ1HY4RCbFz/fTJwgCHjx4ALVaDTOzmltDVFhYiOLiYqOvI5fLYWVlZYKI6haO9NQDZmZmaNGiRW2H0SApFAr+UiDJ4uf76aqpEZ5HWVlZSTJZMRUuWSciIqIGgUkPERERNQhMeogqYWlpifnz58PS0rK2QyEyOX6+qaHiRGYiIiJqEDjSQ0RERA0Ckx4iIiJqEJj0EBERUYPApIfqpN69e2PatGkmv25MTAwcHBxMfl2i2lJTPytEUsSkh+qkzZs3Y+HChbUdxlPDZIykhskY1UVMeqhOcnR0hL29faXnKrvFukajgVYr3ScDU8NkiscJENFDTHqoTnr0r0R3d3csXLgQISEhUCgUmDJlijgysn37dnh5ecHS0hIZGRkoKipCREQEmjdvDltbW/j7++Pw4cOPfa1t27ahS5cusLKygoeHB6KiolBaWvZE5LFjx2LUqFE67UtKStC0aVNs2LABALBnzx48//zzcHBwQJMmTTB48GCkp6eL7a9duwaZTIbNmzejT58+sLGxga+vL5KSkgAAhw8fxuuvv46cnBzIZDLIZDJERkaa5htJ9Urv3r0RFhaGadOmoWnTpggKCsKFCxcwYMAA2NnZwdnZGePHj8cff/yh9xqP+xnIzc2FtbU1du/erdNny5YtsLe3R0FBAQBg1qxZaNu2LWxsbODh4YG5c+eipOThk7cjIyPRqVMnfPvtt3B3d4dSqcTo0aPFhyO/9tprOHLkCFauXCl+pq9du2babxZRNTDpoXrh448/hq+vL37++WfMnTsXAFBQUICPPvoIa9euRWpqKpycnBAWFoakpCTEx8fj3LlzGDFiBPr374/Lly9Xet1jx44hJCQE77zzDi5evIgvv/wSMTEx+PDDDwEAwcHB2LFjB/Ly8sQ+e/fuRUFBAV555RUAQH5+PqZPn44zZ87gwIEDMDMzwyuvvFJh5On9999HREQEUlJS0LZtW4wZMwalpaXo3r07PvnkEygUCmRmZiIzMxMRERE18W2keuCbb76BXC7HiRMnsHjxYrzwwgvo3Lkzzpw5gz179iA7OxsjR47U2/9xPwMKhQKDBw9GXFycTp/Y2FgMHToUNjY2AAB7e3vExMTg4sWLWLlyJdasWYMVK1bo9ElPT8fWrVuRkJCAhIQEHDlyBIsXLwYArFy5EgEBAZg8ebL4mXZ1dTXxd4qoGgSiOqhXr17CO++8IwiCILi5uQlDhw7VOb9+/XoBgJCSkiIeu379umBubi78/vvvOm379u0rzJ49W+ynVCp1zi1atEin/bfffiu4uLgIgiAIJSUlQtOmTYUNGzaI58eMGSOMGjVKb+x37twRAAjnz58XBEEQrl69KgAQ1q5dK7ZJTU0VAAiXLl2qNC5qmHr16iV07txZ3F+4cKEQGBio0+bGjRsCACEtLU3sU/6zUpWfgS1btgh2dnZCfn6+IAiCkJOTI1hZWQm7d+/WG9fSpUsFPz8/cX/+/PmCjY2NkJubKx6bOXOm4O/vr/NeyuMiqiv4lHWqF5599tkKx+RyOXx8fMT98+fPQ6PRoG3btjrtioqK0KRJk0qve/bsWZw4cUIc2QHK5gcVFhaioKAANjY2GDlyJGJjYzF+/Hjk5+dj27ZtiI+PF9tfvnwZ8+bNw6lTp/DHH3+IIzwZGRno2LGj2O7RWF1cXAAAt2/fRrt27Qz5VpDE+fn5iV+fPXsWhw4dgp2dXYV26enpFT7rVfkZGDhwICwsLLB9+3aMHj0amzZtgkKhQL9+/cT2GzduxKpVq5Ceno68vDyUlpZWeBq7u7u7zrw7FxcX3L59u/pvnOgpYNJD9YKtrW2FY9bW1pDJZOJ+Xl4ezM3NkZycDHNzc522lf3SKO8TFRWFYcOGVThnZWUFoKzE1atXL9y+fRuJiYmwtrZG//79xXZDhgyBm5sb1qxZA7VaDa1Wi44dO1aYhGphYSF+XR43J1/T3z36Wc/Ly8OQIUPw0UcfVWhXnjg/qio/A3K5HK+++iri4uIwevRoxMXFYdSoUWjUqOzXQVJSEoKDgxEVFYWgoCAolUrEx8dj2bJlOtd79PMMlH2m+Xmmuo5JD0lG586dodFocPv2bfTo0aNKfbp06YK0tDS0adNGb5vu3bvD1dUVGzduxO7duzFixAjxH/w///wTaWlpWLNmjfiax48fNzh2uVwOjUZjcD+Sti5dumDTpk1wd3cXk5LHqerPQHBwMF588UWkpqbi4MGD+OCDD8RzJ0+ehJubG95//33x2PXr1w2OnZ9pqos4kZkko23btggODkZISAg2b96Mq1ev4j//+Q+io6Oxc+fOSvvMmzcPGzZsQFRUFFJTU3Hp0iXEx8djzpw5Ou3Gjh2L1atXIzExEcHBweLxxo0bo0mTJvjqq69w5coVHDx4ENOnTzc4dnd3d+Tl5eHAgQP4448/xFU01LCFhobi7t27GDNmDE6fPo309HTs3bsXr7/+eqUJRVV/Bnr27AmVSoXg4GC0atUK/v7+4rlnnnkGGRkZiI+PR3p6OlatWoUtW7YYHLu7uztOnTqFa9eu6ZR9iWoTkx6SlPXr1yMkJAQzZsyAp6cnhg4ditOnT6Nly5aVtg8KCkJCQgL27duHrl27olu3blixYgXc3Nx02gUHB+PixYto3rw5nnvuOfG4mZkZ4uPjkZycjI4dOyI8PBxLly41OO7u3btj6tSpGDVqFJo1a4YlS5YYfA2SHrVajRMnTkCj0SAwMBDe3t6YNm0aHBwcYGZW+T/fVfkZkMlkGDNmDM6ePauTxAPASy+9hPDwcISFhaFTp044efKkuGLSEBERETA3N4eXlxeaNWuGjIwMg69BZGoyQRCE2g6CiIiIqKZxpIeIiIgaBCY9RERE1CAw6SEiIqIGgUkPERERNQhMeoiIiKhBYNJDREREDQKTHiIiImoQmPQQERFRg8Ckh4iq5LXXXsPQoUPF/d69e2PatGlPPY7Dhw9DJpPh/v37etvIZDJs3bq1yteMjIxEp06djIrr2rVrkMlkSElJMeo6RFRzmPQQ1WOvvfYaZDIZZDIZ5HI52rRpgwULFqC0tLTGX3vz5s1YuHBhldpWJVEhIqppfMo6UT3Xv39/rF+/HkVFRdi1axdCQ0NhYWGB2bNnV2hbXFwMuVxuktd1dHQ0yXWIiJ4WjvQQ1XOWlpZQqVRwc3PDm2++iX79+mH79u0AHpakPvzwQ6jVanh6egIAbty4gZEjR8LBwQGOjo54+eWXce3aNfGaGo0G06dPh4ODA5o0aYJ3330Xf39M39/LW0VFRZg1axZcXV1haWmJNm3a4Ouvv8a1a9fQp08fAGVPpZfJZHjttdcAAFqtFtHR0WjVqhWsra3h6+uL//u//9N5nV27dqFt27awtrZGnz59dOKsqlmzZqFt27awsbGBh4cH5s6di5KSkgrtvvzyS7i6usLGxgYjR45ETk6Ozvm1a9eiffv2sLKyQrt27fDvf//b4FiIqPYw6SGSGGtraxQXF4v7Bw4cQFpaGhITE5GQkICSkhIEBQXB3t4ex44dw4kTJ2BnZ4f+/fuL/ZYtW4aYmBisW7cOx48fx927d7Fly5bHvm5ISAi+++47rFq1CpcuXcKXX34JOzs7uLq6YtOmTQCAtLQ0ZGZmYuXKlQCA6OhobNiwAatXr0ZqairCw8Mxbtw4HDlyBEBZcjZs2DAMGTIEKSkpeOONN/Dee+8Z/D2xt7dHTEwMLl68iJUrV2LNmjVYsWKFTpsrV67g+++/x44dO7Bnzx78/PPP+Oc//ymej42Nxbx58/Dhhx/i0qVLWLRoEebOnYtvvvnG4HiIqJYIRFRvTZgwQXj55ZcFQRAErVYrJCYmCpaWlkJERIR43tnZWSgqKhL7fPvtt4Knp6eg1WrFY0VFRYK1tbWwd+9eQRAEwcXFRViyZIl4vqSkRGjRooX4WoIgCL169RLeeecdQRAEIS0tTQAgJCYmVhrnoUOHBADCvXv3xGOFhYWCjY2NcPLkSZ22kyZNEsaMGSMIgiDMnj1b8PLy0jk/a9asCtf6OwDCli1b9J5funSp4OfnJ+7Pnz9fMDc3F27evCke2717t2BmZiZkZmYKgiAIrVu3FuLi4nSus3DhQiEgIEAQBEG4evWqAED4+eef9b4uEdUuzukhqucSEhJgZ2eHkpISaLVajB07FpGRkeJ5b29vnXk8Z8+exZUrV2Bvb69zncLCQqSnpyMnJweZmZnw9/cXzzVq1AjPPvtshRJXuZSUFJibm6NXr15VjvvKlSsoKCjAiy++qHO8uLgYnTt3BgBcunRJJw4ACAgIqPJrlNu4cSNWrVqF9PR05OXlobS0FAqFQqdNy5Yt0bx5c53X0Wq1SEtLg729PdLT0zFp0iRMnjxZbFNaWgqlUmlwPERUO5j0ENVzffr0wRdffAG5XA61Wo1GjXR/rG1tbXX28/Ly4Ofnh9jY2ArXatasWbVisLa2NrhPXl4eAGDnzp06yQZQNk/JVJKSkhAcHIyoqCgEBQVBqVQiPj4ey5YtMzjWNWvWVEjCzM3NTRYrEdUsJj1E9ZytrS3atGlT5fZdunTBxo0b4eTkVGG0o5yLiwtOnTqFnj17Aigb0UhOTkaXLl0qbe/t7Q2tVosjR46gX79+Fc6XjzRpNBrxmJeXFywtLZGRkaF3hKh9+/bipOxyP/7445Pf5CNOnjwJNzc3vP/+++Kx69evV2iXkZGBW7duQa1Wi69jZmYGT09PODs7Q61W47fffkNwcLBBr09EdQcnMhM1MMHBwWjatClefvllHDt2DFevXsXhw4fx9ttv4+bNmwCAd955B4sXL8bWrVvxyy+/4J///Odj77Hj7u6OCRMmYOLEidi6dat4ze+//x4A4ObmBplMhoSEBNy5cwd5eXmwt7dHREQEwsPD8c033yA9PR0//fQTPv30U3Fy8NSpU3H58mXMnDkTaWlpiIuLQ0xMjEHv95lnnkFGRgbi4+ORnp6OVatWVTop28rKChMmTMDZs2dx7NgxvP322xg5ciRUKhUAICoqCtHR0Vi1ahV+/fVXnD9/HuvXr8fy5csNioeIag+THqIGxsbGBkePHkXLli0xbNgwtG/fHpMmTUJhYaE48jNjxgyMHz8eEyZMQEBAAOzt7fHKK6889rpffPEFXn31Vfzzn/9Eu3btMHnyZOTn5wMAmjdvjqioKLz33ntwdnZGWFgYAGDhwoWYO3cuoqOj0b59e/Tv3x87d+5Eq1atAJTNs9m0aRO2bt0KX19frF69GosWLTLo/b700ksIDw9HWFgYOnXqhJMnT2Lu3LkV2rVp0wbDhg3DwIEDERgYCB8fH50l6W+88QbWrl2L9evXw9vbG7169UJMTIwYKxHVfTJB38xEIiIiIgnhSA8RERE1CEx6iIiIqEFg0kNEREQNApMeIiIiahCY9BAREVGDwKSHiIiIGgQmPURERNQgMOkhIiKiBoFJDxERETUITHqIiIioQWDSQ0RERA3C/wOfU3mHHVjGtgAAAABJRU5ErkJggg==",
      "text/plain": [
       "<Figure size 640x480 with 2 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from sklearn.metrics import confusion_matrix, ConfusionMatrixDisplay, accuracy_score, f1_score, precision_score, recall_score\n",
    "\n",
    "print(f\"Accuracy: {accuracy_score(yTestVector, yPred)}\")\n",
    "print(f\"F1: {f1_score(yTestVector, yPred)}\")\n",
    "print(f\"Precision: {precision_score(yTestVector, yPred)}\")\n",
    "print(f\"Recall: {recall_score(yTestVector, yPred)}\")\n",
    "\n",
    "cm = confusion_matrix(yTestVector, yPred)\n",
    "disp = ConfusionMatrixDisplay(confusion_matrix=cm,display_labels=[\"irrelevant\",\"relevant\"])\n",
    "disp.plot()\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pickle,os\n",
    "\n",
    "if not os.path.exists(\"models/vectorizers\"):\n",
    "    os.makedirs(\"models/vectorizers\")\n",
    "\n",
    "with open(\"models/vectorizers/hashing_vectorizer.pkl\", \"wb\") as f:\n",
    "    pickle.dump(vectorizer, f)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/LogisticRegression.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Shark catch rates are higher in pelagic longline fisheries than in any other fishery, and sharks are\n",
    "typically discarded (bycatch) at sea. The post-release fate of discarded sharks is largely\n",
    "unobserved and could pose a significant source of unquantified mortality that may change stock\n",
    "assessment outcomes and prevent sound conservation and management advice. This study\n",
    "assessed post-release mortality rates of blue (Prionace glauca), bigeye thresher (Alopias\n",
    "superciliosus), oceanic whitetip (Carcharhinus longimanus), silky (C. falciformis) and shortfin\n",
    "mako (Isurus oxyrhincus) sharks discarded in the Hawaii deep-set and American Samoa longline\n",
    "fisheries targeting tuna in the central Pacific Ocean. The impacts on survival rates were\n",
    "examined considering species, fishery, fishing gear configuration, handling method, animal\n",
    "condition at capture and at release, and the amount of trailing fishing gear remaining on\n",
    "discarded sharks. Bayesian survival analysis showed that the condition at release (good vs.\n",
    "injured), branchline leader material, and the amount of trailing fishing gear left on the animals\n",
    "were among the factors that had the largest effect on post-release fate—animals captured on\n",
    "monofilament branchline leaders and released in good condition without trailing fishing gear had\n",
    "the highest rates of survival. This study shows that fisher behavior can have a significant impact\n",
    "on pelagic shark post-release mortality. Ensuring that sharks are handled carefully and released\n",
    "with minimal amounts of trailing fishing gear may reduce fishing mortality on shark populations.\"\"\"\n",
    "\n",
    "text2 = \"\"\"NumPy is an essential package for high-performance scientific computing and data analysis in the Python ecosystem. It is the foundation of many higher-level tools such as Pandas and scikit-learn.\n",
    "\n",
    "TensorFlow also uses NumPy arrays as the foundation for building Tensor objects and graph flow for deep learning tasks. These heavily rely on linear algebra operations on large lists, vectors, and matrices of numbers.\n",
    "\n",
    "NumPy is faster because it uses vectorized implementation, and many of its core functions are written in C.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vectorizer.transform(pd.Series([text]))\n",
    "t = torch.tensor(v.todense(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[8.3146e-04, 9.9857e-01]], device='cuda:0', grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-pwi3iMQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

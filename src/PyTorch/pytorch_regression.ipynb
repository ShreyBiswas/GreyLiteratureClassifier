{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.notebook import tqdm,trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 2500 entries, 4107 to 5998\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           2500 non-null   object\n",
      " 1   text          2500 non-null   object\n",
      " 2   relevance     2500 non-null   object\n",
      " 3   multiclasses  2500 non-null   object\n",
      "dtypes: object(4)\n",
      "memory usage: 97.7+ KB\n"
     ]
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/labelled/data.json\", group_relevant=True):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    if group_relevant:\n",
    "        data[\"class\"] = data[\"class\"].apply(\n",
    "            lambda x: \"relevant\" if x != \"irrelevant\" else x\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "data = import_labelled_data(\n",
    "    path=\"../../data/level-0.5/data.json\", group_relevant=False\n",
    ")\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.sample(2500)\n",
    "\n",
    "\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "xTrain, xTest, yTrain, yTest = train_test_split(data[\"text\"], data[\"relevance\"], test_size=0.2, random_state=42)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vectorizing data...\n"
     ]
    }
   ],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "\n",
    "vectorizer = CountVectorizer(\n",
    "    stop_words=\"english\"\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "10 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Vectorization took 10.03 seconds.\n",
      "Vectorization of the whole dataset is estimated to take 50.16 seconds.\n"
     ]
    }
   ],
   "source": [
    "time = %timeit -n1 -r1 -o vectorizer.fit(data['text'][:500])\n",
    "\n",
    "print(f\"Vectorization took {time.average:.2f} seconds.\")\n",
    "print(f\"Vectorization of the whole dataset is estimated to take {time.average * len(data) / 500:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "8.89 s ± 0 ns per loop (mean ± std. dev. of 1 run, 1 loop each)\n",
      "Vectorization took 8.89 seconds.\n",
      "Vectorization of the whole dataset is estimated to take 44.45 seconds.\n"
     ]
    }
   ],
   "source": [
    "time = %timeit -n1 -r1 -o vectorizer.transform(data['text'][:500])\n",
    "\n",
    "print(f\"Vectorization took {time.average:.2f} seconds.\")\n",
    "print(f\"Vectorization of the whole dataset is estimated to take {time.average * len(data) / 500:.2f} seconds.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fitting vectorizer...\n",
      "Vectorizer fitted.\n",
      "Vectorizing data...\n",
      "Training data vectorized.\n",
      "Testing data vectorized.\n"
     ]
    }
   ],
   "source": [
    "print(\"Fitting vectorizer...\")\n",
    "vectorizer.fit(data[\"text\"])\n",
    "print(\"Vectorizer fitted.\")\n",
    "\n",
    "\n",
    "print(\"Vectorizing data...\")\n",
    "xTrainVector = vectorizer.transform(xTrain)\n",
    "print(\"Training data vectorized.\")\n",
    "xTestVector = vectorizer.transform(xTest)\n",
    "print(\"Testing data vectorized.\")\n",
    "\n",
    "yTrainVector = yTrain.apply(lambda x: 1 if x == \"relevant\" else 0)\n",
    "yTestVector = yTest.apply(lambda x: 1 if x == \"relevant\" else 0)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "VOCAB_SIZE = len(vectorizer.vocabulary_)\n",
    "NUM_CLASSES = len(data[\"relevance\"].unique())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "from torch import nn\n",
    "from torch.nn import functional\n",
    "\n",
    "class LogisticRegressionClassifier(nn.Module):\n",
    "\n",
    "    def __init__(self, num_labels=NUM_CLASSES, vocab_size=VOCAB_SIZE, hidden_dim=3):\n",
    "        super(LogisticRegressionClassifier, self).__init__()\n",
    "\n",
    "        # two linear layers then sigmoid\n",
    "        self.linear = nn.Linear(vocab_size, hidden_dim)\n",
    "        self.linear2 = nn.Linear(hidden_dim, num_labels)\n",
    "        self.sigmoid = nn.Sigmoid()\n",
    "\n",
    "    def forward(self, vector):\n",
    "        x = self.linear(vector)\n",
    "        x = self.linear2(x)\n",
    "        return self.sigmoid(x)[:,-1]\n",
    "\n",
    "model = LogisticRegressionClassifier()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "xTrainTensor = torch.tensor(xTrainVector.todense(), dtype=torch.float16)\n",
    "xTestTensor = torch.tensor(xTestVector.todense(), dtype=torch.float16)\n",
    "yTrainTensor = torch.tensor(yTrainVector.values, dtype=torch.float16)\n",
    "yTestTensor = torch.tensor(yTestVector.values, dtype=torch.float16)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "tensor([0.5295])\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    sample = xTrainTensor[0:1]\n",
    "    print(model(sample))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from torch.utils.data import DataLoader, TensorDataset\n",
    "\n",
    "train_data = TensorDataset(xTrainTensor, yTrainTensor)\n",
    "test_data = TensorDataset(xTestTensor, yTestTensor)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True,)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "LogisticRegressionClassifier(\n",
      "  (linear): Linear(in_features=527099, out_features=3, bias=True)\n",
      "  (linear2): Linear(in_features=3, out_features=2, bias=True)\n",
      "  (sigmoid): Sigmoid()\n",
      ")\n"
     ]
    }
   ],
   "source": [
    "model = LogisticRegressionClassifier(hidden_dim=3)\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.01)\n",
    "\n",
    "loss_fn = nn.BCELoss()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f87659b96c784fa7be48e81854710265",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/25 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c1b3559135e47c9a647f2c82f5dfa88",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 1/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f3593d8b65d94795995a9714f8034d3b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 2/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "43b8e00649ac433fa7a7683d540114e0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 3/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9c66344567ff4566b789205fb6d1aec9",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 4/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de4375a3fb31470d9ea0bae0f812ea4c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 5/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7a9c3af2558e4d8293acbc5ec500be67",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 6/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3359bf38104e4ccfb8e597ef0b7a1972",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 7/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9dbc0b3b31614d1a947bed20a05cf053",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 8/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "111f071a00c744f0ae8fbcf8ab5f65e2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 9/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "f96326fc8fa740b0bc3abfee20f6d348",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 10/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "ef0eae80f44a4d40b953642444fd5325",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 11/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "54793eed868a4bb98455dad88e0e2c80",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 12/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c6932b1f32f949d0bd5fed7c815f14d5",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 13/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "46e5d0c0f6994a81bbb98330b54e0fde",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 14/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "c2fbeaf157fe4dc7b1ec819db1b9831e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 15/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "705d7f0067954d099f39a104b4be9003",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 16/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "74c14141a05741178acb491d25f15c51",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 17/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7da1e12b02b04de6a9b119723eaeedd3",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 18/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "5fdd5e289a444d35b1e47dde9cb184eb",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 19/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "b6db194804bd4159975f62ffddda6ffc",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 20/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "bbc3c929a74c4009af03d5c5defeaca4",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 21/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3818c528f8eb4dfeb0491b6cbca64030",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 22/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9bbb92c3cc00462c87f3874b583026a0",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 23/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8c4285db13ef46f09d20ea55ef5e4591",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 24/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "de8d8fe942394f208f9b461246e6e032",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Epoch 25/25:   0%|          | 0/63 [00:00<?, ?batch/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "EPOCHS = 25\n",
    "\n",
    "\n",
    "with trange(EPOCHS) as epochs:\n",
    "    for epoch in epochs:\n",
    "        for x, y in tqdm(train_loader, desc=f\"Epoch {epoch+1}/{EPOCHS}\", unit=\"batch\",leave=False,position=0):\n",
    "            optimizer.zero_grad()\n",
    "            y_pred = model(x)\n",
    "            loss = loss_fn(y_pred, y)\n",
    "\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "        epochs.set_postfix(loss=loss.item())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0. 0. 1. 0. 1. 1. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 1. 1. 1. 0. 0. 1. 1.\n",
      " 1. 0. 1. 1. 1. 1. 1. 0. 0. 0. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 0. 0. 1. 0. 1.\n",
      " 0. 0. 0. 1. 0. 1. 1. 1. 1. 0. 1. 0. 0. 1. 0. 1. 0. 0. 0. 0. 1. 0. 0. 0.\n",
      " 1. 1. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0.\n",
      " 1. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1. 1. 1. 0. 1. 1. 1. 1. 1. 0. 1. 1. 0. 0.\n",
      " 0. 0. 0. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1. 1. 0. 0. 1. 0. 1. 0. 1. 0.\n",
      " 1. 0. 1. 0. 1. 1. 0. 0. 1. 1. 1. 1. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 0. 0. 1. 1. 1. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 1. 1. 1. 1. 1. 1. 1. 0. 0. 0. 1. 1. 0. 1. 1. 1. 1. 1.\n",
      " 0. 0. 1. 0. 0. 0. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 1. 0. 0.\n",
      " 0. 1. 1. 1. 0. 0. 1. 1. 1. 1. 1. 1. 0. 0. 0. 0. 0. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 1. 1. 1. 1. 1. 0. 1. 0. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 1. 1. 0.\n",
      " 1. 0. 0. 0. 0. 1. 1. 1. 0. 0. 1. 0. 0. 1. 0. 1. 1. 1. 1. 0. 0. 0. 0. 0.\n",
      " 0. 0. 0. 0. 0. 1. 0. 0. 1. 0. 1. 1. 0. 1. 0. 0. 1. 0. 0. 0. 0. 0. 1. 1.\n",
      " 0. 1. 1. 0. 1. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 1. 0. 0. 0. 1. 0. 0. 0.\n",
      " 0. 0. 0. 0. 1. 1. 0. 1. 1. 1. 0. 1. 0. 1. 1. 0. 1. 0. 1. 0. 0. 1. 0. 0.\n",
      " 1. 0. 1. 0. 0. 0. 1. 1. 0. 1. 0. 1. 1. 0. 0. 1. 0. 0. 0. 0. 0. 0. 0. 1.\n",
      " 1. 0. 1. 1. 0. 0. 1. 1. 0. 1. 0. 0. 1. 1. 0. 0. 0. 1. 0. 1. 1. 0. 0. 0.\n",
      " 0. 0. 0. 1. 1. 1. 1. 0. 1. 0. 0. 0. 0. 1. 1. 0. 1. 0. 0. 0. 1. 0. 1. 1.\n",
      " 1. 1. 1. 0. 1. 1. 1. 0. 0. 0. 0. 1. 1. 1. 1. 0. 0. 0. 1. 1.]\n"
     ]
    }
   ],
   "source": [
    "with torch.no_grad():\n",
    "    model.eval()\n",
    "    yPred = model(xTestTensor)\n",
    "    yPred = yPred.squeeze().round().detach().numpy()\n",
    "\n",
    "print(yPred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       1.00      1.00      1.00       272\n",
      "           1       1.00      1.00      1.00       228\n",
      "\n",
      "    accuracy                           1.00       500\n",
      "   macro avg       1.00      1.00      1.00       500\n",
      "weighted avg       1.00      1.00      1.00       500\n",
      "\n"
     ]
    }
   ],
   "source": [
    "from sklearn.metrics import classification_report\n",
    "\n",
    "print(classification_report(yTestVector, yPred))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "text = \"\"\"Shark catch rates are higher in pelagic longline fisheries than in any other fishery, and sharks are\n",
    "typically discarded (bycatch) at sea. The post-release fate of discarded sharks is largely\n",
    "unobserved and could pose a significant source of unquantified mortality that may change stock\n",
    "assessment outcomes and prevent sound conservation and management advice. This study\n",
    "assessed post-release mortality rates of blue (Prionace glauca), bigeye thresher (Alopias\n",
    "superciliosus), oceanic whitetip (Carcharhinus longimanus), silky (C. falciformis) and shortfin\n",
    "mako (Isurus oxyrhincus) sharks discarded in the Hawaii deep-set and American Samoa longline\n",
    "fisheries targeting tuna in the central Pacific Ocean. The impacts on survival rates were\n",
    "examined considering species, fishery, fishing gear configuration, handling method, animal\n",
    "condition at capture and at release, and the amount of trailing fishing gear remaining on\n",
    "discarded sharks. Bayesian survival analysis showed that the condition at release (good vs.\n",
    "injured), branchline leader material, and the amount of trailing fishing gear left on the animals\n",
    "were among the factors that had the largest effect on post-release fate—animals captured on\n",
    "monofilament branchline leaders and released in good condition without trailing fishing gear had\n",
    "the highest rates of survival. This study shows that fisher behavior can have a significant impact\n",
    "on pelagic shark post-release mortality. Ensuring that sharks are handled carefully and released\n",
    "with minimal amounts of trailing fishing gear may reduce fishing mortality on shark populations.\"\"\"\n",
    "\n",
    "text = \"\"\"NumPy is an essential package for high-performance scientific computing and data analysis in the Python ecosystem. It is the foundation of many higher-level tools such as Pandas and scikit-learn.\n",
    "\n",
    "TensorFlow also uses NumPy arrays as the foundation for building Tensor objects and graph flow for deep learning tasks. These heavily rely on linear algebra operations on large lists, vectors, and matrices of numbers.\n",
    "\n",
    "NumPy is faster because it uses vectorized implementation, and many of its core functions are written in C.\"\"\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.save(model.state_dict(), \"models/LinearRegression.pth\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "v = vectorizer.transform([text])\n",
    "t = torch.tensor(v.todense(), dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.3013], grad_fn=<SelectBackward0>)"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model(t)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-pwi3iMQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

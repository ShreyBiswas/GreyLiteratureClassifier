{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "from tqdm.auto import tqdm,trange"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading data...\n",
      "Data loaded.\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 250 entries, 5902 to 498\n",
      "Data columns (total 4 columns):\n",
      " #   Column        Non-Null Count  Dtype \n",
      "---  ------        --------------  ----- \n",
      " 0   url           250 non-null    object\n",
      " 1   text          250 non-null    object\n",
      " 2   relevance     250 non-null    object\n",
      " 3   multiclasses  250 non-null    object\n",
      "dtypes: object(4)\n",
      "memory usage: 9.8+ KB\n"
     ]
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/labelled/data.json\", group_relevant=True):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    if group_relevant:\n",
    "        data[\"class\"] = data[\"class\"].apply(\n",
    "            lambda x: \"relevant\" if x != \"irrelevant\" else x\n",
    "        )\n",
    "    return data\n",
    "\n",
    "\n",
    "print(\"Loading data...\")\n",
    "\n",
    "data = import_labelled_data(\n",
    "    path=\"../../data/level-0.5/data.json\", group_relevant=False\n",
    ")\n",
    "\n",
    "print(\"Data loaded.\")\n",
    "\n",
    "data = data.sample(frac=1).reset_index(drop=True)\n",
    "\n",
    "data = data.sample(250)\n",
    "\n",
    "\n",
    "\n",
    "data.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "\n",
    "count = CountVectorizer()\n",
    "tfidf = TfidfVectorizer()\n",
    "count.fit(data[\"text\"])\n",
    "tfidf.fit(data[\"text\"])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sentence_transformers import SentenceTransformer\n",
    "\n",
    "sources = [\n",
    "    # 'dunzhang/stella_en_400M_v5',\n",
    "    # \"avsolatorio/GIST-small-Embedding-v0\",\n",
    "    # \"avsolatorio/GIST-Embedding-v0\",\n",
    "    # \"Alibaba-NLP/gte-base-en-v1.5\",\n",
    "    # \"allenai/longformer-base-4096\",\n",
    "    'jinaai/jina-embeddings-v2-small-en'\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading jinaai/jina-embeddings-v2-small-en...\n",
      "Maximum sequence length: 8192\n"
     ]
    }
   ],
   "source": [
    "embedders = []\n",
    "for source in sources:\n",
    "    print(f\"Loading {source}...\")\n",
    "    model = SentenceTransformer(source,trust_remote_code=True)\n",
    "    embedders.append(model)\n",
    "    print('Maximum sequence length:',model.max_seq_length)\n",
    "    model.max_seq_length = min(model.max_seq_length,4096)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{4096}"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "sequence_lengths = set([\n",
    "    model.max_seq_length for model in embedders\n",
    "])\n",
    "\n",
    "sequence_lengths"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "3e92e32701c34f26aa561d2111bfe0da",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/250 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chunking import chunk_dataset_and_explode\n",
    "\n",
    "chunked_data = {}\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    chunked_data[sequence_length] = chunk_dataset_and_explode(data, sequence_length,overlap=int(sequence_length*0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "datasets = {}\n",
    "\n",
    "for sequence_length in sequence_lengths:\n",
    "    datasets[sequence_length] = Dataset.from_pandas(chunked_data[sequence_length])\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "# get version of cuda\n",
    "print(torch.version.cuda)\n",
    "# clear cache\n",
    "torch.cuda.empty_cache()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Embedding SentenceTransformer(\n",
      "  (0): Transformer({'max_seq_length': 4096, 'do_lower_case': False}) with Transformer model: JinaBertModel \n",
      "  (1): Pooling({'word_embedding_dimension': 512, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False, 'pooling_mode_weightedmean_tokens': False, 'pooling_mode_lasttoken': False, 'include_prompt': True})\n",
      ")\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "0e0e1d610b8644edb1bf3606564c3546",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Batches:   0%|          | 0/637 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# time encoding\n",
    "\n",
    "embeddings = dict.fromkeys(embedders,None)\n",
    "\n",
    "for model in embedders:\n",
    "    print(f\"Embedding {model}\")\n",
    "\n",
    "    embeddings[model] = train_embeddings = model.encode(\n",
    "    datasets[model.max_seq_length][\"text\"],\n",
    "    show_progress_bar=True,\n",
    "    batch_size=8,\n",
    "    precision='float32',\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "CPU times: user 3.74 s, sys: 80 ms, total: 3.82 s\n",
      "Wall time: 3.79 s\n",
      "CPU times: user 3.7 s, sys: 84.6 ms, total: 3.78 s\n",
      "Wall time: 3.75 s\n"
     ]
    }
   ],
   "source": [
    "%time count_txt = count.transform(data[\"text\"])\n",
    "%time tfidf_txt = tfidf.transform(data[\"text\"])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-eJH_GeT1",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}

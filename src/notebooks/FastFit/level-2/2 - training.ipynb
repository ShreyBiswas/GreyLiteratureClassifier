{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Separating Actions from conservation-adjacent texts\n",
    "'Irrelevant' now represents non-actions, 'Relevant' represents action evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tqdm.notebook import tqdm\n",
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEV = True\n",
    "\n",
    "if DEV:\n",
    "    model_name = \"avsolatorio/GIST-small-Embedding-v0\"\n",
    "else:\n",
    "    model_name = \"avsolatorio/GIST-Embedding-v0\"\n",
    "\n",
    "\n",
    "model_name = 'jinaai/jina-embeddings-v2-small-en'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 8763 entries, 0 to 8762\n",
      "Data columns (total 5 columns):\n",
      " #   Column        Non-Null Count  Dtype  \n",
      "---  ------        --------------  -----  \n",
      " 0   url           8763 non-null   object \n",
      " 1   text          8763 non-null   object \n",
      " 2   relevance     8763 non-null   object \n",
      " 3   multiclasses  8763 non-null   object \n",
      " 4   score-lv1     167 non-null    float64\n",
      "dtypes: float64(1), object(4)\n",
      "memory usage: 342.4+ KB\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "None"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>multiclasses</th>\n",
       "      <th>score-lv1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>https://www.conservationevidence.com/synopsis/...</td>\n",
       "      <td>1 \\n \\n \\n2 \\n \\n \\nSubtidal Benthic Invertebr...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Marine Invertebrates]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>https://www.conservationevidence.com/synopsis/...</td>\n",
       "      <td>\\n \\n   \\n  Control of freshwater  \\n  invasi...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Fish, Rivers and Lakes, Invasive]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>https://www.conservationevidence.com/synopsis/...</td>\n",
       "      <td>1 \\n \\nGrassland Conservation \\n2 \\n \\nGrassla...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Grassland]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>https://www.conservationevidence.com/synopsis/...</td>\n",
       "      <td>\\n \\n \\nii \\n \\n \\n \\n \\n \\n \\n  \\nPrimate Co...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Mammals]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>https://www.conservationevidence.com/synopsis/...</td>\n",
       "      <td>CONSERVATION EVIDENCE SERIES SYNOPSES\\nTerrest...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Mammals]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                                                 url  \\\n",
       "0  https://www.conservationevidence.com/synopsis/...   \n",
       "1  https://www.conservationevidence.com/synopsis/...   \n",
       "2  https://www.conservationevidence.com/synopsis/...   \n",
       "3  https://www.conservationevidence.com/synopsis/...   \n",
       "4  https://www.conservationevidence.com/synopsis/...   \n",
       "\n",
       "                                                text relevance  \\\n",
       "0  1 \\n \\n \\n2 \\n \\n \\nSubtidal Benthic Invertebr...  relevant   \n",
       "1   \\n \\n   \\n  Control of freshwater  \\n  invasi...  relevant   \n",
       "2  1 \\n \\nGrassland Conservation \\n2 \\n \\nGrassla...  relevant   \n",
       "3   \\n \\n \\nii \\n \\n \\n \\n \\n \\n \\n  \\nPrimate Co...  relevant   \n",
       "4  CONSERVATION EVIDENCE SERIES SYNOPSES\\nTerrest...  relevant   \n",
       "\n",
       "                         multiclasses  score-lv1  \n",
       "0              [Marine Invertebrates]        NaN  \n",
       "1  [Fish, Rivers and Lakes, Invasive]        NaN  \n",
       "2                         [Grassland]        NaN  \n",
       "3                           [Mammals]        NaN  \n",
       "4                           [Mammals]        NaN  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def import_labelled_data(path=\"data/level-1.5/merged/data.json\"):\n",
    "    data = pd.read_json(path, encoding=\"latin-1\")\n",
    "    return data\n",
    "\n",
    "\n",
    "data = import_labelled_data(path=\"../../../data/level-1.5/merged/data.json\", )\n",
    "\n",
    "\n",
    "\n",
    "# train test split\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# 0.45 train, 0.15 val, 0.4 test\n",
    "train_data, test_data = train_test_split(data, test_size=0.4, random_state=42)\n",
    "train_data, val_data = train_test_split(train_data, test_size=0.25, random_state=42)\n",
    "\n",
    "display(data.info())\n",
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8b06fff437a14429a172ac51665bff25",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3942 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e98c54cd00154f4dbd93592adbbbfc91",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/3506 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "48dfdbd2aced40208924107e4288c98c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "  0%|          | 0/1315 [00:00<?, ?it/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from chunking import chunk_dataset_and_explode\n",
    "\n",
    "\n",
    "# roughly 4 characters per token\n",
    "max_len = 512\n",
    "\n",
    "train_data = chunk_dataset_and_explode(train_data, max_len=max_len, overlap=int(max_len * 0.2))\n",
    "test_data = chunk_dataset_and_explode(test_data, max_len=max_len, overlap=int(max_len * 0.2))\n",
    "val_data = chunk_dataset_and_explode(val_data, max_len=max_len, overlap=int(max_len * 0.2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "relevance\n",
       "relevant      52217\n",
       "irrelevant    28784\n",
       "Name: count, dtype: int64"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data['relevance'].value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def stratified_sample(dataset,label_column: str = 'relevance',num_samples_per_label: int = 100):\n",
    "    return (\n",
    "        dataset\n",
    "        .sample(frac=1,random_state=42)\n",
    "        .groupby(label_column)[dataset.columns]\n",
    "        .apply(lambda x: x.sample(min(num_samples_per_label,len(x)),random_state=42),include_groups=True).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "\n",
    "def sorted_stratified_sample(dataset,label_column: str = 'relevance', sorting_column: str = 'score-lv1', num_samples_per_label: int = 100):\n",
    "    # get top num_samples_per_label samples per label\n",
    "    return (\n",
    "        dataset\n",
    "        .sort_values(sorting_column,ascending=False)\n",
    "        .groupby(label_column)[dataset.columns]\n",
    "        .apply(lambda x: x.head(min(num_samples_per_label,len(x))),include_groups=True).reset_index(drop=True)\n",
    "    )\n",
    "\n",
    "if DEV:\n",
    "    train_data = sorted_stratified_sample(train_data,label_column='relevance', sorting_column='score-lv1', num_samples_per_label=500)\n",
    "    val_data = val_data.sample(100,random_state=42)\n",
    "    test_data = test_data.sample(500,random_state=42)\n",
    "else:\n",
    "    train_data = train_data.sample(frac=1,random_state=42)\n",
    "    val_data = val_data.sample(500,random_state=42)\n",
    "    test_data = test_data.sample(frac=1,random_state=42)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>chunk_id</th>\n",
       "      <th>url</th>\n",
       "      <th>text</th>\n",
       "      <th>relevance</th>\n",
       "      <th>multiclasses</th>\n",
       "      <th>score-lv1</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>8725</td>\n",
       "      <td>http://xerces.org/sites/default/files/19-053_B...</td>\n",
       "      <td>ication and reference only. The appearance of ...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>8653</td>\n",
       "      <td>https://yearofthesalmon.org/wp-content/uploads...</td>\n",
       "      <td>Adapting Paciﬁc salmon management systems to a...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>8653</td>\n",
       "      <td>https://yearofthesalmon.org/wp-content/uploads...</td>\n",
       "      <td>ciﬁc Bering Sea British Columbia FAR = 1 - (pr...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>8653</td>\n",
       "      <td>https://yearofthesalmon.org/wp-content/uploads...</td>\n",
       "      <td>25 42-45 29 47 41 24 46 40 39 30-34 23 35-36 3...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>8653</td>\n",
       "      <td>https://yearofthesalmon.org/wp-content/uploads...</td>\n",
       "      <td>m 1960-1988 1988-2012 -0.2 0 0.2 0.4 0.6 0.8 C...</td>\n",
       "      <td>irrelevant</td>\n",
       "      <td>[]</td>\n",
       "      <td>1.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>995</th>\n",
       "      <td>3420</td>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>le vaccines: Lipogen Forte V1, Renogen, Bayova...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Marine]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>996</th>\n",
       "      <td>3420</td>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>k of bacterial kidney disease when compared wi...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Marine]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>997</th>\n",
       "      <td>2625</td>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>Create skylark plots for bird conservationA st...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Birds, Farmland]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>998</th>\n",
       "      <td>2625</td>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>w. Overwinter counts of yellowhammer Emberiza ...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Birds, Farmland]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>999</th>\n",
       "      <td>2625</td>\n",
       "      <td>https://www.conservationevidence.com/individua...</td>\n",
       "      <td>as well as high numbers of yellowhammer Ember...</td>\n",
       "      <td>relevant</td>\n",
       "      <td>[Birds, Farmland]</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>1000 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "     chunk_id                                                url  \\\n",
       "0        8725  http://xerces.org/sites/default/files/19-053_B...   \n",
       "1        8653  https://yearofthesalmon.org/wp-content/uploads...   \n",
       "2        8653  https://yearofthesalmon.org/wp-content/uploads...   \n",
       "3        8653  https://yearofthesalmon.org/wp-content/uploads...   \n",
       "4        8653  https://yearofthesalmon.org/wp-content/uploads...   \n",
       "..        ...                                                ...   \n",
       "995      3420  https://www.conservationevidence.com/individua...   \n",
       "996      3420  https://www.conservationevidence.com/individua...   \n",
       "997      2625  https://www.conservationevidence.com/individua...   \n",
       "998      2625  https://www.conservationevidence.com/individua...   \n",
       "999      2625  https://www.conservationevidence.com/individua...   \n",
       "\n",
       "                                                  text   relevance  \\\n",
       "0    ication and reference only. The appearance of ...  irrelevant   \n",
       "1    Adapting Paciﬁc salmon management systems to a...  irrelevant   \n",
       "2    ciﬁc Bering Sea British Columbia FAR = 1 - (pr...  irrelevant   \n",
       "3    25 42-45 29 47 41 24 46 40 39 30-34 23 35-36 3...  irrelevant   \n",
       "4    m 1960-1988 1988-2012 -0.2 0 0.2 0.4 0.6 0.8 C...  irrelevant   \n",
       "..                                                 ...         ...   \n",
       "995  le vaccines: Lipogen Forte V1, Renogen, Bayova...    relevant   \n",
       "996  k of bacterial kidney disease when compared wi...    relevant   \n",
       "997  Create skylark plots for bird conservationA st...    relevant   \n",
       "998  w. Overwinter counts of yellowhammer Emberiza ...    relevant   \n",
       "999   as well as high numbers of yellowhammer Ember...    relevant   \n",
       "\n",
       "          multiclasses  score-lv1  \n",
       "0                   []        1.0  \n",
       "1                   []        1.0  \n",
       "2                   []        1.0  \n",
       "3                   []        1.0  \n",
       "4                   []        1.0  \n",
       "..                 ...        ...  \n",
       "995           [Marine]        NaN  \n",
       "996           [Marine]        NaN  \n",
       "997  [Birds, Farmland]        NaN  \n",
       "998  [Birds, Farmland]        NaN  \n",
       "999  [Birds, Farmland]        NaN  \n",
       "\n",
       "[1000 rows x 6 columns]"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Dataset({\n",
       "    features: ['chunk_id', 'url', 'text', 'relevance', 'multiclasses', 'score-lv1'],\n",
       "    num_rows: 1000\n",
       "})"
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from datasets import Dataset\n",
    "\n",
    "train_dataset = Dataset.from_pandas(train_data, split=\"train\")\n",
    "test_dataset = Dataset.from_pandas(test_data, split=\"test\")\n",
    "val_dataset = Dataset.from_pandas(val_data, split=\"val\")\n",
    "\n",
    "train_dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "#! had to modify FastFitTrainer to at /fastfit/train.py, line 879, to add trust_remote_code=True to the loading of 'accuracy' metrics\n",
    "#! don't know why it's not default, since accuracy is the default in fastfit\n",
    "\n",
    "\n",
    "\n",
    "#! IMPORTANT: another change in FastFitTrainer, also at line 879; comment out and replace the fixed version above\n",
    "#! since load_metric is deprecated in favour of evaluate.load()\n",
    "#! added functionality for sending in multiple metrics to evaluate at once\n",
    "#! added macro averages for non-accuracy metrics too\n",
    "#! essentially, copy the below code to replace line 879"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Insert into line 879.\n",
    "\n",
    "```python\n",
    "        # metric = load_metric(self.data_args.metric_name, experiment_id=uuid.uuid4())\n",
    "        from evaluate import combine, load\n",
    "        if type(self.data_args.metric_name) == str: # single metric name\n",
    "            metrics = [load(self.data_args.metric_name, experiment_id=uuid.uuid4())]\n",
    "        elif type(self.data_args.metric_name) == list: # compute multiple metrics\n",
    "            metrics = [load(metric,experiment_id=uuid.uuid4()) for metric in self.data_args.metric_name]\n",
    "\n",
    "        # You can define your custom compute_metrics function. It takes an `EvalPrediction` object (a namedtuple with a\n",
    "        # predictions and label_ids field) and has to return a dictionary string to float.\n",
    "        def compute_metrics(p: EvalPrediction):\n",
    "            predictions = (\n",
    "                p.predictions[0] if isinstance(p.predictions, tuple) else p.predictions\n",
    "            )\n",
    "            predictions = (\n",
    "                np.squeeze(predictions)\n",
    "                if self.is_regression\n",
    "                else np.argmax(predictions, axis=1)\n",
    "            )\n",
    "            references = p.label_ids\n",
    "\n",
    "            results = {}\n",
    "\n",
    "            for metric in metrics:\n",
    "                if metric.name != 'accuracy':\n",
    "                    results.update(metric.compute(predictions=predictions, references=references,average='macro'))\n",
    "                else:\n",
    "                    results.update(metric.compute(predictions=predictions, references=references))\n",
    "\n",
    "            return results\n",
    "    ```"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "True\n",
      "0\n",
      "NVIDIA GeForce RTX 3090\n",
      "12.1\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "print(torch.cuda.is_available())\n",
    "print(torch.cuda.current_device())\n",
    "print(torch.cuda.get_device_name(0))\n",
    "\n",
    "print(torch.version.cuda)\n",
    "\n",
    "torch.cuda.empty_cache()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "07/31/2024 13:09:17 - WARNING - fastfit.train - Process rank: 0, device: cuda:0, n_gpu: 1distributed training: True, 16-bits training: True\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|modeling_utils.py:4366] 2024-07-31 13:09:19,049 >> Some weights of BertModel were not initialized from the model checkpoint at jinaai/jina-embeddings-v2-small-en and are newly initialized: ['embeddings.position_embeddings.weight', 'encoder.layer.0.intermediate.dense.bias', 'encoder.layer.0.intermediate.dense.weight', 'encoder.layer.0.output.LayerNorm.bias', 'encoder.layer.0.output.LayerNorm.weight', 'encoder.layer.0.output.dense.bias', 'encoder.layer.0.output.dense.weight', 'encoder.layer.1.intermediate.dense.bias', 'encoder.layer.1.intermediate.dense.weight', 'encoder.layer.1.output.LayerNorm.bias', 'encoder.layer.1.output.LayerNorm.weight', 'encoder.layer.1.output.dense.bias', 'encoder.layer.1.output.dense.weight', 'encoder.layer.2.intermediate.dense.bias', 'encoder.layer.2.intermediate.dense.weight', 'encoder.layer.2.output.LayerNorm.bias', 'encoder.layer.2.output.LayerNorm.weight', 'encoder.layer.2.output.dense.bias', 'encoder.layer.2.output.dense.weight', 'encoder.layer.3.intermediate.dense.bias', 'encoder.layer.3.intermediate.dense.weight', 'encoder.layer.3.output.LayerNorm.bias', 'encoder.layer.3.output.LayerNorm.weight', 'encoder.layer.3.output.dense.bias', 'encoder.layer.3.output.dense.weight']\n",
      "You should probably TRAIN this model on a down-stream task to be able to use it for predictions and inference.\n"
     ]
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "748a0c6e2c6640d18b0b9749fe8109a2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/1000 [00:00<?…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "dfe5dfaf3e1f4d51b0e189775603eb0b",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/100 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "e1ee0cabb5754e968f3a79a79234be65",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset to infer max length for both query and document:   0%|          | 0/500 [00:00<?,…"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "7c79a4a07a2249f18ce3aff179f9472e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/1000 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "44522caa86ea4745ba9476b8f151db3c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/100 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "9ff294cb941a4fd7a35f2cd0ea09df2c",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "Running tokenizer on dataset:   0%|          | 0/500 [00:00<?, ? examples/s]"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cuda:0\n"
     ]
    }
   ],
   "source": [
    "from fastfit import FastFitTrainer\n",
    "\n",
    "# same args as the huggingface TrainingArguments\n",
    "if DEV:\n",
    "    output_dir = f'models/relevance/dev/{model_name}'\n",
    "else:\n",
    "    output_dir = f'models/relevance/{model_name}'\n",
    "\n",
    "trainer = FastFitTrainer(\n",
    "    model_name_or_path=model_name,\n",
    "    train_dataset=train_dataset,\n",
    "    validation_dataset=val_dataset,\n",
    "    test_dataset=test_dataset,\n",
    "    output_dir=output_dir,\n",
    "    overwrite_output_dir=True,\n",
    "    label_column_name='relevance',\n",
    "    text_column_name=\"text\",\n",
    "    num_train_epochs=10,\n",
    "    per_device_train_batch_size=64,\n",
    "    per_device_eval_batch_size=64,\n",
    "    max_text_length=max_len,\n",
    "    num_repeats=1,\n",
    "    eval_strategy=\"epoch\",\n",
    "    save_strategy=\"epoch\",\n",
    "    logging_strategy='epoch',\n",
    "    metric_name=['precision','recall','f1','accuracy'],\n",
    "    load_best_model_at_end=True,\n",
    "    metric_for_best_model='recall',\n",
    "    fp16=True,\n",
    ")\n",
    "\n",
    "print(trainer.model.device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[WARNING|logging.py:328] 2024-07-31 13:09:31,865 >> BertSdpaSelfAttention is used but `torch.nn.functional.scaled_dot_product_attention` does not support non-absolute `position_embedding_type` or `output_attentions=True` or `head_mask`. Falling back to the manual attention implementation, but specifying the manual implementation will be required from Transformers version v5.0.0 onwards. This warning can be removed using the argument `attn_implementation=\"eager\"` when loading the model.\n",
      "[WARNING|modeling_utils.py:1198] 2024-07-31 13:09:32,854 >> Could not estimate the number of tokens of the input, floating-point operations will not be computed\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='160' max='160' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [160/160 00:45, Epoch 10/10]\n",
       "    </div>\n",
       "    <table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       " <tr style=\"text-align: left;\">\n",
       "      <th>Epoch</th>\n",
       "      <th>Training Loss</th>\n",
       "      <th>Validation Loss</th>\n",
       "      <th>Precision</th>\n",
       "      <th>Recall</th>\n",
       "      <th>F1</th>\n",
       "      <th>Accuracy</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <td>1</td>\n",
       "      <td>4.831200</td>\n",
       "      <td>4.766787</td>\n",
       "      <td>0.649830</td>\n",
       "      <td>0.656520</td>\n",
       "      <td>0.652778</td>\n",
       "      <td>0.720000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>2</td>\n",
       "      <td>4.292600</td>\n",
       "      <td>4.780195</td>\n",
       "      <td>0.770374</td>\n",
       "      <td>0.823186</td>\n",
       "      <td>0.783476</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>3</td>\n",
       "      <td>4.192700</td>\n",
       "      <td>4.762451</td>\n",
       "      <td>0.785651</td>\n",
       "      <td>0.825216</td>\n",
       "      <td>0.799505</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>4</td>\n",
       "      <td>4.153400</td>\n",
       "      <td>4.868021</td>\n",
       "      <td>0.788194</td>\n",
       "      <td>0.836885</td>\n",
       "      <td>0.803036</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>5</td>\n",
       "      <td>4.137300</td>\n",
       "      <td>5.062438</td>\n",
       "      <td>0.780378</td>\n",
       "      <td>0.846525</td>\n",
       "      <td>0.789800</td>\n",
       "      <td>0.810000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>6</td>\n",
       "      <td>4.133000</td>\n",
       "      <td>4.985996</td>\n",
       "      <td>0.797802</td>\n",
       "      <td>0.843734</td>\n",
       "      <td>0.812997</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>7</td>\n",
       "      <td>4.127100</td>\n",
       "      <td>5.027970</td>\n",
       "      <td>0.795231</td>\n",
       "      <td>0.820396</td>\n",
       "      <td>0.805731</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>8</td>\n",
       "      <td>4.125200</td>\n",
       "      <td>5.063736</td>\n",
       "      <td>0.797802</td>\n",
       "      <td>0.843734</td>\n",
       "      <td>0.812997</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>9</td>\n",
       "      <td>4.125800</td>\n",
       "      <td>5.070460</td>\n",
       "      <td>0.795231</td>\n",
       "      <td>0.820396</td>\n",
       "      <td>0.805731</td>\n",
       "      <td>0.840000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <td>10</td>\n",
       "      <td>4.127700</td>\n",
       "      <td>5.083606</td>\n",
       "      <td>0.784007</td>\n",
       "      <td>0.813546</td>\n",
       "      <td>0.795649</td>\n",
       "      <td>0.830000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table><p>"
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** train metrics *****\n",
      "  epoch                    =       10.0\n",
      "  total_flos               =        0GF\n",
      "  train_loss               =     4.2246\n",
      "  train_runtime            = 0:00:47.02\n",
      "  train_samples            =       1000\n",
      "  train_samples_per_second =    212.675\n",
      "  train_steps_per_second   =      3.403\n"
     ]
    }
   ],
   "source": [
    "\n",
    "#! another fastfit library modification\n",
    "#! in /fastfit/train.py, line 971, change ignore_keys_for_eval from type set to a list\n",
    "#! since it gets concatenated to a list later on\n",
    "#! note that since we've added lines above, this is now line 981\n",
    "#! the line beginning ignore_keys_for_eval={\"doc_input_ids\",\"doc_attention_mask\",\"labels\"}\n",
    "\n",
    "\n",
    "model = trainer.train()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "\n",
       "    <div>\n",
       "      \n",
       "      <progress value='10' max='2' style='width:300px; height:20px; vertical-align: middle;'></progress>\n",
       "      [2/2 00:01]\n",
       "    </div>\n",
       "    "
      ],
      "text/plain": [
       "<IPython.core.display.HTML object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** eval metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =       0.81\n",
      "  eval_f1                 =     0.7898\n",
      "  eval_loss               =     5.0624\n",
      "  eval_precision          =     0.7804\n",
      "  eval_recall             =     0.8465\n",
      "  eval_runtime            = 0:00:00.26\n",
      "  eval_samples            =        100\n",
      "  eval_samples_per_second =    371.271\n",
      "  eval_steps_per_second   =      7.425\n"
     ]
    }
   ],
   "source": [
    "results = trainer.evaluate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'eval_loss': 5.062438488006592,\n",
       " 'eval_precision': 0.7803776683087028,\n",
       " 'eval_recall': 0.8465246067985794,\n",
       " 'eval_f1': 0.7897997566102445,\n",
       " 'eval_accuracy': 0.81,\n",
       " 'eval_runtime': 0.2693,\n",
       " 'eval_samples_per_second': 371.271,\n",
       " 'eval_steps_per_second': 7.425,\n",
       " 'epoch': 10.0,\n",
       " 'eval_samples': 100}"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Accuracy: 0.81\n"
     ]
    }
   ],
   "source": [
    "print(f'Accuracy: {results[\"eval_accuracy\"]}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "***** test metrics *****\n",
      "  epoch                   =       10.0\n",
      "  eval_accuracy           =       0.84\n",
      "  eval_f1                 =     0.8389\n",
      "  eval_loss               =     5.1147\n",
      "  eval_precision          =     0.8394\n",
      "  eval_recall             =     0.8483\n",
      "  eval_runtime            = 0:00:01.09\n",
      "  eval_samples_per_second =    455.739\n",
      "  eval_steps_per_second   =      7.292\n",
      "  test_samples            =        500\n"
     ]
    }
   ],
   "source": [
    "results = trainer.test()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "\n",
    "from os import makedirs, path\n",
    "if not path.exists(f'models/relevance/dev/{model_name}'):\n",
    "    makedirs(f'models/relevance/dev/{model_name}')\n",
    "\n",
    "model.save_pretrained(f'models/relevance/dev/{model_name}')\n",
    "\n",
    "if not DEV:\n",
    "    model.save_pretrained(f'models/relevance/{model_name}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "GreyLiteratureClassifier-pwi3iMQR",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
